<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Docker进阶]]></title>
    <url>%2F2020%2F12%2F26%2FDocker%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[容器数据卷什么是容器数据卷将应用和环境打包成一个镜像！ 数据？如果数据都在容器中，那么我们容器删除，数据就会丢失！需求：数据可以持久化 MySQL，容器删除了，删库跑路！需求：MySQL数据可以存储在本地！ 容器之间可以有一个数据共享的技术！Docker容器中产生的数据，同步到本地！ 这就是卷技术！目录的挂载，将我们容器内的目录，挂载到Linux上面！ 总结一句话：容器的持久化和同步操作！容器间也是可以数据共享的！ 使用数据卷 方式一 ：直接使用命令挂载 -v 12345678910-v, --volume list Bind mount a volumedocker run -it -v 主机目录:容器内目录 -p 主机端口:容器内端口# /home/ceshi：主机home目录下的ceshi文件夹 映射：centos容器中的/home[root@iz2zeak7 home]# docker run -it -v /home/ceshi:/home centos /bin/bash#这时候主机的/home/ceshi文件夹就和容器的/home文件夹关联了,二者可以实现文件或数据同步了#通过 docker inspect 容器id 查看[root@iz2zeak7sgj6i7hrb2g862z home]# docker inspect 6064c490c371123456789 测试文件的同步 再来测试！ 1、停止容器 2、宿主机修改文件 3、启动容器 4、容器内的数据依旧是同步的 好处：我们以后修改只需要在本地修改即可，容器内会自动同步！ 实战：安装MySQL思考：MySQL的数据持久化的问题 1234567891011121314151617181920# 获取mysql镜像[root@iz2zeak7sgj6i7hrb2g862z home]# docker pull mysql:5.7# 运行容器,需要做数据挂载 #安装启动mysql，需要配置密码的，这是要注意点！# 参考官网hub docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag#启动我们得-d 后台运行-p 端口映射-v 卷挂载-e 环境配置-- name 容器名字$ docker run -d -p 3310:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql03 mysql:5.7# 启动成功之后，我们在本地使用sqlyog来测试一下# sqlyog-连接到服务器的3306--和容器内的3306映射 # 在本地测试创建一个数据库，查看一下我们映射的路径是否ok！12345678910111213141516171819 测试连接：注意3310端口要在阿里云服务器的安全组中打开，否则无法连接。 当我们在本地用SQLyog新建名称为test的数据库时候，容器容器也会创建 假设我们将包含mysql的容器删除时， 发现，我们挂载到本地的数据卷依旧没有丢失，这就实现了容器数据持久化功能。 具名和匿名挂载123456789101112131415161718192021222324252627282930313233343536373839# 匿名挂载-v 容器内路径!$ docker run -d -P --name nginx01 -v /etc/nginx nginx# 查看所有的volume(卷)的情况$ docker volume ls DRIVER VOLUME NAME # 容器内的卷名(匿名卷挂载)local 21159a8518abd468728cdbe8594a75b204a10c26be6c36090cde1ee88965f0d0local b17f52d38f528893dd5720899f555caf22b31bf50b0680e7c6d5431dbda2802c # 这里发现，这种就是匿名挂载，我们在 -v只写了容器内的路径，没有写容器外的路径！# 具名挂载 -P:表示随机映射端口$ docker run -d -P --name nginx02 -v juming-nginx:/etc/nginx nginx9663cfcb1e5a9a1548867481bfddab9fd7824a6dc4c778bf438a040fe891f0ee# 查看所有的volume(卷)的情况$ docker volume ls DRIVER VOLUME NAMElocal 21159a8518abd468728cdbe8594a75b204a10c26be6c36090cde1ee88965f0d0local b17f52d38f528893dd5720899f555caf22b31bf50b0680e7c6d5431dbda2802clocal juming-nginx #多了一个名字# 通过 -v 卷名：查看容器内路径# 查看一下这个卷$ docker volume inspect juming-nginx[ &#123; "CreatedAt": "2020-05-23T13:55:34+08:00", "Driver": "local", "Labels": null, "Mountpoint": "/var/lib/docker/volumes/juming-nginx/_data", #默认目录 "Name": "juming-nginx", "Options": null, "Scope": "local" &#125;]1234567891011121314151617181920212223242526272829303132333435363738 所有的docker容器内的卷，没有指定目录的情况下都是在/var/lib/docker/volumes/自定义的卷名/_data下，如果指定了目录，docker volume ls 是查看不到的。 区分三种挂载方式 12345# 三种挂载： 匿名挂载、具名挂载、指定路径挂载-v 容器内路径 #匿名挂载-v 卷名：容器内路径 #具名挂载-v /宿主机路径：容器内路径 #指定路径挂载 docker volume ls 是查看不到的1234 拓展： 12345678# 通过 -v 容器内路径： ro rw 改变读写权限ro #readonly 只读rw #readwrite 可读可写$ docker run -d -P --name nginx05 -v juming:/etc/nginx:ro nginx$ docker run -d -P --name nginx05 -v juming:/etc/nginx:rw nginx# ro 只要看到ro就说明这个路径只能通过宿主机来操作，容器内部是无法操作！1234567 初始DockerfileDockerfile 就是用来构建docker镜像的构建文件！命令脚本！先体验一下！ 通过这个脚本可以生成镜像，镜像是一层一层的，脚本是一个个的命令，每个命令都是一层！ 12345678910111213141516171819202122232425262728293031323334353637383940414243# 创建一个dockerfile文件，名字可以随便 建议Dockerfile# 文件中的内容： 指令(大写) + 参数$ vim dockerfile1 FROM centos # 当前这个镜像是以centos为基础的 VOLUME ["volume01","volume02"] # 挂载卷的卷目录列表(多个目录) CMD echo "-----end-----" # 输出一下用于测试 CMD /bin/bash # 默认走bash控制台# 这里的每个命令，就是镜像的一层！# 构建出这个镜像 -f dockerfile1 # f代表file，指这个当前文件的地址(这里是当前目录下的dockerfile1)-t caoshipeng/centos # t就代表target，指目标目录(注意caoshipeng镜像名前不能加斜杠‘/’). # 表示生成在当前目录下$ docker build -f dockerfile1 -t caoshipeng/centos .Sending build context to Docker daemon 2.56kBStep 1/4 : FROM centoslatest: Pulling from library/centos8a29a15cefae: Already exists Digest: sha256:fe8d824220415eed5477b63addf40fb06c3b049404242b31982106ac204f6700Status: Downloaded newer image for centos:latest ---&gt; 470671670cacStep 2/4 : VOLUME ["volume01","volume02"] # 卷名列表 ---&gt; Running in c18eefc2c233Removing intermediate container c18eefc2c233 ---&gt; 623ae1d40fb8Step 3/4 : CMD echo "-----end-----" # 输出 脚本命令 ---&gt; Running in 70e403669f3cRemoving intermediate container 70e403669f3c ---&gt; 0eba1989c4e6Step 4/4 : CMD /bin/bash ---&gt; Running in 4342feb3a05bRemoving intermediate container 4342feb3a05b ---&gt; f4a6b0d4d948Successfully built f4a6b0d4d948Successfully tagged caoshipeng/centos:latest# 查看自己构建的镜像$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEcaoshipeng/centos latest f4a6b0d4d948 About a minute ago 237MB123456789101112131415161718192021222324252627282930313233343536373839404142 启动自己写的容器镜像 123$ docker run -it f4a6b0d4d948 /bin/bash # 运行自己写的镜像$ ls -l # 查看目录12 这个卷和外部一定有一个同步的目录 查看一下卷挂载 123# docker inspect 容器id$ docker inspect ca3b45913df512 测试一下刚才的文件是否同步出去了！ 这种方式使用的十分多，因为我们通常会构建自己的镜像！ 假设构建镜像时候没有挂载卷，要手动镜像挂载 -v 卷名：容器内路径！ 数据卷容器多个MySQL同步数据！ 命名的容器挂载数据卷！ 12345678910111213141516171819202122# 测试 启动3个容器，通过刚才自己写的镜像启动# 创建docker01：因为我本机是最新版，故这里用latest，狂神老师用的是1.0如下图$ docker run -it --name docker01 caoshipeng/centos:latest# 查看容器docekr01内容$ lsbin home lost+found opt run sys vardev lib media proc sbin tmp volume01etc lib64 mnt root srv usr volume02# 不关闭该容器退出CTRL + Q + P # 创建docker02: 并且让docker02 继承 docker01$ docker run -it --name docker02 --volumes-from docker01 caoshipeng/centos:latest# 查看容器docker02内容$ lsbin home lost+found opt run sys vardev lib media proc sbin tmp volume01etc lib64 mnt root srv usr volume02123456789101112131415161718192021 123456789# 再新建一个docker03同样继承docker01$ docker run -it --name docker03 --volumes-from docker01 caoshipeng/centos:latest$ cd volume01 #进入volume01 查看是否也同步docker01的数据$ ls docker01.txt# 测试：可以删除docker01，查看一下docker02和docker03是否可以访问这个文件# 测试发现：数据依旧保留在docker02和docker03中没有被删除12345678 多个mysql实现数据共享 123456$ docker run -d -p 3306:3306 -v /home/mysql/conf:/etc/mysql/conf.d -v /home/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 --name mysql01 mysql:5.7$ docker run -d -p 3310:3306 -e MYSQL_ROOT_PASSWORD=123456 --name mysql02 --volumes-from mysql01 mysql:5.7# 这个时候，可以实现两个容器数据同步！12345 结论： 容器之间的配置信息的传递，数据卷容器的生命周期一直持续到没有容器使用为止。 但是一旦你持久化到了本地，这个时候，本地的数据是不会删除的！ DockerFileDockerFile介绍dockerfile是用来构建docker镜像的文件！命令参数脚本！ 构建步骤： 1、 编写一个dockerfile文件 2、 docker build 构建称为一个镜像 3、 docker run运行镜像 4、 docker push发布镜像（DockerHub 、阿里云仓库) 点击后跳到一个Dockerfile 很多官方镜像都是基础包，很多功能没有，我们通常会自己搭建自己的镜像！ 官方既然可以制作镜像，那我们也可以！ DockerFile构建过程基础知识： 1、每个保留关键字(指令）都是必须是大写字母 2、执行从上到下顺序 3、#表示注释 4、每一个指令都会创建提交一个新的镜像曾，并提交！ Dockerfile是面向开发的，我们以后要发布项目，做镜像，就需要编写dockerfile文件，这个文件十分简单！ Docker镜像逐渐成企业交付的标准，必须要掌握！ DockerFile：构建文件，定义了一切的步骤，源代码 DockerImages：通过DockerFile构建生成的镜像，最终发布和运行产品。 Docker容器：容器就是镜像运行起来提供服务。 DockerFile的指令12345678910111213FROM # from:基础镜像，一切从这里开始构建MAINTAINER # maintainer:镜像是谁写的， 姓名+邮箱RUN # run:镜像构建的时候需要运行的命令ADD # add:步骤，tomcat镜像，这个tomcat压缩包！添加内容 添加同目录WORKDIR # workdir:镜像的工作目录VOLUME # volume:挂载的目录EXPOSE # expose:保留端口配置CMD # cmd:指定这个容器启动的时候要运行的命令，只有最后一个会生效，可被替代ENTRYPOINT # entrypoint:指定这个容器启动的时候要运行的命令，可以追加命令ONBUILD # onbuild:当构建一个被继承DockerFile这个时候就会运行onbuild的指令，触发指令COPY # copy:类似ADD，将我们文件拷贝到镜像中ENV # env:构建的时候设置环境变量！123456789101112 实战测试scratch 镜像 12345678910111213141516FROM scratchADD centos-7-x86_64-docker.tar.xz /LABEL \ org.label-schema.schema-version="1.0" \ org.label-schema.name="CentOS Base Image" \ org.label-schema.vendor="CentOS" \ org.label-schema.license="GPLv2" \ org.label-schema.build-date="20200504" \ org.opencontainers.image.title="CentOS Base Image" \ org.opencontainers.image.vendor="CentOS" \ org.opencontainers.image.licenses="GPL-2.0-only" \ org.opencontainers.image.created="2020-05-04 00:00:00+01:00"CMD ["/bin/bash"]123456789101112131415 Docker Hub 中 99%的镜像都是从这个基础镜像过来的 FROM scratch，然后配置需要的软件和配置来进行构建。 创建一个自己的centos 12345678910111213141516171819202122232425262728# 1./home下新建dockerfile目录$ mkdir dockerfile# 2. dockerfile目录下新建mydockerfile-centos文件$ vim mydockerfile-centos# 3.编写Dockerfile配置文件FROM centos # 基础镜像是官方原生的centosMAINTAINER cao&lt;1165680007@qq.com&gt; # 作者ENV MYPATH /usr/local # 配置环境变量的目录 WORKDIR $MYPATH # 将工作目录设置为 MYPATHRUN yum -y install vim # 给官方原生的centos 增加 vim指令RUN yum -y install net-tools # 给官方原生的centos 增加 ifconfig命令EXPOSE 80 # 暴露端口号为80CMD echo $MYPATH # 输出下 MYPATH 路径CMD echo "-----end----" CMD /bin/bash # 启动后进入 /bin/bash# 4.通过这个文件构建镜像# 命令： docker build -f 文件路径 -t 镜像名:[tag] .$ docker build -f mydockerfile-centos -t mycentos:0.1 .# 5.出现下图后则构建成功123456789101112131415161718192021222324252627 123456789101112131415$ docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEmycentos 0.1 cbf5110a646d 2 minutes ago 311MB# 6.测试运行$ docker run -it mycentos:0.1 # 注意带上版本号，否则每次都回去找最新版latest$ pwd /usr/local # 与Dockerfile文件中 WORKDIR 设置的 MYPATH 一致$ vim # vim 指令可以使用$ ifconfig # ifconfig 指令可以使用# docker history 镜像id 查看镜像构建历史步骤$ docker history 镜像id1234567891011121314 我们可以列出本地进行的变更历史 我们平时拿到一个镜像，可以用 “docker history 镜像id” 研究一下是什么做的 CMD 和 ENTRYPOINT区别 123CMD # 指定这个容器启动的时候要运行的命令，只有最后一个会生效，可被替代。ENTRYPOINT # 指定这个容器启动的时候要运行的命令，可以追加命令12 测试cmd 1234567891011121314151617181920212223242526# 编写dockerfile文件$ vim dockerfile-test-cmdFROM centosCMD ["ls","-a"] # 启动后执行 ls -a 命令# 构建镜像$ docker build -f dockerfile-test-cmd -t cmd-test:0.1 .# 运行镜像$ docker run cmd-test:0.1 # 由结果可得，运行后就执行了 ls -a 命令....dockerenvbindevetchome# 想追加一个命令 -l 成为ls -al：展示列表详细数据$ docker run cmd-test:0.1 -ldocker: Error response from daemon: OCI runtime create failed: container_linux.go:349: starting container process caused "exec: \"-l\":executable file not found in $PATH": unknown.ERRO[0000] error waiting for container: context canceled # cmd的情况下 -l 替换了CMD["ls","-l"] 而 -l 不是命令所以报错12345678910111213141516171819202122232425 测试ENTRYPOINT 1234567891011121314151617181920212223242526272829303132333435# 编写dockerfile文件$ vim dockerfile-test-entrypointFROM centosENTRYPOINT ["ls","-a"]# 构建镜像$ docker build -f dockerfile-test-entrypoint -t cmd-test:0.1 .# 运行镜像$ docker run entrypoint-test:0.1....dockerenvbindevetchomeliblib64lost+found ...# 我们的命令，是直接拼接在我们得ENTRYPOINT命令后面的$ docker run entrypoint-test:0.1 -ltotal 56drwxr-xr-x 1 root root 4096 May 16 06:32 .drwxr-xr-x 1 root root 4096 May 16 06:32 ..-rwxr-xr-x 1 root root 0 May 16 06:32 .dockerenvlrwxrwxrwx 1 root root 7 May 11 2019 bin -&gt; usr/bindrwxr-xr-x 5 root root 340 May 16 06:32 devdrwxr-xr-x 1 root root 4096 May 16 06:32 etcdrwxr-xr-x 2 root root 4096 May 11 2019 homelrwxrwxrwx 1 root root 7 May 11 2019 lib -&gt; usr/liblrwxrwxrwx 1 root root 9 May 11 2019 lib64 -&gt; usr/lib64 ....12345678910111213141516171819202122232425262728293031323334 Dockerfile中很多命令都十分的相似，我们需要了解它们的区别，我们最好的学习就是对比他们然后测试效果！ 实战：Tomcat镜像1、准备镜像文件12准备tomcat 和 jdk 到当前目录，编写好README1 2、编写dokerfile123456789101112131415161718192021222324$ vim dockerfileFROM centos # 基础镜像centosMAINTAINER cao&lt;1165680007@qq.com&gt; # 作者COPY README /usr/local/README # 复制README文件ADD jdk-8u231-linux-x64.tar.gz /usr/local/ # 添加jdk，ADD 命令会自动解压ADD apache-tomcat-9.0.35.tar.gz /usr/local/ # 添加tomcat，ADD 命令会自动解压RUN yum -y install vim # 安装 vim 命令ENV MYPATH /usr/local # 环境变量设置 工作目录WORKDIR $MYPATHENV JAVA_HOME /usr/local/jdk1.8.0_231 # 环境变量： JAVA_HOME环境变量ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarENV CATALINA_HOME /usr/local/apache-tomcat-9.0.35 # 环境变量： tomcat环境变量ENV CATALINA_BASH /usr/local/apache-tomcat-9.0.35# 设置环境变量 分隔符是：ENV PATH $PATH:$JAVA_HOME/bin:$CATALINA_HOME/lib:$CATALINA_HOME/bin EXPOSE 8080 # 设置暴露的端口CMD /usr/local/apache-tomcat-9.0.35/bin/startup.sh &amp;&amp; tail -F /usr/local/apache-tomcat-9.0.35/logs/catalina.out # 设置默认命令1234567891011121314151617181920212223 3、构建镜像123# 因为dockerfile命名使用默认命名 因此不用使用-f 指定文件$ docker build -t mytomcat:0.1 .12 4、run镜像12345# -d:后台运行 -p:暴露端口 --name:别名 -v:绑定路径 $ docker run -d -p 8080:8080 --name tomcat01 -v /home/kuangshen/build/tomcat/test:/usr/local/apache-tomcat-9.0.35/webapps/test -v /home/kuangshen/build/tomcat/tomcatlogs/:/usr/local/apache-tomcat-9.0.35/logs mytomcat:0.11234 5、访问测试1234$ docker exec -it 自定义容器的id /bin/bash$ cul localhost:8080123 6、发布项目(由于做了卷挂载，我们直接在本地编写项目就可以发布了！) 发现：项目部署成功，可以直接访问！ 我们以后开发的步骤：需要掌握Dockerfile的编写！我们之后的一切都是使用docker镜像来发布运行！ 发布自己的镜像 发布到 Docker Hub 1、地址 https://hub.docker.com/ 2、确定这个账号可以登录 3、登录 12345678910111213$ docker login --helpUsage: docker login [OPTIONS] [SERVER]Log in to a Docker registry.If no server is specified, the default is defined by the daemon.Options: -p, --password string Password --password-stdin Take the password from stdin -u, --username string Username$ docker login -u 你的用户名 -p 你的密码123456789101112 4、提交 push镜像 123456789# 会发现push不上去，因为如果没有前缀的话默认是push到 官方的library# 解决方法：# 第一种 build的时候添加你的dockerhub用户名，然后在push就可以放到自己的仓库了$ docker build -t kuangshen/mytomcat:0.1 .# 第二种 使用docker tag #然后再次push$ docker tag 容器id kuangshen/mytomcat:1.0 #然后再次push$ docker push kuangshen/mytomcat:1.012345678 发布到 阿里云镜像服务上 看官网 很详细https://cr.console.aliyun.com/repository/ 12345678$ sudo docker login --username=zchengx registry.cn-shenzhen.aliyuncs.com$ sudo docker tag [ImageId] registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:[镜像版本号]# 修改id 和 版本sudo docker tag a5ef1f32aaae registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:1.0# 修改版本$ sudo docker push registry.cn-shenzhen.aliyuncs.com/dsadxzc/cheng:[镜像版本号]1234567 小结 Docker 网络理解Docker 0学习之前清空下前面的docker 镜像、容器 123456# 删除全部容器$ docker rm -f $(docker ps -aq)# 删除全部镜像$ docker rmi -f $(docker images -aq)12345 测试 三个网络 问题： docker 是如果处理容器网络访问的？ 123456789101112131415161718192021222324# 测试 运行一个tomcat$ docker run -d -P --name tomcat01 tomcat# 查看容器内部网络地址$ docker exec -it 容器id ip addr# 发现容器启动的时候会得到一个 eth0@if91 ip地址，docker分配！$ ip addr1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever261: eth0@if91: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.18.0.2/16 brd 172.18.255.255 scope global eth0 valid_lft forever preferred_lft forever # 思考？ linux能不能ping通容器内部！ 可以 容器内部可以ping通外界吗？ 可以！$ ping 172.18.0.2PING 172.18.0.2 (172.18.0.2) 56(84) bytes of data.64 bytes from 172.18.0.2: icmp_seq=1 ttl=64 time=0.069 ms64 bytes from 172.18.0.2: icmp_seq=2 ttl=64 time=0.074 ms1234567891011121314151617181920212223 原理 1、我们每启动一个docker容器，docker就会给docker容器分配一个ip，我们只要按照了docker，就会有一个docker0桥接模式，使用的技术是veth-pair技术！ https://www.cnblogs.com/bakari/p/10613710.html 再次测试 ip addr 2 、再启动一个容器测试，发现又多了一对网络 12345# 我们发现这个容器带来网卡，都是一对对的# veth-pair 就是一对的虚拟设备接口，他们都是成对出现的，一端连着协议，一端彼此相连# 正因为有这个特性 veth-pair 充当一个桥梁，连接各种虚拟网络设备的# OpenStac,Docker容器之间的连接，OVS的连接，都是使用evth-pair技术1234 3、我们来测试下tomcat01和tomcat02是否可以ping通 123456789101112131415# 获取tomcat01的ip 172.17.0.2$ docker-tomcat docker exec -it tomcat01 ip addr 550: eth0@if551: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet 172.17.0.2/16 brd 172.17.255.255 scope global eth0 valid_lft forever preferred_lft forever # 让tomcat02 ping tomcat01 $ docker-tomcat docker exec -it tomcat02 ping 172.17.0.2PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data.64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.098 ms64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.071 ms# 结论：容器和容器之间是可以互相ping通1234567891011121314 网络模型图 结论：tomcat01和tomcat02公用一个路由器，docker0。 所有的容器不指定网络的情况下，都是docker0路由的，docker会给我们的容器分配一个默认的可用ip。 小结 Docker使用的是Linux的桥接，宿主机是一个Docker容器的网桥 docker0 Docker中所有网络接口都是虚拟的，虚拟的转发效率高（内网传递文件） 只要容器删除，对应的网桥一对就没了！ 思考一个场景：我们编写了一个微服务，database url=ip: 项目不重启，数据ip换了，我们希望可以处理这个问题，可以通过名字来进行访问容器？ –-link1234567891011121314151617$ docker exec -it tomcat02 ping tomca01 # ping不通ping: tomca01: Name or service not known# 运行一个tomcat03 --link tomcat02 $ docker run -d -P --name tomcat03 --link tomcat02 tomcat5f9331566980a9e92bc54681caaac14e9fc993f14ad13d98534026c08c0a9aef# 3连接2# 用tomcat03 ping tomcat02 可以ping通$ docker exec -it tomcat03 ping tomcat02PING tomcat02 (172.17.0.3) 56(84) bytes of data.64 bytes from tomcat02 (172.17.0.3): icmp_seq=1 ttl=64 time=0.115 ms64 bytes from tomcat02 (172.17.0.3): icmp_seq=2 ttl=64 time=0.080 ms# 2连接3# 用tomcat02 ping tomcat03 ping不通12345678910111213141516 探究： docker network inspect 网络id 网段相同 docker inspect tomcat03 查看tomcat03里面的/etc/hosts发现有tomcat02的配置 –link 本质就是在hosts配置中添加映射 现在使用Docker已经不建议使用–link了！ 自定义网络，不适用docker0！ docker0问题：不支持容器名连接访问！ 自定义网络123456789docker networkconnect -- Connect a container to a networkcreate -- Creates a new network with a name specified by thedisconnect -- Disconnects a container from a networkinspect -- Displays detailed information on a networkls -- Lists all the networks created by the userprune -- Remove all unused networksrm -- Deletes one or more networks12345678 查看所有的docker网络 网络模式 bridge ：桥接 docker（默认，自己创建也是用bridge模式） none ：不配置网络，一般不用 host ：和所主机共享网络 container ：容器网络连通（用得少！局限很大） 测试 123456789# 我们直接启动的命令 --net bridge,而这个就是我们得docker0# bridge就是docker0$ docker run -d -P --name tomcat01 tomcat等价于 =&gt; docker run -d -P --name tomcat01 --net bridge tomcat# docker0，特点：默认，域名不能访问。 --link可以打通连接，但是很麻烦！# 我们可以 自定义一个网络$ docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet12345678 12$ docker network inspect mynet;1 启动两个tomcat,再次查看网络情况 在自定义的网络下，服务可以互相ping通，不用使用–link 我们自定义的网络docker当我们维护好了对应的关系，推荐我们平时这样使用网络！ 好处： redis -不同的集群使用不同的网络，保证集群是安全和健康的 mysql-不同的集群使用不同的网络，保证集群是安全和健康的 网络连通 12345# 测试两个不同的网络连通 再启动两个tomcat 使用默认网络，即docker0$ docker run -d -P --name tomcat01 tomcat$ docker run -d -P --name tomcat02 tomcat# 此时ping不通1234 123# 要将tomcat01 连通 tomcat—net-01 ，连通就是将 tomcat01加到 mynet网络# 一个容器两个ip（tomcat01）12 123# 01连通 ，加入后此时，已经可以tomcat01 和 tomcat-01-net ping通了# 02是依旧不通的12 结论：假设要跨网络操作别人，就需要使用docker network connect 连通！ 实战：部署Redis集群 123456789101112131415161718192021222324252627282930# 创建网卡docker network create redis --subnet 172.38.0.0/16# 通过脚本创建六个redis配置for port in $(seq 1 6);\do \mkdir -p /mydata/redis/node-$&#123;port&#125;/conftouch /mydata/redis/node-$&#123;port&#125;/conf/redis.confcat &lt;&lt; EOF &gt;&gt; /mydata/redis/node-$&#123;port&#125;/conf/redis.confport 6379bind 0.0.0.0cluster-enabled yescluster-config-file nodes.confcluster-node-timeout 5000cluster-announce-ip 172.38.0.1$&#123;port&#125;cluster-announce-port 6379cluster-announce-bus-port 16379appendonly yesEOFdone# 通过脚本运行六个redisfor port in $(seq 1 6);\docker run -p 637$&#123;port&#125;:6379 -p 1667$&#123;port&#125;:16379 --name redis-$&#123;port&#125; \-v /mydata/redis/node-$&#123;port&#125;/data:/data \-v /mydata/redis/node-$&#123;port&#125;/conf/redis.conf:/etc/redis/redis.conf \-d --net redis --ip 172.38.0.1$&#123;port&#125; redis:5.0.9-alpine3.11 redis-server /etc/redis/redis.confdocker exec -it redis-1 /bin/sh #redis默认没有bashredis-cli --cluster create 172.38.0.11:6379 172.38.0.12:6379 172.38.0.13:6379 172.38.0.14:6379 172.38.0.15:6379 172.38.0.16:6379 --cluster-replicas 11234567891011121314151617181920212223242526272829 docker搭建redis集群完成！ 我们使用docker之后，所有的技术都会慢慢变得简单起来！ SpringBoot微服务打包Docker镜像1、构建SpringBoot项目 2、打包运行 12mvn package1 3、编写dockerfile 123456FROM java:8COPY *.jar /app.jarCMD ["--server.port=8080"]EXPOSE 8080ENTRYPOINT ["java","-jar","app.jar"]12345 4、构建镜像 1234# 1.复制jar和DockerFIle到服务器# 2.构建镜像$ docker build -t xxxxx:xx .123 5、发布运行 以后我们使用了Docker之后，给别人交付就是一个镜像即可！]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 入门]]></title>
    <url>%2F2020%2F12%2F26%2FDocker%20%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[Docker 入门笔记整理来源 B站UP主狂神说Javahttps://space.bilibili.com/95256449/ Docker概述Docker概述Docker为什么出现？ 一款产品： 开发–上线 两套环境！应用环境，应用配置！ 开发 — 运维。 问题：我在我的电脑上可以允许！版本更新，导致服务不可用！对于运维来说考验十分大？ 环境配置是十分的麻烦，每一个及其都要部署环境(集群Redis、ES、Hadoop…) !费事费力。 发布一个项目( jar + (Redis MySQL JDK ES) ),项目能不能带上环境安装打包！ 之前在服务器配置一个应用的环境 Redis MySQL JDK ES Hadoop 配置超麻烦了，不能够跨平台。 开发环境Windows，最后发布到Linux！ 传统：开发jar，运维来做！ 现在：开发打包部署上线，一套流程做完！ 安卓流程：java — apk —发布（应用商店）一 张三使用apk一安装即可用！ docker流程： java-jar（环境） — 打包项目帯上环境（镜像） — ( Docker仓库：商店）——- Docker给以上的问题，提出了解决方案！Docker的思想就来自于集装箱！ JRE – 多个应用(端口冲突) – 原来都是交叉的！隔离：Docker核心思想！打包装箱！每个箱子是互相隔离的。 Docker通过隔离机制，可以将服务器利用到极致！ 本质：所有的技术都是因为出现了一些问题，我们需要去解决，才去学习！ Docker历史2010年，几个的年轻人，就在美国成立了一家公司 dotcloud 做一些pass的云计算服务！LXC（Linux Container容器）有关的容器技术！ Linux Container容器是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源。 他们将自己的技术（容器化技术）命名就是 DockerDocker刚刚延生的时候，没有引起行业的注意！dotCloud，就活不下去！ 开源 2013年，Docker开源！ 越来越多的人发现docker的优点！火了。Docker每个月都会更新一个版本！ 2014年4月9日，Docker1.0发布！ docker为什么这么火？十分的轻巧！ 在容器技术出来之前，我们都是使用虚拟机技术！ 虚拟机：在window中装一个VMware，通过这个软件我们可以虚拟出来一台或者多台电脑！笨重！ 虚拟机也属于虚拟化技术，Docker容器技术，也是一种虚拟化技术！ 123VMware : linux centos 原生镜像（一个电脑！） 隔离、需要开启多个虚拟机！ 几个G 几分钟docker: 隔离，镜像（最核心的环境 4m + jdk + mysql）十分的小巧，运行镜像就可以了！小巧！ 几个M 秒级启动！12 聊聊Docker Docker基于Go语言开发的！开源项目！ docker官网：https://www.docker.com/ 文档：https://docs.docker.com/ Docker的文档是超级详细的！ 仓库：https://hub.docker.com/ Docker能干嘛 之前的虚拟机技术虚拟机技术缺点： 1、 资源占用十分多 2、 冗余步骤多 3、 启动很慢！ 容器化技术 容器化技术不是模拟一个完整的操作系统 比较Docker和虚拟机技术的不同： 传统虚拟机，虚拟出一条硬件，运行一个完整的操作系统，然后在这个系统上安装和运行软件 容器内的应用直接运行在宿主机的内容，容器是没有自己的内核的，也没有虚拟我们的硬件，所以就轻便了 每个容器间是互相隔离，每个容器内都有一个属于自己的文件系统，互不影响 DevOps（开发、运维） 应用更快速的交付和部署 传统：一对帮助文档，安装程序。 Docker：打包镜像发布测试一键运行。 更便捷的升级和扩缩容 使用了 Docker之后，我们部署应用就和搭积木一样项目打包为一个镜像，扩展服务器A！服务器B 更简单的系统运维在容器化之后，我们的开发，测试环境都是高度一致的 更高效的计算资源利用 Docker是内核级别的虚拟化，可以在一个物理机上可以运行很多的容器实例！服务器的性能可以被压榨到极致。 Docker安装Docker的基本组成 镜像（image)： docker镜像就好比是一个目标，可以通过这个目标来创建容器服务，tomcat镜像==&gt;run==&gt;容器（提供服务器），通过这个镜像可以创建多个容器（最终服务运行或者项目运行就是在容器中的）。 容器(container)： Docker利用容器技术，独立运行一个或者一组应用，通过镜像来创建的.启动，停止，删除，基本命令目前就可以把这个容器理解为就是一个简易的 Linux系统。 仓库(repository)： 仓库就是存放镜像的地方！仓库分为公有仓库和私有仓库。(很类似git)Docker Hub是国外的。阿里云…都有容器服务器(配置镜像加速!) 安装Docker 环境准备 1.Linux要求内核3.0以上 2.CentOS 7 123456789101112131415161718[root@iz2zeak7sgj6i7hrb2g862z ~]# uname -r3.10.0-514.26.2.el7.x86_64 # 要求3.0以上[root@iz2zeak7sgj6i7hrb2g862z ~]# cat /etc/os-release NAME="CentOS Linux"VERSION="7 (Core)"ID="centos"ID_LIKE="rhel fedora"VERSION_ID="7"PRETTY_NAME="CentOS Linux 7 (Core)"ANSI_COLOR="0;31"CPE_NAME="cpe:/o:centos:centos:7"HOME_URL="https://www.centos.org/"BUG_REPORT_URL="https://bugs.centos.org/"CENTOS_MANTISBT_PROJECT="CentOS-7"CENTOS_MANTISBT_PROJECT_VERSION="7"REDHAT_SUPPORT_PRODUCT="centos"REDHAT_SUPPORT_PRODUCT_VERSION="7" 安装 帮助文档：https://docs.docker.com/engine/install/卸载与安装 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#1.卸载旧版本sudo yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-engine#2.需要的安装包sudo yum install -y yum-utils#3.设置镜像的仓库sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo#上述方法默认是从国外的，不推荐#推荐使用国内的sudo yum-config-manager \ --add-repo \ https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #更新yum软件包索引sudo yum makecache fast#4.安装docker相关的 docker-ce 社区版 而ee是企业版sudo yum install docker-ce docker-ce-cli containerd.io # 这里我们使用社区版即可#5.启动dockersudo systemctl start docker#6. 使用docker version查看是否按照成功sudo docker version#7. 测试sudo docker run hello-world#8.查看已经下载的镜像(从这里可以查看已有镜像的id)[root@iz2zeak7sgj6i7hrb2g862z ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 4 months ago 13.3kB#9.关闭、重启dockersudo systemctl stop dockersudo systemctl restart docker 卸载docker 12345#1. 卸载依赖sudo yum remove docker-ce docker-ce-cli containerd.io#2. 删除资源rm -rf /var/lib/docker# /var/lib/docker 是docker的默认工作路径！ 阿里云镜像加速当上下载镜像非常慢时，可以使用阿里云镜像来加速： 1、配置使用12345678910111213#1.创建一个目录sudo mkdir -p /etc/docker#2.编写配置文件sudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://t2wwyxhb.mirror.aliyuncs.com"]&#125;EOF#3.重启服务sudo systemctl daemon-reloadsudo systemctl restart docker 回顾HelloWorld流程 docker run 流程图 底层原理Docker是怎么工作的？ Docker是一个Client-Server结构的系统，Docker的守护进程运行在主机上。通过Socket从客户端访问！ Docker-Server接收到Docker-Client的指令，就会执行这个命令！ 为什么Docker比Vm快1、docker有着比虚拟机更少的抽象层。由于docker不需要Hypervisor实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。2、docker利用的是宿主机的内核,而不需要Guest OS。 123GuestOS： VM（虚拟机）里的的系统（OS）HostOS：物理机里的系统（OS） 因此,当新建一个 容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。仍而避免引导、加载操作系统内核返个比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载GuestOS,返个新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了这个复杂的过程,因此新建一个docker容器只需要几秒钟。 Docker的常用命令1.帮助命令123docker version #显示docker的版本信息。docker info #显示docker的系统信息，包括镜像和容器的数量docker 命令 --help #帮助命令 帮助文档的地址：https://docs.docker.com/engine/reference/commandline/build/ 2.镜像命令1234567docker images #查看所有本地主机上的镜像 可以使用docker image ls代替docker search #搜索镜像docker pull #下载镜像 docker image pulldocker rmi #删除镜像 docker image rm docker images查看所有本地的主机上的镜像1234567891011121314151617181920212223[root@iz2zeak7sgj6i7hrb2g862z ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEhello-world latest bf756fb1ae65 4 months ago 13.3kBmysql 5.7 b84d68d0a7db 6 days ago 448MB# 解释#REPOSITORY # 镜像的仓库源#TAG # 镜像的标签(版本) ---lastest 表示最新版本#IMAGE ID # 镜像的id#CREATED # 镜像的创建时间#SIZE # 镜像的大小# 可选项Options: -a, --all Show all images (default hides intermediate images) #列出所有镜像 -q, --quiet Only show numeric IDs # 只显示镜像的id [root@iz2zeak7sgj6i7hrb2g862z ~]# docker images -a #列出所有镜像详细信息[root@iz2zeak7sgj6i7hrb2g862z ~]# docker images -aq #列出所有镜像的idd5f28a0bb0d0f19c56ce92a81b6b1fe7261e1b6b1fe7261e docker search 搜索镜像 12345678910111213[root@iz2zeak7sgj6i7hrb2g862z ~]# docker search mysql# --filter=STARS=3000 #过滤，搜索出来的镜像收藏STARS数量大于3000的Options: -f, --filter filter Filter output based on conditions provided --format string Pretty-print search using a Go template --limit int Max number of search results (default 25) --no-trunc Don't truncate output [root@iz2zeak7sgj6i7hrb2g862z ~]# docker search mysql --filter=STARS=3000NAME DESCRIPTION STARS OFFICIAL AUTOMATEDmysql MySQL IS ... 9520 [OK] mariadb MariaDB IS ... 3456 [OK] docker pull 下载镜像1234567891011121314151617181920# 下载镜像 docker pull 镜像名[:tag][root@iz2zeak7sgj6i7hrb2g862z ~]# docker pull tomcat:88: Pulling from library/tomcat #如果不写tag，默认就是latest90fe46dd8199: Already exists #分层下载： docker image 的核心 联合文件系统35a4f1977689: Already exists bbc37f14aded: Already exists 74e27dc593d4: Already exists 93a01fbfad7f: Already exists 1478df405869: Pull complete 64f0dd11682b: Pull complete 68ff4e050d11: Pull complete f576086003cf: Pull complete 3b72593ce10e: Pull complete Digest: sha256:0c6234e7ec9d10ab32c06423ab829b32e3183ba5bf2620ee66de866df # 签名防伪Status: Downloaded newer image for tomcat:8docker.io/library/tomcat:8 #真实地址#等价于docker pull tomcat:8docker pull docker.io/library/tomcat:8 docker rmi 删除镜像12345docker rmi -f 镜像id #删除指定id的镜像[root@iz2zeak7sgj6i7hrb2g862z ~]# docker rmi -f f19c56ce92a8docker rmi -f $(docker images -aq) #删除全部的镜像[root@iz2zeak7sgj6i7hrb2g862z ~]# docker stop $(docker ps -a -q) 3.容器命令说明：我们有了镜像才可以创建容器，Linux，下载centos镜像来学习 镜像下载123456789101112131415#docker中下载centosdocker pull centosdocker run 镜像id #新建容器并启动docker ps 列出所有运行的容器 docker container listdocker rm 容器id #删除指定容器docker start 容器id #启动容器docker restart 容器id #重启容器docker stop 容器id #停止当前正在运行的容器docker kill 容器id #强制停止当前容器[root@iz2zeak7sgj6i7hrb2g862z ~]# docker container list #h和docker ps相同 新建容器并启动12345678910111213141516171819docker run [可选参数] image | docker container run [可选参数] image #参书说明--name="Name" #容器名字 tomcat01 tomcat02 用来区分容器-d #后台方式运行-it #使用交互方式运行，进入容器查看内容-p #指定容器的端口 -p 8080(宿主机):8080(容器) -p ip:主机端口:容器端口 -p 主机端口:容器端口(常用) -p 容器端口 容器端口-P(大写) 随机指定端口# 测试、启动并进入容器[root@iz2zeak7sgj6i7hrb2g[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -it centos /bin/bash[root@241b5abce65e /]# lsbin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var[root@241b5abce65e /]# exit #从容器退回主机exit 列出所有运行的容器1234docker ps 命令 #列出当前正在运行的容器 -a, --all #列出当前正在运行的容器 + 带出历史运行过的容器 -n=?, --last int #列出最近创建的?个容器 ?为1则只列出最近创建的一个容器,为2则列出2个 -q, --quiet #只列出容器的编号 退出容器12exit #直接停止容器退出ctrl +P +Q #容器不停止退出 ---注意：这个很有用的操作 删除容器123docker rm 容器id #删除指定的容器，不能删除正在运行的容器，如果要强制删除 rm -rfdocker rm -f $(docker ps -aq) #删除所有的容器docker ps -a -q|xargs docker rm #删除所有的容器 启动和停止容器的操作1234docker start 容器id #启动容器docker restart 容器id #重启容器docker stop 容器id #停止当前正在运行的容器docker kill 容器id #强制停止当前容器 4.常用其他命令后台启动命令123456789# 命令 docker run -d 镜像名[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d centosa8f922c255859622ac45ce3a535b7a0e8253329be4756ed6e32265d2dd2fac6c[root@iz2zeak7sgj6i7hrb2g862z ~]# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES# 问题docker ps. 发现centos 停止了# 常见的坑，docker容器使用后台运行，就必须要有要一个前台进程，docker发现没有应用，就会自动停止# nginx，容器启动后，发现自己没有提供服务，就会立刻停止，就是没有程序了 查看日志1234567891011121314docker logs --helpOptions: --details Show extra details provided to logs * -f, --follow Follow log output --since string Show logs since timestamp (e.g. 2013-01-02T13:23:37) or relative (e.g. 42m for 42 minutes)* --tail string Number of lines to show from the end of the logs (default "all")* -t, --timestamps Show timestamps --until string Show logs before a timestamp (e.g. 2013-01-02T13:23:37) or relative (e.g. 42m for 42 minutes)➜ ~ docker run -d centos /bin/sh -c "while true;do echo 6666;sleep 1;done" #模拟日志 #显示日志-tf #显示日志信息（一直更新）--tail number #需要显示日志条数docker logs -t --tail n 容器id #查看n行日志docker logs -ft 容器id #跟着日志 查看容器中进程信息ps1# 命令 docker top 容器id 查看镜像的元数据12345# 命令docker inspect 容器id#测试➜ ~ docker inspect 55321bcae33d 进入当前正在运行的容器我们通常容器都是使用后台方式运行的，需要进入容器，修改一些配置命令docker exec -it 容器id bashshell #测试➜ ~ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES55321bcae33d centos “/bin/sh -c ‘while t…” 10 minutes ago Up 10 minutes bold_bella7215824a4db centos “/bin/sh -c ‘while t…” 13 minutes ago Up 13 minutes zen_kepler55a31b3f8613 centos “/bin/bash” 15 minutes ago Up 15 minutes lucid_clarke➜ ~ docker exec -it 55321bcae33d /bin/bash[root@55321bcae33d /]# 12345678# 方式二docker attach 容器id#测试docker attach 55321bcae33d 正在执行当前的代码...区别#docker exec #进入当前容器后开启一个新的终端，可以在里面操作。（常用）#docker attach # 进入容器正在执行的终端 从容器内拷贝到主机上 1234567891011121314151617181920212223242526docker cp 容器id:容器内路径 主机目的路径[root@iz2zeak7sgj6i7hrb2g862z ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES56a5583b25b4 centos "/bin/bash" 7seconds ago Up 6 seconds #1. 进入docker容器内部[root@iz2zeak7sgj6i7hrb2g862z ~]# docker exec -it 56a5583b25b4 /bin/bash[root@55321bcae33d /]# lsbin dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var#新建一个文件[root@55321bcae33d /]# echo "hello" &gt; java.java[root@55321bcae33d /]# cat hello.java hello[root@55321bcae33d /]# exitexit#hello.java拷贝到home文件加下[root@iz2zeak7sgj6i7hrb2g862z /]# docker cp 56a5583b25b4:/hello.java /home [root@iz2zeak7sgj6i7hrb2g862z /]# cd /home[root@iz2zeak7sgj6i7hrb2g862z home]# ls -l #可以看见java.java存在total 8-rw-r--r-- 1 root root 0 May 19 22:09 haust.java-rw-r--r-- 1 root root 6 May 22 11:12 java.javadrwx------ 3 www www 4096 May 8 12:14 www 学习方式：将我的所有笔记敲一遍，自己记录笔记！ 小结：命令大全 1234567891011121314151617181920212223242526272829303132333435363738394041attach Attach local standard input, output, and error streams to a running container#当前shell下 attach连接指定运行的镜像build Build an image from a Dockerfile # 通过Dockerfile定制镜像commit Create a new image from a container's changes #提交当前容器为新的镜像cp Copy files/folders between a container and the local filesystem #拷贝文件create Create a new container #创建一个新的容器diff Inspect changes to files or directories on a container's filesystem #查看docker容器的变化events Get real time events from the server # 从服务获取容器实时时间exec Run a command in a running container # 在运行中的容器上运行命令export Export a container's filesystem as a tar archive #导出容器文件系统作为一个tar归档文件[对应import]history Show the history of an image # 展示一个镜像形成历史images List images #列出系统当前的镜像import Import the contents from a tarball to create a filesystem image #从tar包中导入内容创建一个文件系统镜像info Display system-wide information # 显示全系统信息inspect Return low-level information on Docker objects #查看容器详细信息kill Kill one or more running containers # kill指定docker容器load Load an image from a tar archive or STDIN #从一个tar包或标准输入中加载一个镜像[对应save]login Log in to a Docker registry #logout Log out from a Docker registrylogs Fetch the logs of a containerpause Pause all processes within one or more containersport List port mappings or a specific mapping for the containerps List containerspull Pull an image or a repository from a registrypush Push an image or a repository to a registryrename Rename a containerrestart Restart one or more containersrm Remove one or more containersrmi Remove one or more imagesrun Run a command in a new containersave Save one or more images to a tar archive (streamed to STDOUT by default)search Search the Docker Hub for imagesstart Start one or more stopped containersstats Display a live stream of container(s) resource usage statisticsstop Stop one or more running containerstag Create a tag TARGET_IMAGE that refers to SOURCE_IMAGEtop Display the running processes of a containerunpause Unpause all processes within one or more containersupdate Update configuration of one or more containersversion Show the Docker version informationwait Block until one or more containers stop, then print their exit codes 作业练习 作业一：Docker 安装Nginx 1234567891011121314151617181920212223242526272829303132333435363738#1. 搜索镜像 search 建议大家去docker搜索，可以看到帮助文档[root@iz2zeak7sgj6i7hrb2g862z ~]# docker search nginx#2. 拉取下载镜像 pull[root@iz2zeak7sgj6i7hrb2g862z ~]# docker pull nginx#3. 查看是否下载成功镜像[root@iz2zeak7sgj6i7hrb2g862z ~]# docker images#3. 运行测试# -d 后台运行# --name 给容器命名# -p 宿主机端口：容器内部端口[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d --name nginx01 -p 3344:80 nginxaa664b0c8ed98f532453ce1c599be823bcc1f3c9209e5078615af416ccb454c2#4. 查看正在启动的镜像[root@iz2zeak7sgj6i7hrb2g862z ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES75943663c116 nginx "nginx -g 'daemon of…" 41 seconds ago Up 40 seconds 0.0.0.0:82-&gt;80/tcp nginx00#5. 进入容器[root@iz2zeak7sgj6i7hrb2g862z ~]# docker exec -it nginx01 /bin/bash #进入root@aa664b0c8ed9:/# whereis nginx #找到nginx位置nginx: /usr/sbin/nginx /usr/lib/nginx /etc/nginx /usr/share/nginxroot@aa664b0c8ed9:/# cd /etc/nginx/root@aa664b0c8ed9:/etc/nginx# lsconf.d fastcgi_params koi-utf koi-win mime.types modules nginx.conf scgi_params uwsgi_params win-utf#6. 退出容器root@aa664b0c8ed9:/etc/nginx# exitexit#7. 停止容器[root@iz2zeak7sgj6i7hrb2g862z ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESaa664b0c8ed9 nginx "nginx -g 'daemon of…" 10 minutes ago Up 10 minutes 0.0.0.0:3344-&gt;80/tcp nginx01[root@iz2zeak7sgj6i7hrb2g862z ~]# docker stop aa664b0c8ed9 宿主机端口 和 容器内部端口 以及端口暴露： 问题：我们每次改动nginx配置文件，都需要进入容器内部？十分麻烦，我要是可以在容器外部提供一个映射路径，达到在容器外部修改文件名，容器内部就可以自动修改？-v 数据卷 技术！ 作业二：用docker 来装一个tomcat 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 下载 tomcat9.0# 之前的启动都是后台，停止了容器，容器还是可以查到， docker run -it --rm 镜像名 一般是用来测试，用完就删除[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -it --rm tomcat:9.0--rm Automatically remove the container when it exits 用完即删#下载 最新版[root@iz2zeak7sgj6i7hrb2g862z ~]# docker pull tomcat#查看下载的镜像[root@iz2zeak7sgj6i7hrb2g862z ~]# docker images#以后台方式，暴露端口方式，启动运行[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d -p 8080:8080 --name tomcat01 tomcat#测试访问有没有问题curl localhost:8080#根据容器id进入tomcat容器[root@iz2zeak7sgj6i7hrb2g862z ~]# docker exec -it 645596565d3f /bin/bashroot@645596565d3f:/usr/local/tomcat# #查看tomcat容器内部内容：root@645596565d3f:/usr/local/tomcat# ls -ltotal 152-rw-r--r-- 1 root root 18982 May 5 20:40 BUILDING.txt-rw-r--r-- 1 root root 5409 May 5 20:40 CONTRIBUTING.md-rw-r--r-- 1 root root 57092 May 5 20:40 LICENSE-rw-r--r-- 1 root root 2333 May 5 20:40 NOTICE-rw-r--r-- 1 root root 3255 May 5 20:40 README.md-rw-r--r-- 1 root root 6898 May 5 20:40 RELEASE-NOTES-rw-r--r-- 1 root root 16262 May 5 20:40 RUNNING.txtdrwxr-xr-x 2 root root 4096 May 16 12:05 bindrwxr-xr-x 1 root root 4096 May 21 11:04 confdrwxr-xr-x 2 root root 4096 May 16 12:05 libdrwxrwxrwx 1 root root 4096 May 21 11:04 logsdrwxr-xr-x 2 root root 4096 May 16 12:05 native-jni-libdrwxrwxrwx 2 root root 4096 May 16 12:05 tempdrwxr-xr-x 2 root root 4096 May 16 12:05 webappsdrwxr-xr-x 7 root root 4096 May 5 20:37 webapps.distdrwxrwxrwx 2 root root 4096 May 5 20:36 workroot@645596565d3f:/usr/local/tomcat# #进入webapps目录root@645596565d3f:/usr/local/tomcat# cd webappsroot@645596565d3f:/usr/local/tomcat/webapps# lsroot@645596565d3f:/usr/local/tomcat/webapps# # 发现问题：1、linux命令少了。 2.webapps目录为空 # 原因：阿里云镜像的原因，阿里云默认是最小的镜像，所以不必要的都剔除掉# 保证最小可运行的环境！# 解决方案：# 将webapps.dist下的文件都拷贝到webapps下即可root@645596565d3f:/usr/local/tomcat# ls 找到webapps.distBUILDING.txt LICENSE README.md RUNNING.txt conf logs temp webapps.distCONTRIBUTING.md NOTICE RELEASE-NOTES bin lib native-jni-lib webapps workroot@645596565d3f:/usr/local/tomcat# cd webapps.dist/ # 进入webapps.dist root@645596565d3f:/usr/local/tomcat/webapps.dist# ls # 查看内容ROOT docs examples host-manager managerroot@645596565d3f:/usr/local/tomcat/webapps.dist# cd ..root@645596565d3f:/usr/local/tomcat# cp -r webapps.dist/* webapps # 拷贝webapps.dist 内容给webappsroot@645596565d3f:/usr/local/tomcat# cd webapps #进入webappsroot@645596565d3f:/usr/local/tomcat/webapps# ls #查看拷贝结果ROOT docs examples host-manager manager 这样docker部署tomcat就可以访问了问题:我们以后要部署项目，如果每次都要进入容器是不是十分麻烦？要是可以在容器外部提供一个映射路径，比如webapps，我们在外部放置项目，就自动同步内部就好了！ 作业三：部署elasticsearch+kibana 123456789101112131415161718192021222324252627282930313233# es 暴露的端口很多！# es 十分耗内存# es 的数据一般需要放置到安全目录！挂载# --net somenetwork ? 网络配置# 启动elasticsearch[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" elasticsearch:7.6.2# 测试一下es是否成功启动➜ ~ curl localhost:9200&#123; "name" : "d73ad2f22dd3", "cluster_name" : "docker-cluster", "cluster_uuid" : "atFKgANxS8CzgIyCB8PGxA", "version" : &#123; "number" : "7.6.2", "build_flavor" : "default", "build_type" : "docker", "build_hash" : "ef48eb35cf30adf4db14086e8aabd07ef6fb113f", "build_date" : "2020-03-26T06:34:37.794943Z", "build_snapshot" : false, "lucene_version" : "8.4.0", "minimum_wire_compatibility_version" : "6.8.0", "minimum_index_compatibility_version" : "6.0.0-beta1" &#125;, "tagline" : "You Know, for Search"&#125;#测试成功就关掉elasticSearch，防止耗内存[root@iz2zeak7sgj6i7hrb2g862z ~]# docker stop d834ce2bd306d834ce2bd306[root@iz2zeak7sgj6i7hrb2g862z ~]# docker stats # 查看docker容器使用内存情况 1234#测试成功就关掉elasticSearch，可以添加内存的限制，修改配置文件 -e 环境配置修改➜ ~ docker rm -f d73ad2f22dd3 # stop命令也行 ➜ ~ docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" -e ES_JAVA_OPTS="-Xms64m -Xmx512m" elasticsearch:7.6.2123 123456789101112131415161718➜ ~ curl localhost:9200&#123; "name" : "b72c9847ec48", "cluster_name" : "docker-cluster", "cluster_uuid" : "yNAK0EORSvq3Wtaqe2QqAg", "version" : &#123; "number" : "7.6.2", "build_flavor" : "default", "build_type" : "docker", "build_hash" : "ef48eb35cf30adf4db14086e8aabd07ef6fb113f", "build_date" : "2020-03-26T06:34:37.794943Z", "build_snapshot" : false, "lucene_version" : "8.4.0", "minimum_wire_compatibility_version" : "6.8.0", "minimum_index_compatibility_version" : "6.0.0-beta1" &#125;, "tagline" : "You Know, for Search"&#125; 作业三：使用kibana连接es (elasticSearch)？思考网络如何才能连接 Portainer 可视化面板安装 portainer(先用这个) 12docker run -d -p 8080:9000 \--restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainer Rancher(CI/CD再用)什么是portainer？ Docker图形化界面管理工具！提供一个后台面板供我们操作！ 1234567891011# 安装命令[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d -p 8080:9000 \&gt; --restart=always -v /var/run/docker.sock:/var/run/docker.sock --privileged=true portainer/portainerUnable to find image 'portainer/portainer:latest' locallylatest: Pulling from portainer/portainerd1e017099d17: Pull complete a7dca5b5a9e8: Pull complete Digest: sha256:4ae7f14330b56ffc8728e63d355bc4bc7381417fa45ba0597e5dd32682901080Status: Downloaded newer image for portainer/portainer:latest81753869c4fd438cec0e31659cbed0d112ad22bbcfcb9605483b126ee8ff306d 测试访问： 外网：8080 ：http://123.56.247.59:8080/进入之后的面板 镜像原理之联合文件系统镜像是什么镜像是一种轻量级、可执行的独立软件保，用来打包软件运行环境和基于运行环境开发的软件，他包含运行某个软件所需的所有内容，包括代码、运行时库、环境变量和配置文件。 所有应用，直接打包docker镜像，就可以直接跑起来！ 如何得到镜像 从远程仓库下载 别人拷贝给你 自己制作一个镜像 DockerFile Docker镜像加载原理 UnionFs （联合文件系统） UnionFs（联合文件系统）：Union文件系统（UnionFs）是一种分层、轻量级并且高性能的文件系统，他支持对文件系统的修改作为一次提交来一层层的叠加，同时可以将不同目录挂载到同一个虚拟文件系统下（ unite several directories into a single virtual filesystem)。Union文件系统是 Docker镜像的基础。镜像可以通过分层来进行继承，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录。 Docker镜像加载原理 docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。boots(boot file system）主要包含 bootloader和 Kernel, bootloader主要是引导加 kernel, Linux刚启动时会加bootfs文件系统，在 Docker镜像的最底层是 boots。这一层与我们典型的Linux/Unix系统是一样的，包含boot加載器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由 bootfs转交给内核，此时系统也会卸载bootfs。rootfs（root file system),在 bootfs之上。包含的就是典型 Linux系统中的/dev,/proc,/bin,/etc等标准目录和文件。 rootfs就是各种不同的操作系统发行版，比如 Ubuntu, Centos等等。 平时我们安装进虚拟机的CentOS都是好几个G，为什么Docker这里才200M？ 对于个精简的OS,rootfs可以很小，只需要包合最基本的命令，工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供rootfs就可以了。由此可见对于不同的Linux发行版， boots基本是一致的， rootfs会有差別，因此不同的发行版可以公用bootfs. 虚拟机是分钟级别，容器是秒级！ 分层理解 分层的镜像 我们可以去下载一个镜像，注意观察下载的日志输出，可以看到是一层层的在下载 思考：为什么Docker镜像要采用这种分层的结构呢？ 最大的好处，我觉得莫过于资源共享了！比如有多个镜像都从相同的Base镜像构建而来，那么宿主机只需在磁盘上保留一份base镜像，同时内存中也只需要加载一份base镜像，这样就可以为所有的容器服务了，而且镜像的每一层都可以被共享。 查看镜像分层的方式可以通过docker image inspect 命令 1➜ / docker image inspect redis 理解： 所有的 Docker镜像都起始于一个基础镜像层，当进行修改或培加新的内容时，就会在当前镜像层之上，创建新的镜像层。 举一个简单的例子，假如基于 Ubuntu Linux16.04创建一个新的镜像，这就是新镜像的第一层；如果在该镜像中添加 Python包，就会在基础镜像层之上创建第二个镜像层；如果继续添加一个安全补丁，就会创健第三个镜像层该像当前已经包含3个镜像层，如下图所示（这只是一个用于演示的很简单的例子）。 在添加额外的镜像层的同时，镜像始终保持是当前所有镜像的组合，理解这一点. 在添加额外的镜像层的同时，镜像始终保持是当前所有镜像的组合，理解这一点非常重要。下图中举了一个简单的例子，每个镜像层包含3个文件，而镜像包含了来自两个镜像层的6个文件。 上图中的镜像层跟之前图中的略有区別，主要目的是便于展示文件下图中展示了一个稍微复杂的三层镜像，在外部看来整个镜像只有6个文件，这是因为最上层中的文件7是文件5的一个更新版。 文种情況下，上层镜像层中的文件覆盖了底层镜像层中的文件。这样就使得文件的更新版本作为一个新镜像层添加到镜像当中 Docker通过存储引擎（新版本采用快照机制）的方式来实现镜像层堆栈，并保证多镜像层对外展示为统一的文件系统 Linux上可用的存储引撃有AUFS、 Overlay2、 Device Mapper、Btrfs以及ZFS。顾名思义，每种存储引擎都基于 Linux中对应的件系统或者块设备技术，井且每种存储引擎都有其独有的性能特点。 Docker在 Windows上仅支持 windowsfilter 一种存储引擎，该引擎基于NTFS文件系统之上实现了分层和CoW [1]。 下图展示了与系统显示相同的三层镜像。所有镜像层堆并合井，对外提供统一的视图。 特点 Docker 镜像都是只读的，当容器启动时，一个新的可写层加载到镜像的顶部！ 这一层就是我们通常说的容器层，容器之下的都叫镜像层！ commit镜像1234docker commit 提交容器成为一个新的副本# 命令和git原理类似docker commit -m="描述信息" -a="作者" 容器id 目标镜像名:[版本TAG] 实战测试 1234567891011121314151617181920212223242526272829303132# 1、启动一个默认的tomcat[root@iz2zeak7sgj6i7hrb2g862z ~]# docker run -d -p 8080:8080 tomcatde57d0ace5716d27d0e3a7341503d07ed4695ffc266aef78e0a855b270c4064e# 2、发现这个默认的tomcat 是没有webapps应用，官方的镜像默认webapps下面是没有文件的！#docker exec -it 容器id /bin/bash[root@iz2zeak7sgj6i7hrb2g862z ~]# docker exec -it de57d0ace571 /bin/bashroot@de57d0ace571:/usr/local/tomcat# # 3、从webapps.dist拷贝文件进去webapproot@de57d0ace571:/usr/local/tomcat# cp -r webapps.dist/* webappsroot@de57d0ace571:/usr/local/tomcat# cd webappsroot@de57d0ace571:/usr/local/tomcat/webapps# lsROOT docs examples host-manager manager# 4、将操作过的容器通过commit调教为一个镜像！我们以后就使用我们修改过的镜像即可，而不需要每次都重新拷贝webapps.dist下的文件到webapps了，这就是我们自己的一个修改的镜像。docker commit -m="描述信息" -a="作者" 容器id 目标镜像名:[TAG]docker commit -a="kuangshen" -m="add webapps app" 容器id tomcat02:1.0[root@iz2zeak7sgj6i7hrb2g862z ~]# docker commit -a="csp提交的" -m="add webapps app" de57d0ace571 tomcat02.1.0sha256:d5f28a0bb0d0b6522fdcb56f100d11298377b2b7c51b9a9e621379b01cf1487e[root@iz2zeak7sgj6i7hrb2g862z ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEtomcat02.1.0 latest d5f28a0bb0d0 14 seconds ago 652MBtomcat latest 1b6b1fe7261e 5 days ago 647MBnginx latest 9beeba249f3e 5 days ago 127MBmysql 5.7 b84d68d0a7db 5 days ago 448MBelasticsearch 7.6.2 f29a1ee41030 8 weeks ago 791MBportainer/portainer latest 2869fc110bf7 2 months ago 78.6MBcentos latest 470671670cac 4 months ago 237MBhello-world latest bf756fb1ae65 4 months ago 13.3kB 如果你想要保存当前容器的状态，就可以通过commit来提交，获得一个镜像，就好比我们我们使用虚拟机的快照]]></content>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[单例模式之双重锁模式：为什么要加双重锁？为什么要加volatile？]]></title>
    <url>%2F2020%2F09%2F16%2F%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%E4%B9%8B%E5%8F%8C%E9%87%8D%E9%94%81%E6%A8%A1%E5%BC%8F%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%A0%E5%8F%8C%E9%87%8D%E9%94%81%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%8A%A0volatile%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[单例模式介绍单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 这种模式涉及到一个单一的类，该类负责创建自己的对象，同时确保只有单个对象被创建。这个类提供了一种访问其唯一的对象的方式，可以直接访问，不需要实例化该类的对象。 注意： 1、单例类只能有一个实例。 2、单例类必须自己创建自己的唯一实例。 3、单例类必须给所有其他对象提供这一实例。 优点： 1、在内存里只有一个实例，减少了内存的开销，尤其是频繁的创建和销毁实例（比如管理学院首页页面缓存）。 2、避免对资源的多重占用（比如写文件操作）。 缺点：没有接口，不能继承，与单一职责原则冲突，一个类应该只关心内部逻辑，而不关心外面怎么样来实例化。 使用场景： 1、要求生产唯一序列号。 2、WEB 中的计数器，不用每次刷新都在数据库里加一次，用单例先缓存起来。 3、创建的一个对象需要消耗的资源过多，比如 I/O 与数据库的连接等。 实现123456789101112131415161718192021222324252627public class Singleton &#123; /** * 定义一个私有变量来存放单例，私有的目的是指外部无法直接获取这个变量，而要使用提供的私有方法来获取 * volatile是用来防止指令重排 */ private volatile static Singleton instance = null; /** * 定义私有构造方法，表示只在类内部使用，亦指单例的实例只能在单例类内部创建，外部不能new */ private Singleton() &#123; &#125; public static Singleton getInstance() &#123; // 第一次判空 if (null == instance) &#123; synchronized (Singleton.class) &#123; // 第二次判空 if (null == instance) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 1.为什么要进行第一次判空 我们知道单例模式只有第一次执行getInstance()方法的时候才会走synchronized 中的代码，后面再次访问的时候直接返回single 对象。如果说我们没有第一次验校，每一个线程都要走synchronized 中的代码，而每一次线程都要去拿到同步锁才能执行。在多线程的情况下每一个线程要拿到single 对象都要排队等待同步锁释放。因此第一次验校作用就是为了提高程序的效率。 2.为什么要进行第二次判空 举个例子：假如现在没有第二次验校，线程A执行到第一次验校那里，它判断到single ==null。此时它的资源被线程B抢占了，B执行程序，进入同步代码块创建对象，然后释放同步锁，此时线程A又拿到了资源也拿到了同步锁，然后执行同步代码块，因为之前线程A它判断到single ==null，因此它会直接创建新的对象。所以就违反了我们设计的最终目的。 3.变量为什么要加volatile关键字 在上面例子中volatile保证代码指令不会被重排序，首先我们得先了解什么是volatile关键字与及它的特性。 volatile关键字的特性volatile具有可见性、有序性，不具备原子性。 注意，volatile不具备原子性，这是volatile与java中的synchronized、java.util.concurrent.locks.Lock最大的功能差异，这一点在面试中也是非常容易问到的点。 原子性：不可打断的操作，要么成功要么失败。例如基本数据类型的读写操作都是属于原子性，int a = 10这一过程就是原子性（即可以看成当一个线程执行int a = 10这句代码时其他线程是处于等待获取cpu资源的状态，因为线程并发是通过cpu调度交替执行，并不是真正的并行执行），但是像a++这样的运算操作就是非原子性，因为a++虚拟机要运行三个指令：读取a，a+1，a赋值，这个过程是允许打断的。 volatile具有可见性是指当一个线程对变量进行原子操作的时候，另外的线程能立即获取到最新的数据。 java如何实现可见性需要了解Java的内存模型： Java内存模型(即Java Memory Model，简称JMM)本身是一种抽象的概念，并不真实存在。它描述了不同线程访问共享变量的规则。 我们程序所有的共享变量都是放在主内存中，而线程会从主内存拷贝它需要的共享变量到自己的工作内存。 线程之间变量的传递则需要靠主内存。 Java内存模型规定对共享变量的操作只能在自己工作内存进行，即我读一个共享变量时需要先从工作内存找。写的时候也需要修改工作内存的变量后再push到主内存。 在多线程并发的情况下会发生什么问题？假如主内存中有个共享变量 int a = 10,线程A和线程B的工作内存都有这个变量的副本。假如线程A在自己工作内存中修改了a的值，int a = 20，此时线程A还未来得及push到主内存中线程B就已经读取了a的值,线程B读取到的值是10，因此就造成了数据的混乱。 而java的volatile保证了共享变量的可见性：volatile修饰的变量当线程在工作内存修改后会立马push到主内存中，同时会把工作内存的变量设置为禁止读取，因此访问这个变量的时候不能从自己的工作内存访问，必须要去主内存中取。 还是举上面的例子：变量a被volatile修饰 volatile int a = 10，线程A修改了a的值立马push到主存中，线程B访问的时候到主存访问就可以得到最新的值。 其次volatile还可以禁止虚拟机对指令进行重排序。指令重排作用是虚拟机在不影响单线程程序执行结果的前提下对指令重新排序，提高程序运行效率。但是重排序在多线程并发的情况下也是容易出现问题的。 在上诉单例模式中volatile保证了虚拟机执行字节码的时候指令不会重排序。 single = new Single() 在我们看来就是一句话操作而已，但在虚拟机看来它一共分为了几个指令操作： 为对象分配内存空间 初始化对象 引用指向对象的内存空间地址 虚拟机执行的时候不一定是按顺序123的执行，也有可能是132。这是虚拟机的重排序引起的，单线程情况下是没有什么bug的，最终都会创建出对象，只是先后顺序不同。 但是在上面例子中会出现这么一种情况： 假如线程A执行 single = new Single()虚拟机是按132排序执行，当执行到3的时候single 引用已经不为空。此时若线程B执行到第一次验校处（第一次验校不在同步代码中，因此所有线程随时都可以访问），它判断 single ==null 得到false,直接返回single对象。但是此时single对象还没初始化完成，因此很有可能就会发生bug。 查看原文]]></content>
      <categories>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP三次握手和四次挥手过程]]></title>
    <url>%2F2020%2F09%2F13%2FTCP%E4%B8%89%E6%AC%A1%E6%8F%A1%E6%89%8B%E5%92%8C%E5%9B%9B%E6%AC%A1%E6%8C%A5%E6%89%8B%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[1、三次握手（1）三次握手的详述首先Client端发送连接请求报文，Server段接受连接后回复ACK报文，并为这次连接分配资源。Client端接收到ACK报文后也向Server段发生ACK报文，并分配资源，这样TCP连接就建立了。 最初两端的TCP进程都处于CLOSED关闭状态，A主动打开连接，而B被动打开连接。（A、B关闭状态CLOSED——B收听状态LISTEN——A同步已发送状态SYN-SENT——B同步收到状态SYN-RCVD——A、B连接已建立状态ESTABLISHED） B的TCP服务器进程先创建传输控制块TCB，准备接受客户进程的连接请求。然后服务器进程就处于LISTEN（收听）状态，等待客户的连接请求。若有，则作出响应。 1）第一次握手：A的TCP客户进程也是首先创建传输控制块TCB，然后向B发出连接请求报文段，（首部的同步位SYN=1，初始序号seq=x），（SYN=1的报文段不能携带数据）但要消耗掉一个序号，此时TCP客户进程进入SYN-SENT（同步已发送）状态。 2）第二次握手：B收到连接请求报文段后，如同意建立连接，则向A发送确认，在确认报文段中（SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y**），测试TCP服务器进程进入SYN-RCVD（同步收到）状态； 3）第三次握手：TCP客户进程收到B的确认后，要向B给出确认报文段（ACK=1，确认号ack=y+1，序号seq=x+1）（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。TCP连接已经建立，A进入ESTABLISHED（已建立连接）。 当B收到A的确认后，也进入ESTABLISHED状态。 （2）总结三次握手过程： 第一次握手：起初两端都处于CLOSED关闭状态，Client将标志位SYN置为1，随机产生一个值seq=x，并将该数据包发送给Server，Client进入SYN-SENT状态，等待Server确认； 第二次握手：Server收到数据包后由标志位SYN=1得知Client请求建立连接，Server将标志位SYN和ACK都置为1，ack=x+1，随机产生一个值seq=y，并将该数据包发送给Client以确认连接请求，Server进入SYN-RCVD状态，此时操作系统为该TCP连接分配TCP缓存和变量； 第三次握手：Client收到确认后，检查ack是否为x+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=y+1，并且此时操作系统为该TCP连接分配TCP缓存和变量，并将该数据包发送给Server，Server检查ack是否为y+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client和Server就可以开始传输数据。 起初A和B都处于CLOSED状态——B创建TCB，处于LISTEN状态，等待A请求——A创建TCB，发送连接请求（SYN=1，seq=x），进入SYN-SENT状态——B收到连接请求，向A发送确认（SYN=ACK=1，确认号ack=x+1，初始序号seq=y），进入SYN-RCVD状态——A收到B的确认后，给B发出确认（ACK=1，ack=y+1，seq=x+1），A进入ESTABLISHED状态——B收到A的确认后，进入ESTABLISHED状态。 TCB传输控制块Transmission Control Block，存储每一个连接中的重要信息，如TCP连接表，到发送和接收缓存的指针，到重传队列的指针，当前的发送和接收序号。 （3）为什么A还要发送一次确认呢？可以二次握手吗？ 答：主要为了防止已失效的连接请求报文段突然又传送到了B，因而产生错误。如A发出连接请求，但因连接请求报文丢失而未收到确认，于是A再重传一次连接请求。后来收到了确认，建立了连接。数据传输完毕后，就释放了连接，A工发出了两个连接请求报文段，其中第一个丢失，第二个到达了B，但是第一个丢失的报文段只是在某些网络结点长时间滞留了，延误到连接释放以后的某个时间才到达B，此时B误认为A又发出一次新的连接请求，于是就向A发出确认报文段，同意建立连接，不采用三次握手，只要B发出确认，就建立新的连接了，此时A不理睬B的确认且不发送数据，则B一致等待A发送数据，浪费资源。 （4）Server端易受到SYN攻击？服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击，SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。 防范SYN攻击措施：降低主机的等待时间使主机尽快的释放半连接的占用，短时间受到某IP的重复SYN则丢弃后续请求。 2、四次挥手（1）四次挥手的详述 假设Client端发起中断连接请求，也就是发送FIN报文。Server端接到FIN报文后，意思是说”我Client端没有数据要发给你了”，但是如果你还有数据没有发送完成，则不必急着关闭Socket，可以继续发送数据。所以你先发送ACK，”告诉Client端，你的请求我收到了，但是我还没准备好，请继续你等我的消息”。这个时候Client端就进入FIN_WAIT状态，继续等待Server端的FIN报文。当Server端确定数据已发送完成，则向Client端发送FIN报文，”告诉Client端，好了，我这边数据发完了，准备好关闭连接了”。Client端收到FIN报文后，”就知道可以关闭连接了，但是他还是不相信网络，怕Server端不知道要关闭，所以发送ACK后进入TIME_WAIT状态，如果Server端没有收到ACK则可以重传。“，Server端收到ACK后，”就知道可以断开连接了”。Client端等待了2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，我Client端也可以关闭连接了。Ok，TCP连接就这样关闭了！ 数据传输结束后，通信的双方都可释放连接，A和B都处于ESTABLISHED状态。（A、B连接建立状态ESTABLISHED——A终止等待1状态FIN-WAIT-1——B关闭等待状态CLOSE-WAIT——A终止等待2状态FIN-WAIT-2——B最后确认状态LAST-ACK——A时间等待状态TIME-WAIT——B、A关闭状态CLOSED） 1）A的应用进程先向其TCP发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN-WAIT-1（终止等待1）状态，等待B的确认。 2）B收到连接释放报文段后即发出确认报文段，（ACK=1，确认号ack=u+1，序号seq=v），B进入CLOSE-WAIT（关闭等待）状态，此时的TCP处于半关闭状态，A到B的连接释放。 3）A收到B的确认后，进入FIN-WAIT-2（终止等待2）状态，等待B发出的连接释放报文段。 4）B没有要向A发出的数据，B发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），B进入LAST-ACK（最后确认）状态，等待A的确认。 5）A收到B的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），A进入TIME-WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，A才进入CLOSED状态。 （2）总结四次挥手过程：起初A和B处于ESTABLISHED状态——A发出连接释放报文段并处于FIN-WAIT-1状态——B发出确认报文段且进入CLOSE-WAIT状态——A收到确认后，进入FIN-WAIT-2状态，等待B的连接释放报文段——B没有要向A发出的数据，B发出连接释放报文段且进入LAST-ACK状态——A发出确认报文段且进入TIME-WAIT状态——B收到确认报文段后进入CLOSED状态——A经过等待计时器时间2MSL后，进入CLOSED状态。 （3）为什么A在TIME-WAIT状态必须等待2MSL的时间？MSL最长报文段寿命Maximum Segment Lifetime，MSL=2 答： 两个理由：1）保证A发送的最后一个ACK报文段能够到达B。2）防止“已失效的连接请求报文段”出现在本连接中。 1）这个ACK报文段有可能丢失，使得处于LAST-ACK状态的B收不到对已发送的FIN+ACK报文段的确认，B超时重传FIN+ACK报文段，而A能在2MSL时间内收到这个重传的FIN+ACK报文段，接着A重传一次确认，重新启动2MSL计时器，最后A和B都进入到CLOSED状态，若A在TIME-WAIT状态不等待一段时间，而是发送完ACK报文段后立即释放连接，则无法收到B重传的FIN+ACK报文段，所以不会再发送一次确认报文段，则B无法正常进入到CLOSED状态。 2）A在发送完最后一个ACK报文段后，再经过2MSL，就可以使本连接持续的时间内所产生的所有报文段都从网络中消失，使下一个新的连接中不会出现这种旧的连接请求报文段。 （4）为什么连接的时候是三次握手，关闭的时候却是四次握手？答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。 （5）为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？答：虽然按道理，四个报文都发送完毕，我们可以直接进入CLOSE状态了，但是我们必须假象网络是不可靠的，有可以最后一个ACK丢失。所以TIME_WAIT状态就是用来重发可能丢失的ACK报文。 查看原文]]></content>
      <tags>
        <tag>网络</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中的锁]]></title>
    <url>%2F2020%2F09%2F04%2FJava%E4%B8%AD%E7%9A%84%E9%94%81%2F</url>
    <content type="text"><![CDATA[前言Java提供了种类丰富的锁，每种锁因其特性的不同，在适当的场景下能够展现出非常高的效率。本文旨在对锁相关源码（本文中的源码来自JDK 8）、使用场景进行举例，为读者介绍主流锁的知识点，以及不同的锁的适用场景。 Java中往往是按照是否含有某一特性来定义锁，我们通过特性将锁进行分组归类，再使用对比的方式进行介绍，帮助大家更快捷的理解相关知识。下面给出本文内容的总体分类目录： 锁分类图 1. 乐观锁 VS 悲观锁乐观锁与悲观锁是一种广义上的概念，体现了看待线程同步的不同角度。在Java和数据库中都有此概念对应的实际应用。 先说概念。对于同一个数据的并发操作，悲观锁认为自己在使用数据的时候一定有别的线程来修改数据，因此在获取数据的时候会先加锁，确保数据不会被别的线程修改。Java中，synchronized关键字和Lock的实现类都是悲观锁。 而乐观锁认为自己在使用数据时不会有别的线程修改数据，所以不会添加锁，只是在更新数据的时候去判断之前有没有别的线程更新了这个数据。如果这个数据没有被更新，当前线程将自己修改的数据成功写入。如果数据已经被其他线程更新，则根据不同的实现方式执行不同的操作（例如报错或者自动重试）。 乐观锁在Java中是通过使用无锁编程来实现，最常采用的是CAS算法，Java原子类中的递增操作就通过CAS自旋实现的。 乐观锁vs悲观锁根据从上面的概念描述我们可以发现： 悲观锁适合写操作多的场景，先加锁可以保证写操作时数据正确。 乐观锁适合读操作多的场景，不加锁的特点能够使其读操作的性能大幅提升。 光说概念有些抽象，我们来看下乐观锁和悲观锁的调用方式示例： 悲观锁vs乐观锁的调用方式通过调用方式示例，我们可以发现悲观锁基本都是在显式的锁定之后再操作同步资源，而乐观锁则直接去操作同步资源。那么，为何乐观锁能够做到不锁定同步资源也可以正确的实现线程同步呢？我们通过介绍乐观锁的主要实现方式 “CAS” 的技术原理来为大家解惑。 CAS全称 Compare And Swap（比较与交换），是一种无锁算法。在不使用锁（没有线程被阻塞）的情况下实现多线程之间的变量同步。java.util.concurrent包中的原子类就是通过CAS来实现了乐观锁。 CAS算法涉及到三个操作数： 需要读写的内存值 V。 进行比较的值 A。 要写入的新值 B。 当且仅当 V 的值等于 A 时，CAS通过原子方式用新值B来更新V的值（“比较+更新”整体是一个原子操作），否则不会执行任何操作。一般情况下，“更新”是一个不断重试的操作。 之前提到java.util.concurrent包中的原子类，就是通过CAS来实现了乐观锁，那么我们进入原子类AtomicInteger的源码，看一下AtomicInteger的定义： AtomicInteger根据定义我们可以看出各属性的作用： unsafe： 获取并操作内存的数据。 valueOffset： 存储value在AtomicInteger中的偏移量。 value： 存储AtomicInteger的int值，该属性需要借助volatile关键字保证其在线程间是可见的。 接下来，我们查看AtomicInteger的自增函数incrementAndGet()的源码时，发现自增函数底层调用的是unsafe.getAndAddInt()。但是由于JDK本身只有Unsafe.class，只通过class文件中的参数名，并不能很好的了解方法的作用，所以我们通过OpenJDK 8 来查看Unsafe的源码： 根据OpenJDK 8的源码我们可以看出，getAndAddInt()循环获取给定对象o中的偏移量处的值v，然后判断内存值是否等于v。如果相等则将内存值设置为 v + delta，否则返回false，继续循环进行重试，直到设置成功才能退出循环，并且将旧值返回。整个“比较+更新”操作封装在compareAndSwapInt()中，在JNI里是借助于一个CPU指令完成的，属于原子操作，可以保证多个线程都能够看到同一个变量的修改值。 后续JDK通过CPU的cmpxchg指令，去比较寄存器中的 A 和 内存中的值 V。如果相等，就把要写入的新值 B 存入内存中。如果不相等，就将内存值 V 赋值给寄存器中的值 A。然后通过Java代码中的while循环再次调用cmpxchg指令进行重试，直到设置成功为止。 CAS虽然很高效，但是它也存在三大问题，这里也简单说一下： ABA问题。CAS需要在操作值的时候检查内存值是否发生变化，没有发生变化才会更新内存值。但是如果内存值原来是A，后来变成了B，然后又变成了A，那么CAS进行检查时会发现值没有发生变化，但是实际上是有变化的。ABA问题的解决思路就是在变量前面添加版本号，每次变量更新的时候都把版本号加一，这样变化过程就从“A－B－A”变成了“1A－2B－3A”。 JDK从1.5开始提供了AtomicStampedReference类来解决ABA问题，具体操作封装在compareAndSet()中。compareAndSet()首先检查当前引用和当前标志与预期引用和预期标志是否相等，如果都相等，则以原子方式将引用值和标志的值设置为给定的更新值。 循环时间长开销大。CAS操作如果长时间不成功，会导致其一直自旋，给CPU带来非常大的开销。 只能保证一个共享变量的原子操作。对一个共享变量执行操作时，CAS能够保证原子操作，但是对多个共享变量操作时，CAS是无法保证操作的原子性的。 Java从1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，可以把多个变量放在一个对象里来进行CAS操作。 2. 自旋锁 VS 适应性自旋锁在介绍自旋锁前，我们需要介绍一些前提知识来帮助大家明白自旋锁的概念。 阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长。 在许多场景中，同步资源的锁定时间很短，为了这一小段时间去切换线程，线程挂起和恢复现场的花费可能会让系统得不偿失。如果物理机器有多个处理器，能够让两个或以上的线程同时并行执行，我们就可以让后面那个请求锁的线程不放弃CPU的执行时间，看看持有锁的线程是否很快就会释放锁。 而为了让当前线程“稍等一下”，我们需让当前线程进行自旋，如果在自旋完成后前面锁定同步资源的线程已经释放了锁，那么当前线程就可以不必阻塞而是直接获取同步资源，从而避免切换线程的开销。这就是自旋锁。 自旋锁本身是有缺点的，它不能代替阻塞。自旋等待虽然避免了线程切换的开销，但它要占用处理器时间。如果锁被占用的时间很短，自旋等待的效果就会非常好。反之，如果锁被占用的时间很长，那么自旋的线程只会白浪费处理器资源。所以，自旋等待的时间必须要有一定的限度，如果自旋超过了限定次数（默认是10次，可以使用-XX:PreBlockSpin来更改）没有成功获得锁，就应当挂起线程。 自旋锁的实现原理同样也是CAS，AtomicInteger中调用unsafe进行自增操作的源码中的do-while循环就是一个自旋操作，如果修改数值失败则通过循环来执行自旋，直至修改成功。 自旋锁在JDK1.4.2中引入，使用-XX:+UseSpinning来开启。JDK 6中变为默认开启，并且引入了自适应的自旋锁（适应性自旋锁）。 自适应意味着自旋的时间（次数）不再固定，而是由前一次在同一个锁上的自旋时间及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源。 在自旋锁中 另有三种常见的锁形式:TicketLock、CLHlock和MCSlock，本文中仅做名词介绍，不做深入讲解。 3. 无锁 VS 偏向锁 VS 轻量级锁 VS 重量级锁这四种锁是指锁的状态，专门针对synchronized的。在介绍这四种锁状态之前还需要介绍一些额外的知识。 首先为什么Synchronized能实现线程同步？ 在回答这个问题之前我们需要了解两个重要的概念：“Java对象头”、“Monitor”。 Java对象头synchronized是悲观锁，在操作同步资源之前需要给同步资源先加锁，这把锁就是存在Java对象头里的，而Java对象头又是什么呢？ 我们以Hotspot虚拟机为例，Hotspot的对象头主要包括两部分数据：Mark Word（标记字段）、Klass Pointer（类型指针）。 Mark Word：默认存储对象的HashCode，分代年龄和锁标志位信息。这些信息都是与对象自身定义无关的数据，所以Mark Word被设计成一个非固定的数据结构以便在极小的空间内存存储尽量多的数据。它会根据对象的状态复用自己的存储空间，也就是说在运行期间Mark Word里存储的数据会随着锁标志位的变化而变化。 Klass Point：对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 MonitorMonitor可以理解为一个同步工具或一种同步机制，通常被描述为一个对象。每一个Java对象就有一把看不见的锁，称为内部锁或者Monitor锁。 Monitor是线程私有的数据结构，每一个线程都有一个可用monitor record列表，同时还有一个全局的可用列表。每一个被锁住的对象都会和一个monitor关联，同时monitor中有一个Owner字段存放拥有该锁的线程的唯一标识，表示该锁被这个线程占用。 现在话题回到synchronized，synchronized通过Monitor来实现线程同步，Monitor是依赖于底层的操作系统的Mutex Lock（互斥锁）来实现的线程同步。 如同我们在自旋锁中提到的“阻塞或唤醒一个Java线程需要操作系统切换CPU状态来完成，这种状态转换需要耗费处理器时间。如果同步代码块中的内容过于简单，状态转换消耗的时间有可能比用户代码执行的时间还要长”。这种方式就是synchronized最初实现同步的方式，这就是JDK 6之前synchronized效率低的原因。这种依赖于操作系统Mutex Lock所实现的锁我们称之为“重量级锁”，JDK 6中为了减少获得锁和释放锁带来的性能消耗，引入了“偏向锁”和“轻量级锁”。 所以目前锁一共有4种状态，级别从低到高依次是：无锁、偏向锁、轻量级锁和重量级锁。锁状态只能升级不能降级。 通过上面的介绍，我们对synchronized的加锁机制以及相关知识有了一个了解，那么下面我们给出四种锁状态对应的的Mark Word内容，然后再分别讲解四种锁状态的思路以及特点： 无锁 无锁没有对资源进行锁定，所有的线程都能访问并修改同一个资源，但同时只有一个线程能修改成功。 无锁的特点就是修改操作在循环内进行，线程会不断的尝试修改共享资源。如果没有冲突就修改成功并退出，否则就会继续循环尝试。如果有多个线程修改同一个值，必定会有一个线程能修改成功，而其他修改失败的线程会不断重试直到修改成功。上面我们介绍的CAS原理及应用即是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。 偏向锁 偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁，降低获取锁的代价。 在大多数情况下，锁总是由同一线程多次获得，不存在多线程竞争，所以出现了偏向锁。其目标就是在只有一个线程执行同步代码块时能够提高性能。 当一个线程访问同步代码块并获取锁时，会在Mark Word里存储锁偏向的线程ID。在线程进入和退出同步块时不再通过CAS操作来加锁和解锁，而是检测Mark Word里是否存储着指向当前线程的偏向锁。引入偏向锁是为了在无多线程竞争的情况下尽量减少不必要的轻量级锁执行路径，因为轻量级锁的获取及释放依赖多次CAS原子指令，而偏向锁只需要在置换ThreadID的时候依赖一次CAS原子指令即可。 偏向锁只有遇到其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁，线程不会主动释放偏向锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，判断锁对象是否处于被锁定状态。撤销偏向锁后恢复到无锁（标志位为“01”）或轻量级锁（标志位为“00”）的状态。 偏向锁在JDK 6及以后的JVM里是默认启用的。可以通过JVM参数关闭偏向锁：-XX:-UseBiasedLocking=false，关闭之后程序默认会进入轻量级锁状态。 轻量级锁 是指当锁是偏向锁的时候，被另外的线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能。 在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为“01”状态，是否为偏向锁为“0”），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间，用于存储锁对象目前的Mark Word的拷贝，然后拷贝对象头中的Mark Word复制到锁记录中。 拷贝成功后，虚拟机将使用CAS操作尝试将对象的Mark Word更新为指向Lock Record的指针，并将Lock Record里的owner指针指向对象的Mark Word。 如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为“00”，表示此对象处于轻量级锁定状态。 如果轻量级锁的更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧，如果是就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行，否则说明多个线程竞争锁。 若当前只有一个等待线程，则该线程通过自旋进行等待。但是当自旋超过一定的次数，或者一个线程在持有锁，一个在自旋，又有第三个来访时，轻量级锁升级为重量级锁。 重量级锁 升级为重量级锁时，锁标志的状态值变为“10”，此时Mark Word中存储的是指向重量级锁的指针，此时等待锁的线程都会进入阻塞状态。 整体的锁状态升级流程如下： 综上，偏向锁通过对比Mark Word解决加锁问题，避免执行CAS操作。而轻量级锁是通过用CAS操作和自旋来解决加锁问题，避免线程阻塞和唤醒而影响性能。重量级锁是将除了拥有锁的线程以外的线程都阻塞。 4. 公平锁 VS 非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁，线程直接进入队列中排队，队列中的第一个线程才能获得锁。公平锁的优点是等待锁的线程不会饿死。缺点是整体吞吐效率相对非公平锁要低，等待队列中除第一个线程以外的所有线程都会阻塞，CPU唤醒阻塞线程的开销比非公平锁大。 非公平锁是多个线程加锁时直接尝试获取锁，获取不到才会到等待队列的队尾等待。但如果此时锁刚好可用，那么这个线程可以无需阻塞直接获取到锁，所以非公平锁有可能出现后申请锁的线程先获取锁的场景。非公平锁的优点是可以减少唤起线程的开销，整体的吞吐效率高，因为线程有几率不阻塞直接获得锁，CPU不必唤醒所有线程。缺点是处于等待队列中的线程可能会饿死，或者等很久才会获得锁。 直接用语言描述可能有点抽象，这里作者用从别处看到的一个例子来讲述一下公平锁和非公平锁。 如上图所示，假设有一口水井，有管理员看守，管理员有一把锁，只有拿到锁的人才能够打水，打完水要把锁还给管理员。每个过来打水的人都要管理员的允许并拿到锁之后才能去打水，如果前面有人正在打水，那么这个想要打水的人就必须排队。管理员会查看下一个要去打水的人是不是队伍里排最前面的人，如果是的话，才会给你锁让你去打水；如果你不是排第一的人，就必须去队尾排队，这就是公平锁。 但是对于非公平锁，管理员对打水的人没有要求。即使等待队伍里有排队等待的人，但如果在上一个人刚打完水把锁还给管理员而且管理员还没有允许等待队伍里下一个人去打水时，刚好来了一个插队的人，这个插队的人是可以直接从管理员那里拿到锁去打水，不需要排队，原本排队等待的人只能继续等待。如下图所示： 接下来我们通过ReentrantLock的源码来讲解公平锁和非公平锁。 根据代码可知，ReentrantLock里面有一个内部类Sync，Sync继承AQS（AbstractQueuedSynchronizer），添加锁和释放锁的大部分操作实际上都是在Sync中实现的。它有公平锁FairSync和非公平锁NonfairSync两个子类。ReentrantLock默认使用非公平锁，也可以通过构造器来显示的指定使用公平锁。 下面我们来看一下公平锁与非公平锁的加锁方法的源码: 通过上图中的源代码对比，我们可以明显的看出公平锁与非公平锁的lock()方法唯一的区别就在于公平锁在获取同步状态时多了一个限制条件：hasQueuedPredecessors()。 再进入hasQueuedPredecessors()，可以看到该方法主要做一件事情：主要是是判断前面是否有线程在排队，有返回true 否则返回false。 综上，公平锁就是通过同步队列来实现多个线程按照申请锁的顺序来获取锁，从而实现公平的特性。非公平锁加锁时不考虑排队等待问题，直接尝试获取锁，所以存在后申请却先获得锁的情况。 5. 可重入锁 VS 非可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，再进入该线程的内层方法会自动获取锁（前提锁对象得是同一个对象或者class），不会因为之前已经获取过还没释放而阻塞。Java中ReentrantLock和synchronized都是可重入锁，可重入锁的一个优点是可一定程度避免死锁。下面用示例代码来进行分析： 在上面的代码中，类中的两个方法都是被内置锁synchronized修饰的，doSomething()方法中调用doOthers()方法。因为内置锁是可重入的，所以同一个线程在调用doOthers()时可以直接获得当前对象的锁，进入doOthers()进行操作。 如果是一个不可重入锁，那么当前线程在调用doOthers()之前需要将执行doSomething()时获取当前对象的锁释放掉，实际上该对象锁已被当前线程所持有，且无法释放。所以此时会出现死锁。 之前我们说过ReentrantLock和synchronized都是重入锁，那么我们通过重入锁ReentrantLock以及非可重入锁NonReentrantLock的源码来对比分析一下为什么非可重入锁在重复调用同步资源时会出现死锁。 首先ReentrantLock和NonReentrantLock都继承父类AQS，其父类AQS中维护了一个同步状态status来计数重入次数，status初始值为0。 当线程尝试获取锁时，可重入锁先尝试获取并更新status值，如果status == 0表示没有其他线程在执行同步代码，则把status置为1，当前线程开始执行。如果status != 0，则判断当前线程是否是获取到这个锁的线程，如果是的话执行status+1，且当前线程可以再次获取锁。而非可重入锁是直接去获取并尝试更新当前status的值，如果status != 0的话会导致其获取锁失败，当前线程阻塞。 释放锁时，可重入锁同样先获取当前status的值，在当前线程是持有锁的线程的前提下。如果status-1 == 0，则表示当前线程所有重复获取锁的操作都已经执行完毕，然后该线程才会真正释放锁。而非可重入锁则是在确定当前线程是持有锁的线程之后，直接将status置为0，将锁释放。 6. 独享锁 VS 共享锁独享锁和共享锁同样是一种概念。我们先介绍一下具体的概念，然后通过ReentrantLock和ReentrantReadWriteLock的源码来介绍独享锁和共享锁。 独享锁也叫排他锁，是指该锁一次只能被一个线程所持有。如果线程T对数据A加上排它锁后，则其他线程不能再对A加任何类型的锁。获得排它锁的线程即能读数据又能修改数据。JDK中的synchronized和JUC中Lock的实现类就是互斥锁。 共享锁是指该锁可被多个线程所持有。如果线程T对数据A加上共享锁后，则其他线程只能对A再加共享锁，不能加排它锁。获得共享锁的线程只能读数据，不能修改数据。 独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。 下图为ReentrantReadWriteLock的部分源码： 我们看到ReentrantReadWriteLock有两把锁：ReadLock和WriteLock，由词知意，一个读锁一个写锁，合称“读写锁”。再进一步观察可以发现ReadLock和WriteLock是靠内部类Sync实现的锁。Sync是AQS的一个子类，这种结构在CountDownLatch、ReentrantLock、Semaphore里面也都存在。 在ReentrantReadWriteLock里面，读锁和写锁的锁主体都是Sync，但读锁和写锁的加锁方式不一样。读锁是共享锁，写锁是独享锁。读锁的共享锁可保证并发读非常高效，而读写、写读、写写的过程互斥，因为读锁和写锁是分离的。所以ReentrantReadWriteLock的并发性相比一般的互斥锁有了很大提升。 那读锁和写锁的具体加锁方式有什么区别呢？在了解源码之前我们需要回顾一下其他知识。 在最开始提及AQS的时候我们也提到了state字段（int类型，32位），该字段用来描述有多少线程获持有锁。 在独享锁中这个值通常是0或者1（如果是重入锁的话state值就是重入的次数），在共享锁中state就是持有锁的数量。但是在ReentrantReadWriteLock中有读、写两把锁，所以需要在一个整型变量state上分别描述读锁和写锁的数量（或者也可以叫状态）。于是将state变量“按位切割”切分成了两个部分，高16位表示读锁状态（读锁个数），低16位表示写锁状态（写锁个数）。如下图所示： 了解了概念之后我们再来看代码，先看写锁的加锁源码： 这段代码首先取到当前锁的个数c，然后再通过c来获取写锁的个数w。因为写锁是低16位，所以取低16位的最大值与当前的c做与运算（ int w = exclusiveCount(c); ），高16位和0与运算后是0，剩下的就是低位运算的值，同时也是持有写锁的线程数目。 在取到写锁线程的数目后，首先判断是否已经有线程持有了锁。如果已经有线程持有了锁（c!=0），则查看当前写锁线程的数目，如果写线程数为0（即此时存在读锁）或者持有锁的线程不是当前线程就返回失败（涉及到公平锁和非公平锁的实现）。 如果写入锁的数量大于最大数（65535，2的16次方-1）就抛出一个Error。 如果当且写线程数为0（那么读线程也应该为0，因为上面已经处理c!=0的情况），并且当前线程需要阻塞那么就返回失败；如果通过CAS增加写线程数失败也返回失败。 如果c=0，w=0或者c&gt;0，w&gt;0（重入），则设置当前线程或锁的拥有者，返回成功！ tryAcquire()除了重入条件（当前线程为获取了写锁的线程）之外，增加了一个读锁是否存在的判断。如果存在读锁，则写锁不能被获取，原因在于：必须确保写锁的操作对读锁可见，如果允许读锁在已被获取的情况下对写锁的获取，那么正在运行的其他读线程就无法感知到当前写线程的操作。 因此，只有等待其他读线程都释放了读锁，写锁才能被当前线程获取，而写锁一旦被获取，则其他读写线程的后续访问均被阻塞。写锁的释放与ReentrantLock的释放过程基本类似，每次释放均减少写状态，当写状态为0时表示写锁已被释放，然后等待的读写线程才能够继续访问读写锁，同时前次写线程的修改对后续的读写线程可见。 接着是读锁的代码： 可以看到在tryAcquireShared(int unused)方法中，如果其他线程已经获取了写锁，则当前线程获取读锁失败，进入等待状态。如果当前线程获取了写锁或者写锁未被获取，则当前线程（线程安全，依靠CAS保证）增加读状态，成功获取读锁。读锁的每次释放（线程安全的，可能有多个读线程同时释放读锁）均减少读状态，减少的值是“1&lt;&lt;16”。所以读写锁才能实现读读的过程共享，而读写、写读、写写的过程互斥。 此时，我们再回头看一下互斥锁ReentrantLock中公平锁和非公平锁的加锁源码： 我们发现在ReentrantLock虽然有公平锁和非公平锁两种，但是它们添加的都是独享锁。根据源码所示，当某一个线程调用lock方法获取锁时，如果同步资源没有被其他线程锁住，那么当前线程在使用CAS更新state成功后就会成功抢占该资源。而如果公共资源被占用且不是被当前线程占用，那么就会加锁失败。所以可以确定ReentrantLock无论读操作还是写操作，添加的锁都是都是独享锁。 结语其实Java本身已经对锁本身进行了良好的封装，降低了研发同学在平时工作中的使用难度。但是研发同学也需要熟悉锁的底层原理，不同场景下选择最适合的锁。而且源码中的思路都是非常好的思路，也是值得大家去学习和借鉴的。 查看原文]]></content>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[java8 HashMap]]></title>
    <url>%2F2020%2F08%2F27%2Fjava8-HashMap%2F</url>
    <content type="text"><![CDATA[1、什么是Hash1.1、核心理论：Hash也称散列、哈希，对应的英文都是Hash。基本原理就是把任意长度的输入，通过Hash算法变成固定长度的输出。 这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。 1.2、Hash的特点： 从Hash值不可以反向推导出原始的数据 输入数据的微小变化会得到完全不同的Hash值，相同的数据会得到相同的值 哈希算法的执行效率要高效，长的文本也能快速地计算出哈希值 Hash算法的冲突概率要小 2、HashMap存在的意义&emsp;&emsp;动态数组虽然能够自动扩容，但是必须在初始时刻指定初始容量。而对于那些在编译时无法确定具体的数量即动态增长的数据，就需要用到Java集合类了。对于ArrayList 和 LinkedList，还有 Vector它们都有一些缺点，要么插入删除速度慢、要么就是遍历速度慢。那么有没有一种插入、删除、遍历都比较不错的集合类呢？于是 HashMap 就出现了。HashMap 是一个散列表，它存储的是一组键值对(key-value)的集合，并实现快速的查找。 （1）为了实现快速查找，HashMap 选择了数组而不是链表。以利用数组的索引实现 O(1) 复杂度的查找效率。 （2）为了利用索引查找，HashMap 引入 Hash 算法, 将 key 映射成数组下标: key -&gt; Index。 （3）引入 Hash 算法又导致了 Hash 冲突。为了解决 Hash 冲突，HashMap 采用链地址法，在冲突位置转为使用链表存储。 （4）链表存储过多的节点又导致了在链表上节点的查找性能的恶化。为了优化查找性能，HashMap 在链表长度超过 8 之后转而将链表转变成红黑树，以将 O(n) 复杂度的查找效率提升至 O(log n)。 综上 HashMap 存在的意义就是实现一种快速的查找并且插入、删除性能都不错的K/V（key/value）数据结构。 2.1、数组（Array）1、数组特点：&emsp;&emsp;所谓数组，就是相同数据类型的元素按一定顺序排列的集合；数组的存储区间是连续的，占用内存比较大，故空间复杂的很大。但数组的二分查找时间复杂度小，都是O(1)；数组的特点是：查询简单，增加和删除困难； 在内存中，数组是一块连续的区域 数组需要预留空间 在使用前需要提前申请所占内存的大小，如果提前不知道需要的空间大小时，预先申请就可能会浪费内存空间，即数组的空间利用率较低。注：数组的空间在编译阶段就需要进行确定，所以需要提前给出数组空间的大小(在运行阶段是不允许改变的) 在数组起始位置处，插入数据和删除数据效率低。插入数据时，待插入位置的元素和他后面的所有元素都需要向后搬移，删除数据时，待删除位置后面的所有元素都需要向前搬移。 随机访问效率很高，时间复杂度可以达到O(1)，因为数组的内存是连续的，想要访问那个元素，直接从数组的首地址向后偏移就可以访问到了。 数组开辟的空间，在不够使用的时候需要进行扩容；扩容的话，就涉及到需要把旧数组中的所有元素向新数组中搬移。 数组的空间是从栈分配的。（栈：先进后出） 2、数组的优点：&emsp;&emsp;随机访问性强,查找速度快，时间复杂度是0(1) 3、数组的缺点： 从头部删除、从头部插入的效率低，时间复杂度是o(n),因为需要相应的向前搬移和向后搬移。 空间利用率不高。 内存空间要求高，必须要有足够的连续的内存空间。 数组的空间大小是固定的，不能进行动态扩展。 2.2、链表(ListNode)1、链表特点：&emsp;&emsp;所谓链表，链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 相比于线性表顺序结构，操作复杂。由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而线性表和顺序表相应的时间复杂度分别是O(logn)和O(1)。 链表:链表存储区间离散，占用内存比较宽松，故空间复杂度很小，但时间复杂度很大，达O（N）。链表的特点是：查询相对于数组困难，增加和删除容易。 在内存中，元素的空间可以在任意地方，空间是分散的，不需要连续。 链表中的元素有两个属性，一个是元素的值，另一个是指针，此指针标记了下一个元素的地址。 每一个数据都会保存下一个数据的内存地址，通过该地址就可以找到下一个数据 查找数据时间效率低，时间复杂度是o(n)。 因为链表的空间是分散的，所以不具有随机访问性，如果需要访问某个位置的数据，需要从第一个数开始找起，依次往后遍历，知道找到待查询的位置，故可能在查找某个元素时，时间复杂度是o(n) 空间不需要提前指定大小，是动态申请的，根据需求动态的申请和删除内存空间，扩展方便，故空间的利用率较高。 任意位置插入元素和删除元素时间效率较高，时间复杂度是o(1)。(找到对应位置后直接插入即可，不需要移动其他数据) 链表的空间是从堆中分配的。（堆：先进先出，后进后出） 2、链表优点： 任意位置插入元素和删除元素的速度快，时间复杂度是o(1)。 内存利用率高，不会浪费内存。 链表的空间大小不固定，可以动态拓展。 3、链表缺点：&emsp;&emsp;随机访问效率低，时间复杂度是o(n)。 综上所诉： 对于想要快速访问数据，不经常有插入和删除元素的时候，选择数组； 对于需要经常的插入和删除元素，而对访问元素时的效率没有很高要求的话，选择链表。 那么HashMap就是结合了两者的特点，既有数组查找效率快的优势，又有链表插入和删除元素速度快的特点，并且有动态扩展机制。 3、HashMap类继承关系 HashMap类继承图 4、HashMap内部组成4.1、HashMap底层存储结构&emsp;&emsp;HashMap（java8）底层由有数组 + 链表 + 红黑树构成。 HashMap底层存储结构 4.2、HashMap中的常量12345678910111213141516171819202122232425262728/** * 默认table大小：16，必须是2的指数幂 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** * table最大长度：2x10^23 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** * 默认负载因子大小：0.75f */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** * 树化阈值：8 */static final int TREEIFY_THRESHOLD = 8;/** * 树降级称为链表的阈值：6 */static final int UNTREEIFY_THRESHOLD = 6;/** * 树化的另一个参数，当哈希表中的所有元素个数超过64时，才会允许树化 */static final int MIN_TREEIFY_CAPACITY = 64; HashMap为什么初始容量是2的指数幂？ 12345678910111213141516171819202122232425262728293031/** * HashMap取值方法 */final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; //tab：引用当前hashMap的散列表 //first：桶位中的头元素 //e：临时node元素 //n：table数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //第一种情况：定位出来的桶位元素，即为咱们要get的数据 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //说明当前桶位不止一个元素，可能是链表，也可能是红黑树 if ((e = first.next) != null) &#123; //第二种情况：桶位升级成了红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //第三种情况：桶位形成链表 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 1、从上面hashmap getNode方法中可以看到，在确定元素落在数组的位置的时候，计算方法是(n - 1) &amp; hash，n为数组长度也就是初始容量 ，这是因为“取模”运算的消耗还是比较大的，那么如何保证(n - 1) &amp; hash和hash%n的结果相同呢，当n为2的指数次幂时，会满足一个公式：(n - 1) &amp; hash = hash % n，这样就可以用(n - 1) &amp; hash的位运算来使计算更加高效。 2、如果初始容量是奇数，那么（n-1)就为偶数，偶数2进制的结尾都是0，经过hash值&amp;运算后末尾都是0，那么0001，0011，0101，1001，1011，0111，1101这几个位置永远都不能存放元素了，空间浪费相当大，更糟的是这种情况中，数组可以使用的位置比数组长度小了很多，这样就会造成空间的浪费而且会增加hash冲突。所以初始容量是2的指数幂，达到了让哈希后的结果更均匀的分部，减少哈希碰撞，提升hashmap的运行效率。 3、只有是2的指数次幂的数字经过n-1之后，二进制肯定是 …11111111 这样的格式，这种格式计算的位置的时候，完全是由产生的hash值类决定，而不受n-1 影响。这样会提高效率。比如要扩容了，2的幂次方*2，在二进制中比如4和8，代表2的2次方和3次方，他们的2进制结构相 似,比如 4和8 00000100 0000 1000 只是高位向前移了一位，这样扩容的时候，只需要判断高位hash,移动到之前位置的倍数就可以了，免去了重新计算位置的运算。并且这样可以保证(n - 1) &amp; hash得到的存储位置是在hashmap的length之内的,也就是n之内。因为最大也就是hash值也全是…1111111 4.3、HashMap中的常用字段属性1234567891011121314151617181920212223242526/** * 哈希表（并没有在一开始就初始化，采用了懒加载的方式，在put的时候才初始化） */transient Node&lt;K,V&gt;[] table;/** * 当前哈希表中元素个数 */transient int size;/** * 当前哈希表结构修改次数 */transient int modCount;/** * 扩容阈值，当你的哈希表中的元素超过阈值时，触发扩容 * threshold = capacity * loadFactor */int threshold;/** * 负载因子 * threshold = capacity * loadFactor */final float loadFactor; 其中，哈希表(table)是中Node类型的数组，其中Node部分结构如下 12345678910111213static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; &#125; Node类图 注意： Node[] table 并没有在一开始就完成初始化，通过put 方法可以发现：当发现哈希表为空或者长度为 0 时，会使用 resize 方法进行初始化，这里很显然运用了 lazy-load（懒加载） 原则，当哈希表被首次使用时，才进行初始化 12if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; 4.4、HashMap中常用的静态方法1、hash方法12345678/**重新计算hash值 * 作用：让key的hash值的高16位也参与路由运算（无符号右移16位。 &gt;&gt;&gt;表示忽略符号位移动，空位都以0补齐） * 异或：相同则返回0，不同返回1 */static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; 为什么要重新计算hash值，并且是让key的hash值的高16位也参与路由运算？ 12345h = key.hashCode() 0010 0101 1010 1100 0011 1111 0010 1110^h&gt;&gt;&gt;16 0000 0000 0000 0000 0010 0101 1010 1100----------------------------------------------------------- 0010 0101 1010 1100 0001 1010 1000 0010 &emsp;&emsp;将h无符号右移16为相当于将高区16位移动到了低区的16位，再与原hashcode做异或运算，可以将高低位二进制特征混合起来。 &emsp;&emsp;从上文可知高区的16位与原hashcode相比没有发生变化，低区的16位发生了变化。所以(h = key.hashCode()) ^ (h &gt;&gt;&gt; 16)进行运算可以把高区与低区的二进制特征混合到低区，那么为什么要这么做呢？ &emsp;&emsp;我们都知道重新计算出的新哈希值在后面将会参与hashmap中数组槽位的计算，计算公式：(n - 1) &amp; hash，假如这时数组长度为16，则槽位计算如下： 12345hash 0010 0101 1010 1100 0011 1111 0010 1110&amp;16-1 0000 0000 0000 0000 0000 0000 0000 0111-------------------------------------------------------(16-1) &amp; hash 0000 0000 0000 0000 0000 0000 0000 0110 &emsp;&emsp;仔细观察不难发现，hash高区的16位很有可能会被（n-1）的二进制码锁屏蔽，如果我们不做刚才移位异或运算，那么在计算槽位时将丢失高区特征 &emsp;&emsp;也许你可能会说，即使丢失了高区特征不同hashcode也可以计算出不同的槽位来，但是细想当两个哈希码很接近时，那么这高区的一点点差异就可能导致一次哈希碰撞，所以这也是将性能做到极致的一种体现。 2、tableSizeFor方法123456789101112/** * 作用：返回一个大于等于当前值cap的一个数字，并且这个数字一定是2的次方数 */static final int tableSizeFor(int cap) &#123; int n = cap - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1; &#125; 举个栗子： 1234567891011121314151617cap = 10n = 10 - 1 =&gt; 91001 | 0100 =&gt; 11011101 | 0011 =&gt; 11111111 | 0000 =&gt; 11111111 =&gt; 15return 15 + 1;cap = 16cap = 17n = 16;10000 | 01000 =&gt;1100011000 | 00110 =&gt;1111011110 | 00001 =&gt;1111111111 =&gt; 31return 31 + 1;所以一定是2的次方数 5、HashMap get1234public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value; &#125; 其中主要的是getNode(hash(key), key)方法： 123456789101112131415161718192021222324252627282930final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; //tab：引用当前hashMap的散列表 //first：桶位中的头元素 //e：临时node元素 //n：table数组长度 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; //第一种情况：定位出来的桶位元素 即为咱们要get的数据 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; //说明当前桶位不止一个元素，可能 是链表 也可能是 红黑树 if ((e = first.next) != null) &#123; //第二种情况：桶位升级成了 红黑树 if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); //第三种情况：桶位形成链表 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null; &#125; 主要流程如下： HashMap的get方法. 6、HashMap put123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true); &#125; 其中最主要的是putVal(hash(key), key, value, false, true)方法，如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; //tab：引用当前hashMap的散列表 //p：表示当前散列表的元素 //n：表示散列表数组的长度 //i：表示路由寻址 结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; //延迟初始化逻辑，第一次调用putVal时会初始化hashMap对象中的最耗费内存的散列表 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; //最简单的一种情况：寻址找到的桶位 刚好是 null，这个时候，直接将当前k-v=&gt;node 扔进去就可以了 if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; //e：不为null的话，找到了一个与当前要插入的key-value一致的key的元素 //k：表示临时的一个key Node&lt;K,V&gt; e; K k; //表示桶位中的该元素，与你当前插入的元素的key完全一致，表示后续需要进行替换操作 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode)//红黑树 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); else &#123; //链表的情况，而且链表的头元素与我们要插入的key不一致。 for (int binCount = 0; ; ++binCount) &#123; //条件成立的话，说明迭代到最后一个元素了，也没找到一个与你要插入的key一致的node //说明需要加入到当前链表的末尾 if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); //条件成立的话，说明当前链表的长度，达到树化标准了，需要进行树化 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st //树化操作 treeifyBin(tab, hash); break; &#125; //条件成立的话，说明找到了相同key的node元素，需要进行替换操作 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; //e不等于null，条件成立说明，找到了一个与你插入元素key完全一致的数据，需要进行替换 if (e != null) &#123; // existing mapping for key V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; //modCount：表示散列表结构被修改的次数，替换Node元素的value不计数 ++modCount; //插入新元素，size自增，如果自增后的值大于扩容阈值，则触发扩容。 if (++size &gt; threshold) resize(); afterNodeInsertion(evict); return null; &#125; 主要流程如下： HashMap put方法 HashMap put过程形象图 7、resize123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122final Node&lt;K,V&gt;[] resize() &#123; // oldTab：引用扩容前的哈希表 Node&lt;K,V&gt;[] oldTab = table; // oldCap：表示扩容之前table数组的长度 int oldCap = (oldTab == null) ? 0 : oldTab.length; // oldThr：表示扩容之前的扩容阈值，触发本次扩容的阈值 int oldThr = threshold; // newCap：扩容之后table数组的大小 // newThr：扩容之后，下次再次触发扩容的条件 int newCap, newThr = 0; // 条件如果成立说明 hashMap中的散列表已经初始化过了，这是一次正常扩容 if (oldCap &gt; 0) &#123; // 扩容之前的table数组大小已经达到 最大阈值后，则不扩容，且设置扩容条件为 int 最大值。 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // oldCap左移一位实现数值翻倍，并且赋值给newCap， newCap 小于数组最大值限制 且 扩容之前的数组长度 &gt;= 16 // 这种情况下，则 下一次扩容的阈值 等于当前阈值翻倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; // oldCap == 0,说明hashMap中的散列表是null // 1.new HashMap(initCap, loadFactor); // 2.new HashMap(initCap); // 3.new HashMap(map); 并且这个map有数据 else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; // oldCap == 0，oldThr == 0 // new HashMap(); else &#123; // zero initial threshold signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY;//16 newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY);//12 &#125; // newThr为零时，通过newCap和loadFactor计算出一个newThr if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; // 创建出一个更长 更大的数组 @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // 说明，hashMap本次扩容之前，table不为null if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; // 当前node节点 Node&lt;K,V&gt; e; // 说明当前桶位中有数据，但是数据具体是 单个数据，还是链表 还是 红黑树 并不知道 if ((e = oldTab[j]) != null) &#123; // 方便JVM GC时回收内存 oldTab[j] = null; // 第一种情况：当前桶位只有一个元素，从未发生过碰撞，这情况 直接计算出当前元素应存放在 新数组中的位置，然后 // 扔进去就可以了 if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; // 第二种情况：当前节点已经树化 else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // preserve order // 第三种情况：桶位已经形成链表 // 低位链表：扩容之后的数组对应位置的Node，位置与当前数组的下标位置一致。 Node&lt;K,V&gt; loHead = null, loTail = null; // 高位链表：扩容之后的数组对应位置的Node，位置为当前数组下标位置 + 扩容之前数组的长度 Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 假设oldCap=16，则二进制为1 0000 // hash-&gt; 0 1111 &amp; 1 0000 = 0 则表示是低位 // hash-&gt; 1 1111 &amp; 1 0000 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 如果lo链表非空, 我们就把整个lo链表放到新table的j位置上 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 如果hi链表非空, 我们就把整个hi链表放到新table的j+oldCap位置上 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab; &#125; 其中最难理解的地方是： 1234567891011121314if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e;&#125;else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e;&#125; 1234567891011121314151617181920212223// 比如原来HashMap数组长度为16，现在需要扩容// table数组下标为14的key的位置,14是通过（length-1） &amp; hash 计算出来的length-1=15 01111&amp; hash ?---------------- 01110所以当前key所对应的hash值为...1110(可能为...01110或者...11110)当e.hash为01110时：e.hash 01110&amp;oldCap 10000----------------- 00000e.hash &amp; oldCap == 0，所以放在新数组中的14的位置当e.hash为11110时：e.hash 11110&amp;oldCap 10000----------------- 10000e.hash &amp; oldCap == 1，所以放在新数组中的14 + 16 = 30的位置 如果 (e.hash &amp; oldCap) == 0， 则该节点在新表的下标位置与旧表一致都为 j 如果 (e.hash &amp; oldCap) == 0，则该节点在新表的下标位置与旧表一致都为 j + oldCap e.hash &amp; oldCap，因为oldCap是2的次方数，都是…100的形式，所以与e.hash “&amp;” 后的结果以e.hash最高位的值确定。最高位是0则结果是0，坐标不变即原坐标。最高位是1则结果是1，,坐标变为“j + oldCap”，即“原坐标 + 原长度” 因此，在扩容时，不需要重新计算元素的hash了，只需要判断e.hash最高位是1还是0就好了 HashMap 扩容 8、remove12345678/** * 返回值：如果该键存在，则该方法返回先前映射到指定键的值，否则该方法返回NULL */public V remove(Object key) &#123; Node&lt;K,V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869final Node&lt;K,V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) &#123; //tab：引用当前hashMap中的散列表 //p：当前node元素 //n：表示散列表数组长度 //index：表示寻址结果 Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, index; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) &#123; //说明路由的桶位是有数据的，需要进行查找操作，并且删除 //node：查找到的结果 //e：当前Node的下一个元素 Node&lt;K,V&gt; node = null, e; K k; V v; //第一种情况：当前桶位中的元素即为你要删除的元素（即头元素） if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) node = p; else if ((e = p.next) != null) &#123; //说明，当前桶位 要么是 链表 要么 是红黑树 if (p instanceof TreeNode)//判断当前桶位是否升级为 红黑树了 //第二种情况 //红黑树查找操作，下一期再说 node = ((TreeNode&lt;K,V&gt;)p).getTreeNode(hash, key); else &#123; //第三种情况 //链表的情况 do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) &#123; node = e; break; &#125; p = e; &#125; while ((e = e.next) != null); &#125; &#125; //判断node不为空的话，说明按照key查找到需要删除的数据了 if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) &#123; //第一种情况：node是树节点，说明需要进行树节点移除操作 if (node instanceof TreeNode) ((TreeNode&lt;K,V&gt;)node).removeTreeNode(this, tab, movable); //第二种情况：桶位元素即为查找结果，则将该元素的下一个元素放至当前桶位中（如果不是链表，则下一个元素是null） else if (node == p) tab[index] = node.next; else //第三种情况：将当前元素p的下一个元素 设置成 要删除元素的 下一个元素。 p.next = node.next; ++modCount; --size; afterNodeRemoval(node); return node; &#125; &#125; return null; &#125;]]></content>
      <categories>
        <category>源码</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ConnectTimeout和ReadTimeout所代表的意义]]></title>
    <url>%2F2020%2F08%2F11%2FConnectTimeout%E5%92%8CReadTimeout%E6%89%80%E4%BB%A3%E8%A1%A8%E7%9A%84%E6%84%8F%E4%B9%89%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;今天在生产环境中遇到一个问题，在用http工具类post请求调用别人接口时，没有收到响应信息，我们代码中是做了判断的，如果响应信息为空，则发起查询，奇怪的是我们并没有发起查询。查看了业务代码也是没有问题的，在http工具类中也是捕获了异常的，但是日志中没有任何响应相关的信息，感觉是请求发出去后，线程就阻塞了。经过排查发现，是http工具类的问题。项目中用的是HttpURLConnection，其中只设置了ConnectTimeout，没有设置ReadTimeout，导致请求没收到响应后没有执行后续查询相关操作。这里就记录一下ConnectTimeout、ReadTimeout所代表的意义。 ConnectTimeout 指的是建立连接所用的时间，适用于网络状况正常的情况下，两端连接所用的时间。在java中，网络状况正常的情况下，例如使用HttpClient或者HttpURLConnetion连接时设置参数connectTimeout=5000即5秒，如果连接用时超过5秒就是抛出java.net.SocketException: connetct time out的异常 ReadTimeout 指的是建立连接后从服务器读取到可用资源所用的时间（即传递数据的超时时间）。在这里我们可以这样理解ReadTimeout：正常情况下，当我们发出请求时可以收到请求的结果，也就是页面上展示的内容，但是当网络状况很差的时候，就会出现页面上无法展示出内容的情况。另外当我们使用爬虫或者其他全自动的程序时，无法判断当前的网络状况是否良好，此时就有了ReadTimeout的用武之地了，通过设置ReadTimeout参数，例：ReadTimeout=5000，超过5秒没有读取到内容时，就认为此次读取不到内容并抛出Java.net.SocketException: read time out的异常。根据上面关于ConnectTimeout和ReadTimeout的描述，在我们使用需要设置这两项参数的服务或程序时，应该对两项参数一起设置。 &emsp;&emsp;一般而言两项参数的数值可以设置成一样的，但根据笔者个人经验，可以把ReadTimeout设置的长一点，ConnectTimeout可以相对比较短，这是源于我们的网络状况一般较为稳定，连接时很少出现问题，但是读取时因为数据下载时的网络波动，出状况的可能性更大一些。]]></content>
      <tags>
        <tag>JAVA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[什么是IOC]]></title>
    <url>%2F2020%2F08%2F08%2F%E4%BB%80%E4%B9%88%E6%98%AFIOC%2F</url>
    <content type="text"><![CDATA[1、IOC的理论背景&emsp;&emsp;我们知道在面向对象设计的软件系统中，它的底层都是由N个对象构成的，各个对象之间通过相互合作，最终实现系统地业务逻辑 图1 软件系统中耦合的对象 &emsp;&emsp;如果我们打开机械式手表的后盖，就会看到与上面类似的情形，各个齿轮分别带动时针、分针和秒针顺时针旋转，从而在表盘上产生正确的时间。图1中描述的就是这样的一个齿轮组，它拥有多个独立的齿轮，这些齿轮相互啮合在一起，协同工作，共同完成某项任务。我们可以看到，在这样的齿轮组中，如果有一个齿轮出了问题，就可能会影响到整个齿轮组的正常运转。 &emsp;&emsp;齿轮组中齿轮之间的啮合关系,与软件系统中对象之间的耦合关系非常相似。对象之间的耦合关系是无法避免的，也是必要的，这是协同工作的基础。现在，伴随着工业级应用的规模越来越庞大，对象之间的依赖关系也越来越复杂，经常会出现对象之间的多重依赖性关系，因此，架构师和设计师对于系统的分析和设计，将面临更大的挑战。对象之间耦合度过高的系统，必然会出现牵一发而动全身的情形。 图2 对象之间的依赖关系 &emsp;&emsp;耦合关系不仅会出现在对象与对象之间，也会出现在软件系统的各模块之间，以及软件系统和硬件系统之间。如何降低系统之间、模块之间和对象之间的耦合度，是软件工程永远追求的目标之一。 2、什么是IOC&emsp;&emsp;IOC是Inversion of Control的缩写，多数书籍翻译成“控制反转”。简单来说就是把复杂系统分解成相互合作的对象，这些对象类通过封装以后，内部实现对外部是透明的，从而降低了解决问题的复杂度，而且可以灵活地被重用和扩展。 &emsp;&emsp;IOC理论提出的观点大体是这样的：借助于“第三方”实现具有依赖关系的对象之间的解耦。如下图： 图3 IOC解耦过程 &emsp;&emsp;大家看到了吧，由于引进了中间位置的“第三方”，也就是IOC容器，使得A、B、C、D这4个对象没有了耦合关系，齿轮之间的传动全部依靠“第三方”了，全部对象的控制权全部上缴给“第三方”IOC容器，所以，IOC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IOC容器比喻成“粘合剂”的由来。 &emsp;&emsp;我们再来做个试验：把上图中间的IOC容器拿掉，然后再来看看这套系统： 图4 拿掉IOC容器之后的系统 &emsp;&emsp;我们现在看到的画面，就是我们要实现整个系统所需要完成的全部内容。这时候，A、B、C、D这4个对象之间已经没有了耦合关系，彼此毫无联系，这样的话，当你在实现A的时候，根本无须再去考虑B、C和D了，对象之间的依赖关系已经降低到了最低程度。所以，如果真能实现IOC容器，对于系统开发而言，这将是一件多么美好的事情，参与开发的每一成员只要实现自己的类就可以了，跟别人没有任何关系！ &emsp;&emsp;我们再来看看，控制反转(IOC)到底为什么要起这么个名字？我们来对比一下： &emsp;&emsp;软件系统在没有引入IOC容器之前，如图1所示，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。 &emsp;&emsp;软件系统在引入IOC容器之后，这种情形就完全改变了，如图3所示，由于IOC容器的加入，对象A与对象B之间失去了直接联系，所以，当对象A运行到需要对象B的时候，IOC容器会主动创建一个对象B注入到对象A需要的地方。 &emsp;&emsp;通过前后的对比，我们不难看出来：对象A获得依赖对象B的过程,由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。 3、IOC也叫依赖注入(DI)&emsp;&emsp;既然IOC是控制反转，那么到底是“哪些方面的控制被反转了呢？”，答案是：“获得依赖对象的过程被反转了”。控制被反转之后，获得依赖对象的过程由自身管理变为了由IOC容器主动注入。于是“控制反转”又有一个更合适的名字叫做“依赖注入（Dependency Injection）”。所谓依赖注入，就是由IOC容器在运行期间，动态地将某种依赖关系注入到对象之中。 所以，依赖注入(DI)和控制反转(IOC)是从不同的角度的描述的同一件事情，就是指通过引入IOC容器，利用依赖关系注入的方式，实现对象之间的解耦。]]></content>
      <categories>
        <category>JAVA</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java中CountDownLatch和CyclicBarrier的理解和区别]]></title>
    <url>%2F2020%2F08%2F06%2FJava%E4%B8%ADCountDownLatch%E5%92%8CCyclicBarrier%E7%9A%84%E7%90%86%E8%A7%A3%E5%92%8C%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[CountDownLatch和CyclicBarrier的功能看起来很相似，不易区分，有一种谜之的神秘。本文将通过通俗的例子并结合代码讲解两者的使用方法和区别。 CountDownLatch和CyclicBarrier都是java.util.concurrent（juc）包下面的多线程工具类。 从字面上理解，CountDown表示减法计数，Latch表示门闩的意思，计数为0的时候就可以打开门闩了。Cyclic Barrier表示循环的障碍物。两个类都含有这一个意思：对应的线程都完成工作之后再进行下一步动作，也就是大家都准备好之后再进行下一步。然而两者最大的区别是，进行下一步动作的动作实施者是不一样的。这里的“动作实施者”有两种，一种是主线程，另一种是执行任务的其他线程，后面叫这种线程为“其他线程”，区分于主线程。对于CountDownLatch，当计数为0的时候，下一步的动作实施者是main函数；对于CyclicBarrier，下一步动作实施者是“其他线程”。 下面举例说明： 对于CountDownLatch，其他线程为游戏玩家，比如英雄联盟，主线程为控制游戏开始的线程。在所有的玩家都准备好之前，主线程是处于等待状态的，也就是游戏不能开始。当所有的玩家准备好之后，下一步的动作实施者为主线程，即开始游戏。 对于CyclicBarrier，假设有一家公司要全体员工进行团建活动，活动内容为翻越三个障碍物，每一个人翻越障碍物所用的时间是不一样的。但是公司要求所有人在翻越当前障碍物之后再开始翻越下一个障碍物，也就是所有人翻越第一个障碍物之后，才开始翻越第二个，以此类推。类比地，每一个员工都是一个“其他线程”。当所有人都翻越的所有的障碍物之后，程序才结束。而主线程可能早就结束了，这里我们不用管主线程。 总结：CountDownLatch和CyclicBarrier都有让多个线程等待同步然后再开始下一步动作的意思，但是CountDownLatch的下一步的动作实施者是主线程，具有不可重复性；而CyclicBarrier的下一步动作实施者还是“其他线程”本身，具有往复多次实施动作的特点。]]></content>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA 中的 CAS]]></title>
    <url>%2F2020%2F08%2F06%2FJAVA-%E4%B8%AD%E7%9A%84-CAS%2F</url>
    <content type="text"><![CDATA[简介在计算机科学中，比较和交换（Conmpare And Swap）是用于实现多线程同步的原子指令。 它将内存位置的内容与给定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。 这是作为单个原子操作完成的。 原子性保证新值基于最新信息计算; 如果该值在同一时间被另一个线程更新，则写入将失败。 操作结果必须说明是否进行替换; 这可以通过一个简单的布尔响应（这个变体通常称为比较和设置），或通过返回从内存位置读取的值来完成。 一个 CAS 涉及到以下操作：我们假设内存中的原数据V，旧的预期值A，需要修改的新值B。 比较 A 与 V 是否相等。（比较）如果比较相等，将 B 写入 V。（交换）返回操作是否成功。当多个线程同时对某个资源进行CAS操作，只能有一个线程操作成功，但是并不会阻塞其他线程,其他线程只会收到操作失败的信号。可见 CAS 其实是一个乐观锁。 如上图中，主存中保存V值，线程中要使用V值要先从主存中读取V值到线程的工作内存A中，然后计算后变成B值，最后再把B值写回到内存V值中。多个线程共用V值都是如此操作。CAS的核心是在将B值写入到V之前要比较A值和V值是否相同，如果不相同证明此时V值已经被其他线程改变，重新将V值赋给A，并重新计算得到B，如果相同，则将B值赋给V。 如果不使用CAS机制，看看存在什么问题，假如V=1，现在Thread1要对V进行加1，Thread2也要对V进行加1，首先Thread1读取V=1到自己工作内存A中此时A=1，假设Thread2此时也读取V=1到自己的工作内存A中，分别进行加1操作后，两个线程中B的值都为2，此时写回到V中时发现V的值为2，但是两个线程分别对V进行加处理结果却只加了1有问题。 ABA 问题CAS 由三个步骤组成，分别是“读取-&gt;比较-&gt;写回”。考虑这样一种情况，线程1和线程2同时执行 CAS 逻辑，两个线程的执行顺序如下： 时刻1：线程1执行读取操作，获取原值 A，然后线程被切换走时刻2：线程2执行完成 CAS 操作将原值由 A 修改为 B时刻3：线程2再次执行 CAS 操作，并将原值由 B 修改为 A时刻4：线程1恢复运行，将比较值（compareValue）与原值（oldValue）进行比较，发现两个值相等。然后用新值（newValue）写入内存中，完成 CAS 操作 如上流程，线程1并不知道原值已经被修改过了，在它看来并没什么变化，所以它会继续往下执行流程。对于 ABA 问题，通常的处理措施是对每一次 CAS 操作设置版本号。java.util.concurrent.atomic 包下提供了一个可处理 ABA 问题的原子类 AtomicStampedReference。 ABA问题的解决办法 1.在变量前面追加版本号：每次变量更新就把版本号加1，则A-B-A就变成1A-2B-3A。2.atomic包下的AtomicStampedReference类：其compareAndSet方法首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用的该标志的值设置为给定的更新值。 其他问题 CAS除了ABA问题，仍然存在循环时间长开销大和只能保证一个共享变量的原子操作 1. 循环时间长开销大 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 2. 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 CAS 的应用 1.Java的concurrent包下就有很多类似的实现类，如Atomic开头那些。 2.自旋锁 3.令牌桶限流器 令牌桶限流器就是系统以恒定的速度向桶内增加令牌。每次请求前从令牌桶里面获取令牌。如果获取到令牌就才可以进行访问。当令牌桶内没有令牌的时候，拒绝提供服务。我们来看看 eureka 的限流器是如何使用 CAS 来维护多线程环境下对 token 的增加和分发的。]]></content>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka通过控制台模拟消息发送和消息接收正常,但是通过javaAPI操作生产者发送消息不成功]]></title>
    <url>%2F2020%2F04%2F19%2Fkafka%E9%80%9A%E8%BF%87%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%A8%A1%E6%8B%9F%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E5%92%8C%E6%B6%88%E6%81%AF%E6%8E%A5%E6%94%B6%E6%AD%A3%E5%B8%B8-%E4%BD%86%E6%98%AF%E9%80%9A%E8%BF%87javaAPI%E6%93%8D%E4%BD%9C%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E4%B8%8D%E6%88%90%E5%8A%9F%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;通过命令行工具kafka-console-producer.sh和kafka-console-consumer.sh是能够相互通信的，producer发布的信息consumer能够接收到。 &emsp;&emsp;但是java通过kafka-client的API写的代码始终不能跟kafka通信：java producer的消息发不出去， java comsumer也收不到任何消息。仔细检查了下代码中IP、端口都没有写错。 解决办法：将kafka/config/server.properties文件中advertised.listeners改为如下属性。 1advertised.listeners=PLAINTEXT://192.168.17.101:9092 192.168.17.101是kafka所在服务器的IP。改完后重启，OK了。Java端的代码就能通信了]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客中公式无法显示的问题]]></title>
    <url>%2F2020%2F01%2F04%2FHexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%85%AC%E5%BC%8F%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 在文章的Front-matter里打开mathjax开关，如下：12345678910title: Kafka进阶author: kaizhangtags: - Kafka categories: - 中间件 date: 2020-01-02 20:20:00mathjax: true]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka进阶]]></title>
    <url>%2F2020%2F01%2F02%2FKafka%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[第1章 Kafka 架构深入1.1 Kafka 工作流程及文件存储机制 Kafka 工作流程 &emsp;&emsp;图中的0、1、2、3等就是偏移量”offset“，每个分区都有自己的从头开始的偏移量，是每个分区独立的，所以kafka中不能保证消息的全局有序性，只能保证区内有序。 &emsp;&emsp;Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic的。 &emsp;&emsp;topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。 Kafka文件存储机制 &emsp;&emsp;由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment（当log文件达到指定大小时，自动生成新文件，默认是1G）。每个 segment对应两个文件——“.index”文和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0, first-1, first-2。 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示图。 &emsp;&emsp;“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元 数据指向对应数据文件中message 的物理偏移地址。 1.2 Kafka生产者1.2.1 分区策略（1） 分区的原因 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发，因为可以以 Partition 为单位读写了。 （2）分区的原则&emsp;&emsp;我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 1.2.2 数据可靠性保证&emsp;&emsp;为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 （1）副本数据同步策略 方案 优点 缺点 半数以上完成同步，就发送 ack 延迟低 选举新的 leader 时，容忍 n 台节点的故障，需要 2n+1 个副本 全部完成同步，才发送 ack 选举新的 leader 时，容忍 n 台节点的故障，需要 n+1 个副本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1个副本，而Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。【半数以上完成同步才可以发ACK，如果挂了n台，那么就需要有另外n台正常发送（这样正常发送的刚好是总数（挂的和没挂的）的一半（n（挂的）+n（正常的）=2n））,因为是半数以上所以2n+1.(所以总数2n+1的时候最多只能容忍n台有故障)】 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 （2）ISR&emsp;&emsp;采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据，但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去，直到它完成同步，才能发送 ack。这个问题怎么解决呢？ &emsp;&emsp;Leader 维护了一个动态的 in-sync replica set (ISR，同步副本)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower长时间未向 leader 同 步 数 据 ，则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 （3）ack 应答机制&emsp;&emsp;对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。&emsp;&emsp;所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。acks 参数配置：&emsp;&emsp;acks： 0： &emsp;&emsp;producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1： &emsp;&emsp;producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower（ISR中的follower）同步成功之前 leader 故障，那么将会丢失数据；该值为ack的默认值 -1(或者all)： &emsp;&emsp;producer 等待 broker 的 ack，partition 的 leader 和 follower（ISR中的follower）全部落盘成功后才返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。 &emsp;&emsp;但是这样就代表数据一定不会丢失了吗？当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。这样才能保证说写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。 （4）故障处理细节 LEO：称为“日志末端偏移量”，指的是每个副本最大的 offset；HW：称为“高水位”，指的是消费者能见到的最大的 offset，消费者只能拉取到这个offset之前的消息，ISR 队列中最小的 LEO。 1）follower 故障 &emsp;&emsp;follower 发生故障后会被临时踢出 ISR，待 该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 2）leader 故障 &emsp;&emsp;leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。 &emsp;&emsp;注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 1.2.3 Exactly Once 语义&emsp;&emsp;将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即 At Most Once 语义。 &emsp;&emsp;At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的，At Most Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 &emsp;&emsp;0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即： At Least Once + 幂等性 = Exactly Once&emsp;&emsp;要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。 &emsp;&emsp;但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 1.3 Kafka消费者1.3.1 消费方式&emsp;&emsp;consumer 采用 pull（拉）模式从 broker 中读取数据。&emsp;&emsp;push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。&emsp;&emsp;pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为 timeout。 1.3.2 分区分配策略&emsp;&emsp;一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 &emsp;&emsp;Kafka分区分配策略，就是它的内部的默认的分区分配策略：Range（范围） 和 RoundRobin（轮询）。当下面的事情发生的时候，Kafka将会进行一次分区分配。 ​ 1、当同一个Consumer Group 内新增消费者。（消费者层面发生改动） ​ 2、消费者离开当前所属的Consumer Group，包括Shuts down（关闭、停工）或者是crashes（崩溃）。（消费者层面发生改动） ​ 3、消费者订阅的主题新增分区的时候 &emsp;&emsp;将分区的所有权从一个消费者移到另一个消费者成为重新平衡（rebalance），如何rebalance就涉及到分区分配策略。 &emsp;&emsp;我们假设有一个名为Top1的主题，它有十个分区，然后我们有两个消费者（C1、C2）来消费这十个分区里面的数据，而且C1的num.streams=1，C2的num.streams=2。 第一个默认的分区策略：Range Startegy（根据范围消费） &emsp;&emsp;Range startegy是对每个主题而言的 ， 首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母进行排序。在对十个分区排序的话是0-9；消费者线程排完序是C1-0，C2-0，C2-1。然后用partitions的总数除以消费者的总数来决定每个消费者线程消费几个分区。如果有余数，那么前面的几个消费者线程将会多消费一个分区。在我们的例子里面，我们有十个分区，三个消费者线程，10/3=3—-1，那么消费者线程C1-0 将会多消费一个分区，所以最后分区分配的结构看起来是这样的： 12345C1-0将消费0,1,2,3分区C2-0将消费4,5,6分区C2-1将消费7,8,9分区 如果有第十一个分区的话，那么分区是这样的： 12345C1-0将消费0,1,2,3分区C2-0将消费4,5,6,7分区C2-1将消费8,9,10分区 如果我们有2个主题（T1和T2），分别都有十个分区，那么最后的分配结果是： 12345C1-0将消费T1主题中的0,1,2,3分区以及T2主题中0,1,2,3分区C2-0将消费T1主题中的4,5,6分区以及T2主题中的4,5,6,分区C2-1将消费T1主题中的7,8,9分区以及T2主题中的7,8,9分区 &emsp;&emsp;这就是消费的策略！ 就是用总的分区数/消费者线程总数=每个消费者线程应该消费的分区数。当还有余数的时候就将余数分别分发到另外的消费组线程中。 &emsp;&emsp;在这里我们不难看出来。C1-0消费者线程比其他消费者线程多消费了两个分区，这就是Range Strategy的一个明显的弊端。 当分区很多的时候，会有个别的线程压力巨大！ 第二个默认的分区策略：RoundRobin strategy（轮询的消费策略） 在使用RoundRobin Starategy的时候我们必须满足两个条件： 1、同一个consumer Group里面的所有消费者的num.streams必须相等； 2、每个消费者订阅的 主题必须相同; &emsp;&emsp;在这里我们假设2个消费者的num.streams=2. RoundRobin starategy的工作原理： 将所有主题的分区组成TopicAndPartition列表，然后对TopAndPartition列表按照hashcode进行排序。最后按照round-robin风格将分区分别分配给不同的消费者线程。 &emsp;&emsp;在我们的例子中，假如按照hashcode排序完的topic-partition组依次为T1-5,T1-3,T1-0.T1-8.T1-2,T1-1,T1-4,T1-6,T1-9，我们的消费者线程排序为C1-0, C1-1 ,C2-0,C2-1，最后分区分配结果为： 1234567C1-0将消费T1-5 , T1-2 , T1-6 分区；C1-1将消费T1-3 , T1-1 , T1-9 分区；C2-0将消费T1-0 , T1-4分区；C2-1将消费T1-8 , T1-7分区； &emsp;&emsp;我们可以通过partition.assignment.strategy参数选择range或roundrobin。 partition.assignment.strategy参数默认的值是range。 1.3.3 offset 的维护&emsp;&emsp;由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 &emsp;&emsp;Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 1）修改配置文件 consumer.properties 12exclude.internal.topics=false（没有就新增） 2）读取 offset ​ 0.11.0.0 之前版本: 1bin/kafka-console-consumer.sh --topic __consumer_offsets -zookeeper server1:2181/kafka --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config/consumer.properties --from-beginning ​ 0.11.0.0 之后版本(含): 1bin/kafka-console-consumer.sh --topic __consumer_offsets -zookeeper server1:2181/kafka --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageForm atter&quot; --consumer.config config/consumer.properties --frombeginning 1.3.4 消费者组案例1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 2）案例实操 （1）在 server1、server2 上修改/opt/software/kafka/config/consumer.properties 配置文件中的 group.id 属性为任意组名。 12[kaizhang@server1 config]$ vi consumer.properties group.id=kaizhang （2）在 server1、server2上分别启动消费者 12[kaizhang@server1 kafka]$ bin/kafka-console-consumer.sh --zookeeper server1:2181/kafka --topic first --consumer.config config/consumer.properties[kaizhang@server2 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first --consumer.config config/consumer.properties （3）在 server3上启动生产者 12[kaizhang@server3 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first &gt;hello world （4）查看 server1和 server2的接收者。 &emsp;&emsp;同一时刻只有一个消费者接收到消息。 1.4 Kafka 高效读写数据1）顺序写磁盘 &emsp;&emsp;Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零拷贝技术 &emsp;&emsp;零拷贝：零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。 传统的读取文件数据并发送到网络的步骤如下： （1）操作系统将数据从磁盘文件中读取到内核空间的页面缓存； （2）应用程序将数据从内核空间读入用户空间缓冲区； （3）应用程序将读到数据写回内核空间并放入socket缓冲区； （4）操作系统将数据从socket缓冲区复制到网卡接口，此时数据才能通过网络发送。 &emsp;&emsp;“零拷贝技术”只用将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中（发送给不同的订阅者时，都可以使用同一个页面缓存），避免了重复复制操作。 &emsp;&emsp;如果有10个消费者，传统方式下，数据复制次数为4*10=40次，而使用“零拷贝技术”只需要1+10=11次，一次为从磁盘复制到页面缓存，10次表示10个消费者各自读取一次页面缓存。 1.5 Zookeeper 在 Kafka 中的作用&emsp;&emsp;Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。&emsp;&emsp;Controller 的管理工作都是依赖于 Zookeeper 的。 以下为 partition 的 leader 选举过程：]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka入门+实战]]></title>
    <url>%2F2019%2F11%2F30%2FKafka%E5%85%A5%E9%97%A8%2B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Kafka入门+实战第 1 章 Kafka概述1.1 定义Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 1.2 消息队列1.2.1 传统消息队列的应用场景MQ传统应用场景之异步处理： 使用消息队列的好处 解耦允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况（生产消息的速度 &gt; 消费消息的速度）。 灵活性 &amp; 峰值处理能力 （削峰）在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 1.2.2 消息队列的两种模式 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。 消息被消费以后，Queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布/订阅模式（一对多，消费者消费数据之后不会清除消息） 消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。其中，发布/订阅模式又分为两种： 一种为消费者主动拉取队列中数据(Kafka就属于这种模式)。这种模式有一个缺点就是要维护一个长轮询，不断去拉取数据，造成资源浪费。 一种为由队列主动推送给消费者。 图为发布订阅模式： 1.3 Kafka 基础架构Kafka架构图： Producer ：消息生产者，就是向 kafka broker 发消息的客户端； Consumer ：消息消费者，向 kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 如果分区数大于或者等于组中的消费者实例数，一个消费者会负责多个分区。当然最理想的是分区数等于组中的消费者实例数。 Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。 Topic ：可以理解为一个队列，生产者和消费者面向的都是一个 topic Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。 follower：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 leader。 第 2 章 Kafka 快速入门2.1 安装部署2.1.1 集群规划linux1 linux2 linux3 zk zk zk kafka kafka kafka 2.1.2 安装包下载下载地址 这里使用0.11.0.0版本 2.1.3 集群部署 下面流程中，只有配置文件中的broker.id不同，其他在各个机器上都是相同的操作 1、解压安装包：把下载的安装包放到/opt/software下并解压 1[kaizhang@localhost software]$ tar -zxvf kafka_2.11-0.11.0.0.gz 2、修改解压后的文件名 1[kaizhang@localhost software]$ mv kafka_2.11-0.11.0.0 kafka 3、在/opt/software/kafka 目录下创建 logs 文件夹 1[kaizhang@localhost kafka_2.11-0.11.0.0]$ mkdir logs 4、修改配置文件 12[kaizhang@localhost kafka_2.11-0.11.0.0]$ cd config[kaizhang@localhost config]$ vi server.properties ​ 修改部分配置如下：(其中三台centos中broker.id分别是1、2、3，不得重复) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # 基本配置 # The id of the broker. This must be set to a unique integer for each broker. broker id, id必须是唯一的整数 broker.id=0 # Switch to enable topic deletion or not, default value is false # 是否可以删除topic，如果为true，我们可以在命令行删除topic，否则，不能。 #delete.topic.enable=true ############################# Socket Server Settings ############################# # socket配置 # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 # broker监听地址。如果没有配置，默认为java.net.InetAddress.getCanonicalHostName()方法返回的地址 #listeners=PLAINTEXT://:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for "listeners" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). # broker的主机名和端口号将会广播给消费者与生产者。如果没有设置，默认为监听配置，否者使用 # java.net.InetAddress.getCanonicalHostName()方法返回的地址 #advertised.listeners=PLAINTEXT://your.host.name:9092，每台服务器填写各自ip advertised.listeners=PLAINTEXT://192.168.17.101:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details # 监听协议，默认为PLAINTEXT #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network # 服务器接受请求和相应请求的线程数 num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O # 处理请求的线程数，包括磁盘的IO操作 num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server # 服务器socket发送缓存 socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server # 服务器socket接受缓存 socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) # 服务器接收请求的最大值 socket.request.max.bytes=104857600 ############################# Log Basics ############################# # log基本配置 # A comma seperated list of directories under which to store log files # kafka运行日志存放的路径log.dirs=/opt/software/kafka/logs # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. # 每个topic的默认日志分区数。允许分区数大于并行消费数，这样可能导致，更多的文件将会跨broker num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. # 在启动和关闭刷新时，没有数据目录用于日志恢复的线程数。 # 这个值，强烈建议在随着在RAID阵列中的安装数据目录的增长而增长。 num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # 内部topic配置 # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state" # For anything other than development testing,a value greater than 1 is recommended for to ensure availability such as 3. # 内部__consumer_offsets和__transaction_state两个topic，分组元数据的复制因子。 # 除开发测试外的使用，强烈建议值大于1，以保证可用性，比如3。 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # 日志刷新策略 # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 消息立刻被写到文件系统，默认调用fsync方法，懒同步操作系统缓存。下面的配置用于控制刷新数据到磁盘。 # 这里是一些折中方案： # 1. Durability: Unflushed data may be lost if you are not using replication. # 持久性：如果没有使用replication，没刷新的数据可能丢失。 # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 延迟性：当有大量的数据需要刷新，刷新操作发生时，比较大的刷新间隔可能会导致延时。 # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # 吞吐量：刷新操作代价比较高，较小的刷新间隔，将会引起过渡的seek文件操作。 # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # 下面的配置刷新策略，允许在一个的刷新间隔或消息数量下，刷新数据，这个配置是全局的，可以在每个主题下重写。 # The number of messages to accept before forcing a flush of data to disk # 在强制刷新数据到磁盘前，允许接受消息数量 #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush # 在强制刷新前，一个消息可以日志中停留在最大时间 #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # 日志保留策略 # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # 下面的配置用于控制日志segments的处理。这些策略可以在一定的时间间隔和数据累积到一定的size，可以删除segments。两种策略只要有 一种触发，segments将会被删除。删除总是从log的末端。 # The minimum age of a log file to be eligible for deletion due to age # log文件的保留的时间，超时将被删除log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don't drop below log.retention.bytes. Functions independently of log.retention.hours. # log文件保留的size #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. # 日志segments文件最大size，当日志文件的大于最大值，则创建一个新的log segment log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies # 日志保留检查间隔 log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper配置 # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 配置连接Zookeeper集群地址，zk允许每个客户端为自己设置已给命名空间。如果一个zookeeper客户端设置了Chroot，那么该客户端对服务器的任何操作，都将会被限制在自己的命名空间下。# 客户端可以通过在connectString中添加后缀的方式来设置Chroot，如下所示：192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181/apps/X # 这个client的chrootPath就是/apps/X，将这样一个connectString传入客户端的ConnectStringParser后就能够解析出Chroot并保存在chrootPath属性中zookeeper.connect=server1:2181,server2:2181,server3:2181/kafka # Timeout in ms for connecting to zookeeper # 连接zookeeper超时时间 zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # 分组协调配置 # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. # 下面的配置为毫秒时间，用于延时消费者重平衡的时间。重平衡将会进一步在新成员添加分组是，延时group.initial.rebalance.delay.ms时间，直到到达maximum of max.poll.interval.ms时间。 # 默认值为3秒，我们重写0，主要是用户开发测试体验。在生产环境下，默认值3s，在应用启动期间， 帮助避免不必要及潜在的代价高的rebalances，是比较合适的。 group.initial.rebalance.delay.ms=0 5、配置环境变量 1234567kaizhang@localhost config]$ sudo vi /etc/profile #KAFKA_HOMEexport KAFKA_HOME=/opt/software/kafkaexport PATH=$PATH:$KAFKA_HOME/bin [kaizhang@localhost config]$ source /etc/profile 6、启动集群 ​ 依次在三台机器上启动kafka 123[kaizhang@server1 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties [kaizhang@server2 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties [kaizhang@server3 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties 7、关闭集群 123[kaizhang@server1 kafka]$ bin/kafka-server-stop.sh[kaizhang@server2 kafka]$ bin/kafka-server-stop.sh[kaizhang@server3 kafka]$ bin/kafka-server-stop.sh 8、kafka操作脚本 123456789101112131415#!/bin/bashcase $1 in"start")&#123; echo "========== server1 start ==========" sh /opt/software/kafka/bin/kafka-server-start.sh -daemon /opt/software/kafka/config/server.properties&#125;;case $1 in"stop")&#123; echo "========== server1 stop ===========" sh /opt/software/kafka/bin/kafka-server-stop.sh -daemon /opt/software/kafka/config/server.properties&#125;; esac 2.2 Kafka命令行操作(在/opt/software/kafka目录下执行命令)1、查看当前服务器中的所有 topic 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --list__consumer_offsetsfivefour 2、创建 topic 12[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 2 --partitions 3 --topic firstCreated topic "first". 选项说明：—topic 定义 topic 名—replication-factor 定义副本数—partitions 定义分区数 在创建完topic后，可以到kafka数据文件中查看，这里到设置的目录里查看即 log.dirs=/opt/software/kafka/logs 12drwxrwxr-x. 2 kaizhang kaizhang 141 12月 13 00:04 first-0drwxrwxr-x. 2 kaizhang kaizhang 141 12月 13 00:04 first-1 在first-0中，first是主题名称，0为分区 这里主题first是3个分区，2个副本，所以在三台服务器的日志文件中共可以看到6个first相关的数据： first-0、first-1、first-2、first-0、first-1、first-2 如果创建的是2个分区，3个副本，则数据为： first-0、first-0、first-0、first-1、first-1、first-1 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 0 --partitions 3 --topic firstError while executing topic command : replication factor must be larger than 0[2019-12-13 00:39:23,664] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor must be larger than 0 (kafka.admin.TopicCommand$) 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 4 --partitions 2 --topic firstError while executing topic command : replication factor: 4 larger than available brokers: 3[2019-12-13 00:22:07,440] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor: 4 larger than available brokers: 3 (kafka.admin.TopicCommand$) 这里可以看到，如果副本数等于0或者大于brokers的数量，都是会报错的 3、删除topic 123[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --delete --topic firstTopic first is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除。 4、发送消息 123[kaizhang@server1 kafka]$ bin/kafka-console-producer.sh --topic first --broker-list server1:9092&gt;hello world&gt;kafka 其中9092是kafka服务器需要监听的端口号，在server.properties文件中配置。 5、消费消息 123456#通过zookeeper消费，这是kafka 0.9版本之前的消费方式，0.9版本之前消息是存放在zookeeper中的[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --zookeeper server1:2181/kafka --topic first #通过kafka 0.9版本之后的消费方式[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first#从最初的的消息开始读取[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first --from-beginning --from-beginning：会把主题中以往所有的数据都读取出来。 6、查看某个 Topic 的详情 12345[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --describe --topic firstTopic:first PartitionCount:3 ReplicationFactor:2 Configs: Topic: first Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: first Partition: 1 Leader: 2 Replicas: 2,3 Isr: 2,3 Topic: first Partition: 2 Leader: 3 Replicas: 3,1 Isr: 3,1 7、修改分区数 123[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --alter --topic first --partitions 6 WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedAdding partitions succeeded! 12345678910111213[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --alter --topic first --partitions 2WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedError while executing topic command : The number of partitions for a topic can only be increased[2019-12-13 00:31:37,496] ERROR kafka.admin.AdminOperationException: The number of partitions for a topic can only be increased at kafka.admin.AdminUtils$.addPartitions(AdminUtils.scala:292) at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:147) at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:124) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:124) at kafka.admin.TopicCommand$.main(TopicCommand.scala:64) at kafka.admin.TopicCommand.main(TopicCommand.scala) (kafka.admin.TopicCommand$) 这里可以看到，分区数只能增加，不能减少 按照Kafka现有的代码逻辑而言，此功能完全可以实现，不过也会使得代码的复杂度急剧增大。实现此功能需要考虑的因素很多，比如删除掉的分区中的消息该作何处理？如果随着分区一起消失则消息的可靠性得不到保障；如果需要保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于Spark、Flink这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入到现有的分区中，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题、以及分区和副本的状态机切换问题都是不得不面对的。反观这个功能的收益点却是很低，如果真的需要实现此类的功能，完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。虽然分区数不可以减少，但是分区对应的副本数是可以减少的，这个其实很好理解，你关闭一个副本时就相当于副本数减少了。 不过正规的做法是使用kafka-reassign-partition.sh脚本来实现，具体用法可以自行搜索。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建及内部原理]]></title>
    <url>%2F2019%2F09%2F29%2FZookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[第1章 Zookeeper 实战1.1 分布式安装部署0）集群规划在CentOS_1、CentOS_2 和CentOS_3 三个服务器上部署 Zookeeper。 1）解压安装1、解压 zookeeper 安装包到/opt/software/目录下1[kaizhang@localhost software]$ tar -zxvf zookeeper-3.4.14.tar.gz 2、在/opt/software/zookeeper-3.4.14/这个目录下创建 zkData1[kaizhang@localhost zookeeper-3.4.14]$ mkdir -p zkData 3、复制/opt/software/zookeeper-3.4.14/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg1[kaizhang@localhost conf]$ cp zoo_sample.cfg zoo.cfg 2）配置 zoo.cfg 文件1、具体配置123456dataDir=/opt/software/zookeeper-3.4.14/zkData 增加如下配置 #######################cluster##########################server.1=192.168.17.101:2888:3888server.2=192.168.17.102:2888:3888server.3=192.168.17.103:2888:3888 2、配置参数解读1234567Server.A=B:C:D。 A 是一个数字，表示这个是第几号服务器； B 是这个服务器的 ip 地址； C 是这个服务器与集群中的 Leader 服务器交换信息的端口； D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。集群模式下配置一个文件myid，这个文件在zkData目录下，这个文件里面有一个数据就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个 server。 3） 修改日志输出位置 当执行zkServer.sh 时，会在执行命令的文件夹下会产生zookeeper.out日志文件记录zookeeper的运行日志，该种方式会让日志文件不便于查找，容易遗忘。这里修改日志输出到指定文件夹。 1、修改bin/log4j.properties文件 zookeeper.out文件属于运行时的日志文件，通过conf/log4j.properties文件配置。 1234567# 以下是原配置zookeeper.root.logger=INFO, CONSOLElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender# 以下是修改后配置zookeeper.root.logger=INFO, ROLLINGFILElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender 2、在/opt/software/zookeeper-3.4.14下创建日志目录1[kaizhang@localhost zookeeper-3.4.14]$ mkdir logs 3、修改bin/zkEnv.sh12345678910111213141516171819202122# 以下是原配置if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;.&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,CONSOLE&quot;fi# 以下是修改后配置if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;/opt/software/zookeeper-3.4.14/logs&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot;fi 3）集群操作1、在/opt/software/zookeeper-3.4.14/zkData 目录下创建一个 myid 的文件1[kaizhang@localhost zkData]$ touch myid 添加 myid 文件，注意一定要在 linux 里面创建，在 notepad++里面很可能乱码 2、编辑 myid 文件1[kaizhang@localhost zkData]$ vi myid 在文件中添加与 server 对应的编号：如 1 3、在其他机器上进行相同的操作 ，并分别修改 myid 文件中内容为 2、34、在三台机器上分别启动 zookeeper1[kaizhang@localhost bin]$ ./zkServer.sh start 5、查看状态1234567891011121314[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: leader[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: follower[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: follower 6、操作脚本12345678910111213141516171819#!/bin/bashcase $1 in"start")&#123; echo "========== server2 start ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh start&#125;;;"stop")&#123; echo "========== server2 stop ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh stop&#125;;;"status")&#123; echo "========== server2 status ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh status&#125;;;esac 7、Chroot：客户端隔离命名空间​ 在3.2.0及其之后版本的ZooKeeper中，添加了“Chroot”特性，该特性允许每个客户端为自己设置一个命名空间（Namespace）。如果一个ZooKeeper客户端设置了Chroot，那么该客户端对服务器的任何操作，都将会被限制在其自己的命名空间下。 ​ 如果使用 “127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a” ，客户端将把”/app/a”作为跟路径，并且所有的路径都与这个根路径相关，比如getting、setting等。”/foo/bar” 将导致操作在”/app/a/foo/bar”（从服务端的观点来看）。这个特性在多租户下面是非常也有用的，ZooKeeper服务的每个用户可以有不同的根路径。这让再使用变得非常简单，因为每个用户都可以编写代码让他的应用好像在”/”根路径下，但实际的位置能在部署时决定，这对于实现不同应用之间的相互隔离非常有帮助。 ​ 客户端可以通过在connecString中添加后缀的方式来设置Chroot，如下所示： 192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181/apps/a ​ 将这样一个connectString传入客户端的ConnectStringParser后就能够解析出Chroot并保存在chrootPath属性中 1.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 ​ 1）启动客户端 1[kaizhang@localhost bin]$ ./zkCli.sh ​ 2）显示所有操作命令 1[zk: localhost:2181(CONNECTED) 0] help ​ 3）查看当前 znode 中所包含的内容 12[zk: localhost:2181(CONNECTED) 1] ls /[zookeeper] ​ 4）查看当前节点数据并能看到更新次数等数据 12345678910111213[zk: localhost:2181(CONNECTED) 2] ls2 /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 ​ 5）创建普通节点 12345[zk: localhost:2181(CONNECTED) 3] create /app1 &quot;hello app1&quot; Created /app1[zk: localhost:2181(CONNECTED) 4] create /app1/server101 &quot;192.168.1.101&quot;Created /app1/server101 ​ 6）获得节点的值 123456789101112131415161718192021222324252627[zk: localhost:2181(CONNECTED) 5] get /app1hello app1cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x100000004mtime = Mon Sep 30 21:27:00 CST 2019pZxid = 0x100000007cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 1[zk: localhost:2181(CONNECTED) 6] get /app1/server101192.168.1.101cZxid = 0x100000007ctime = Mon Sep 30 21:28:25 CST 2019mZxid = 0x100000007mtime = Mon Sep 30 21:28:25 CST 2019pZxid = 0x100000007cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0 ​ 7）创建短暂节点 12[zk: localhost:2181(CONNECTED) 8] create -e /app-emphemeral 8888 Created /app-emphemeral ​ （1）在当前客户端是能查看到的 12[zk: localhost:2181(CONNECTED) 6] ls / [zookeeper, app1, app-emphemeral] ​ （2）退出当前客户端然后再重启客户端 12[zk: localhost:2181(CONNECTED) 7] quit[kaizhang@localhost bin]$ ./zkCli.sh ​ （3）再次查看根目录下短暂节点已经删除 12[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper, app1] ​ 8）创建带序号的节点 ​ （1）先创建一个普通的根节点 app2 12[zk: localhost:2181(CONNECTED) 2] create /app2 &quot;app2&quot; Created /app2 ​ （2）创建带序号的节点 123456[zk: localhost:2181(CONNECTED) 4] create -s /app2/aa 888Created /app2/aa0000000000[zk: localhost:2181(CONNECTED) 5] create -s /app2/bb 888Created /app2/bb0000000001[zk: localhost:2181(CONNECTED) 6] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有 1 个节点，则再排序时从 1 开始，以此类推。 12[zk: localhost:2181(CONNECTED) 7] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 123456789101112[zk: localhost:2181(CONNECTED) 8] set /app1 999 cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x10000001fmtime = Mon Sep 30 21:46:15 CST 2019pZxid = 0x100000007cversion = 1dataVersion = 3aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 1 10）节点的值变化监听 ​ （1）在 101 主机上注册监听/app1 节点数据变化 1[zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在 102 主机上修改/app1 节点的数据 1[zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察 101 主机收到数据变化的监听 123WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） （1）在 101 主机上注册监听/app1 节点的子节点变化 12[zk: localhost:2181(CONNECTED) 17] ls /app1 watch[server101] ​ （2）在 102 主机/app1 节点上创建子节点 12[zk: localhost:2181(CONNECTED) 15] create /app1/bb 666Created /app1/bb ​ （3）观察 101 主机收到子节点变化的监听 123WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 1[zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 1[zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 123456789101112[zk: localhost:2181(CONNECTED) 8] stat /app1 cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x100000021mtime = Mon Sep 30 21:51:53 CST 2019pZxid = 0x100000022cversion = 2dataVersion = 5aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 2 1.3 API 应用参考： https://github.com/SparksFlyMe/zookeeperlearning 第2章 Zookeeper 内部原理2.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以 zookeeper适合装在奇数台机器上。 2）Zookeeper 虽然在配置文件中并没有指定 master 和 slave。但是，zookeeper 工作时，是有一个节点为 leader，其他则为 follower，Leader 是通过内部的选举机制临时产生的 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的 zookeeper 集群，它们的 id 从 1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器 1 启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是 LOOKING 状态。 （2）服务器 2 启动，它与最开始启动的服务器 1 进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以 id 值较大的服务器 2 胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是 3)，所以服务器 1、2 还是继续保持LOOKING 状态。 （3）服务器 3 启动，根据前面的理论分析，服务器 3 成为服务器 1、2、3 中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的 leader。 （4）服务器 4 启动，根据前面的分析，理论上服务器 4 应该是服务器 1、2、3、4 中最大的，但是由于前面已经有半数以上的服务器选举了服务器 3，所以它只能接收当小弟的命了。 （5）服务器 5 启动，同 4 一样当小弟。 2.2 节点类型1）Znode 有两种类型：短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode 有四种形式的目录节点（默认是 persistent ）（1）持久化目录节点（PERSISTENT） 客户端与 zookeeper 断开连接后，该节点依旧存在（2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号 ​ （3）临时目录节点（EPHEMERAL）客户端与 zookeeper 断开连接后，该节点被删除（4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号 3）创建 znode 时设置顺序标识，znode 名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 2.3 stat 结构体1）czxid- 引起这个 znode 创建的 zxid，创建节点的事务的 zxid 每次修改 ZooKeeper 状态都会收到一个 zxid 形式的时间戳，也就是 ZooKeeper 事务 ID。事务 ID 是 ZooKeeper 中所有修改总的次序。每个修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。 2）ctime - znode 被创建的毫秒数(从 1970 年开始)3）mzxid - znode 最后更新的 zxid4）mtime - znode 最后修改的毫秒数(从 1970 年开始)5）pZxid-znode 最后更新的子节点 zxid6）cversion - znode 子节点变化号，znode 子节点修改次数7）dataversion - znode 数据变化号8）aclVersion - znode 访问控制列表的变化号9）ephemeralOwner- 如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。10）dataLength- znode 的数据长度11）numChildren - znode 子节点数量 2.4 监听器原理 1）监听原理详解：（1）首先要有一个 main()线程（2）在 main 线程中创建 Zookeeper 客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 （3）通过 connect 线程将注册的监听事件发送给 Zookeeper。（4）在 Zookeeper 的注册监听器列表中将注册的监听事件添加到列表中。（5）Zookeeper 监听到有数据或路径变化，就会将这个消息发送给 listener 线程。（6）listener 线程内部调用了 process（）方法。 2）常见的监听（1）监听节点数据的变化： 1get path [watch] （2）监听子节点增减的变化 1ls path [watch] 2.5 写数据流程 ZooKeeper 的写数据流程主要分为以下几步：1）比如 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。 2）如果 Server1 不是 Leader，那么 Server1 会把接受到的请求进一步转发给 Leader，因为每个 ZooKeeper 的 Server 里面有一个是 Leader。这个 Leader 会将写请求广播给各个Server，比如 Server1 和 Server2， 各个 Server 写成功后就会通知 Leader。 3）当 Leader 收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader 会告诉 Server1 数据写成功了。 4）Server1 会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。ZooKeeper 整个写数据流程就是这样的。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper概述及单机搭建]]></title>
    <url>%2F2019%2F09%2F28%2FZookeeper%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[第 1 章 Zookeeper 概述1.1 概述 Zookeeper 是一个开源的分布式的，为分布式应用提供协调服务的 Apache 项目。 Zookeeper 从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式 。 Zookeeper=文件系统+通知机制。 1.2 特点1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。2）Leader 负责进行投票的发起和决议，更新系统状态3）Follower 用于接收客户请求并向客户端返回结果，在选举 Leader 过程中参与投票4）集群中只要有半数以上节点存活，Zookeeper 集群就能正常服务。5）全局数据一致：每个 server 保存一份相同的数据副本，client 无论连接到哪个 server，数据都是一致的。6）更新请求顺序进行，来自同一个 client 的更新请求按其发送顺序依次执行。7）数据更新原子性，一次数据更新要么成功，要么失败。8）实时性，在一定时间范围内，client 能读到最新数据。 1.3 数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。 很显然 zookeeper 集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个 znode 默认能够存储 1MB 的数据，每个 ZNode都可以通过其路径唯一标识 1.4 应用场景提供的服务包括：分布式消息同步和协调机制、服务器节点动态上下线、统一配置管理、负载均衡、集群管理等。 1.5 下载地址https://zookeeper.apache.org 第二章 Zookeeper 安装2.1 本地模式安装部署 （单机版）1）安装前准备： ​ （1）安装 jdk​ （2）通过 xshell等工具拷贝 zookeeper 到 linux 系统下​ （3）解压到指定目录 1[kaizhang@localhost software]$ tar -zvxf zookeeper-3.4.14.tar.gz 2）配置修改 ​ （1）将/opt/software/zookeeper-3.4.14/conf 这个路径下的 zoo_sample.cfg 复制为 zoo.cfg： 1[kaizhang@localhost conf]$ cp zoo_sample.cfg zoo.cfg ​ （2）进入 zoo.cfg 文件：vim zoo.cfg ​ 修改 dataDir 路径为​ dataDir=/opt/software/zookeeper-3.4.14/zkData ​ （3）在/opt/software/zookeeper-3.4.14/这个目录上创建 zkData 文件夹 1[kaizhang@localhost zookeeper-3.4.14]$ mkdir zkData 3）操作 zookeeper ​ （1）启动 zookeeper ： 1[kaizhang@localhost bin]$ ./zkServer.sh start ​ （2）查看进程是否启动 ： 123[kaizhang@localhost bin]$ jps8465 Jps7751 QuorumPeerMain ​ （3）查看状态： 1234[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: standalone ​ （4）启动客户端： 1[kaizhang@localhost bin]$ ./zkCli.sh ​ （5）退出客户端： 1[zk: localhost:2181(CONNECTED) 0] quit ​ （6）停止 zookeeper ： 1[kaizhang@localhost bin]$ ./zkServer.sh stop 2.2 配置参数解读解读zoo.cfg 文件中参数含义1）tickTime：通信心跳数。Zookeeper服务器心跳时间，单位毫秒 ​ Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每隔tickTime时间就会发送一个心跳，时间单位为毫秒。 ​ 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit：LF初始通信时限 集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 ​ 投票选举新leader的初始化时间​ Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 ​ Leader允许F在initLimit时间内完成这个工作。 3）syncLimit：LF 同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort：客户端连接端口 监听客户端连接的端口]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring自动装配]]></title>
    <url>%2F2019%2F07%2F21%2FSpring%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Spring的自动装配功能：自动装配：Spring利用依赖注入（DI），完成对IOC容器中各种组件的依赖关系赋值@Autowired的使用场景在构造器（CONSTRUCTOR），参数（PARAMETER），方法（METHOD），属性（FIELD），注释类型（ANNOTATION_TYPE）上，都能使用这个注解{@link Autowired}。且都是从容器中获取参数组件的值 标注在构造器上：默认加载在ioc容器中的组件，容器启动会调用无参构造器创建对象，再进行初始化赋值等操作。 构造器要用的组件，都是从容器中获取。如果组件只有一个有参构造器，这个有参构造器的@Autowired可以省略，参数位置的组件还是可以自动从容器中获取。 12345678910@Componentpublic class PersonService &#123; private Student student; @Autowired //可以省略 public PersonService(Student student)&#123; this.student = student; System.out.println("PersonService 有参构造器。。。"); &#125;&#125; 标注在参数上： 12345678@Componentpublic class PersonService &#123; private Student student; public PersonService(@Autowired Student student)&#123; this.student = student; &#125;&#125; 标注在方法上：Spring容器创建当前对象，就会调用方法，完成赋值；方法使用的参数，自定义类型的值从ioc容器中获取。常用的有：@Bean + 方法参数，参数从容器中获取 1234567891011121314@Componentpublic class PersonService &#123; private Student student; /** * * @param student 这里方法的参数（student）的值，从ioc容器中获取 * @return */ @Autowired public Student getStudent(Student student) &#123; return this.student = student; &#125;&#125; @Autowired的详细使用 默认优先按照类型去容器中找对应的组件。 1annotationConfigApplicationContext.getBean(BookDao.class); 如果找到多个相同类型的组件，再将属性的名称作为组件的id去容器中查找。 12345 /** * 这里bookDao为注入的属性名 */annotationConfigApplicationContext.getBean("bookDao") @Qualifier(“bookDao)：使用@Qualifier指定需要装配的组件的id，而不是使用属性名。 自动装配默认一定要将属性赋值好，如果没有就会报错。 @Autowired(required = true)，required默认为true，改为false后就不会报错 @Primary：让Spring进行自动装配的时候，默认使用首先的bean，也可以继续使用@Qualifier指定需要装配的bean的名字 @Autowired是Spring规范的注解，同时Spring还支持java规范的注解：@Resource（JSR250规范）和@Inject(JSR330规范) @Resource 可以和@Autowired一样实现自动装配功能；默认是按照组件名称进行装配的，不支持@Primary,也不支持@Autowired（require=false） @Inject 需要导入javax的包，和Autowired的功能一样。没有require=false的功能 注意事项这里说到的对各种组件的自动装配，前提是这些一定是Spring的组件（即需要添加@Configuration等注解）]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 注册组件的几种方式]]></title>
    <url>%2F2019%2F07%2F14%2FSpring-%E6%B3%A8%E5%86%8C%E7%BB%84%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在Spring中，给容器注册组件有多种方式，从之前的xml配置到后来的全注解配置，省去了复杂繁琐的xml配置，给开发人员带来的极大便利。给容器中注册组件的几种方式：1、包扫描 + 组件标注注解（@Controller、@Service、@Repository、@Component）2、@Bean（导入第三方包里面的组件，比如RestTemplate）3、@Import（快速给容器中导入一个组件） @Import（要导入到容器的组件）；容器中就会自动注册这个组件，id默认是全类名 实现ImportSelector类：返回需要导入的组件的全类名数组 实现ImportBeanDefinitionRegistrar：手动注册bean到容器中 4、使用Spring提供的FactoryBean(工厂Bean)]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis foreach循环 collection的三种方式]]></title>
    <url>%2F2019%2F07%2F13%2FMybatis-foreach%E5%BE%AA%E7%8E%AF-collection%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[自从有了Mybatis加强版Mybatis,再也不用写那么复杂麻烦的xml配置文件了！但是偶尔也有需求还得用xml，对于Mybatis的foreach也是经常用到的，但是每次都是从项目里粘贴复制、修修改改就用，对于其具体使用理解也是模模糊糊，导致有时用到的时候遇到各种问题，此文就针对Mybatis的foreach做个记录，以便更透彻的理解与使用。foreach标签主要用于构建in条件，它可以在sql中对集合进行迭代，通常可以将之用到批量删除、添加等操作中，示例如下： 1234567891011&lt;select id="getRequestLogList" parameterType="java.util.HashMap" resultType="RequestLog"&gt; SELECT * FROM request_log &lt;where&gt; &lt;if test="sessionIdList !=null and sessionIdList.size &gt; 0"&gt; and sessionId in ( &lt;foreach collection="sessionIdList" item="item" index="index" separator=","&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 加入我们输入的参数为 Long sessionIdList[] = {1L, 2L, 4L}; 那么对应执行的sql就是 ： 1SELECT * FROM request_log where sessionId in (1, 2, 4); foreach元素的属性主要有 item，index，collection，open，separator，close。 ​ item：表示集合中每一个元素进行迭代时的别名， ​ index：指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， ​ open：表示该语句以什么开始， ​ separator：表示在每次进行迭代之间以什么符号作为分隔 符， ​ close：表示以什么结束。 在使用foreach的时候最关键的也是最容易出错的就是collection属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： ​ 如果传入的是单参数且参数类型是一个List的时候，collection属性值为list ​ 如果传入的是单参数且参数类型是一个array数组的时候，collection的属性值为array ​ 如果传入的参数是多个的时候，我们就需要把它们封装成一个Map了，当然单参数也可 1、collection属性为List方法调用 123public List&lt;Area&gt; findUserListByIdList(List&lt;Long&gt; idList) &#123; return getSqlSession().findUserListByIdList(idList); &#125; 对应的mapper： 123456789&lt;select id="findUserListByIdList" parameterType="java.util.ArrayList" resultType="User"&gt; select * from user &lt;where&gt; ID in ( &lt;foreach collection="list" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/where&gt; &lt;/select&gt; 即单独传入list时，foreach中的collection必须是 list，不管变量的具体名称是什么。比如这里变量名为idList， collection却是List。 2、collection属性为array方法调用 123public List&lt;Area&gt; findUserListByIdList(int[] ids) &#123; return getSqlSession().findUserListByIdList(ids); &#125; 对应的mapper： 123456789&lt;select id="findUserListByIdList" parameterType="java.util.HashList" resultType="User"&gt; select * from user &lt;where&gt; ID in ( &lt;foreach collection="array" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/where&gt; &lt;/select&gt; 单独传入数组时，foreach中的collection必须是 array，不管变量的具体名称是什么。比如这里变量名为ids，collection却是array。 3、collection属性为map方法调用： 12345public boolean exists(Map&lt;String, Object&gt; map)&#123; Object count = getSqlSession().exists(map); int totalCount = Integer.parseInt(count.toString()); return totalCount &gt; 0 ? true : false; &#125; 对应的mapper： 1234567891011121314151617&lt;select id="exists" parameterType="java.util.HashMap" resultType="java.lang.Integer"&gt; SELECT COUNT(*) FROM USER user &lt;where&gt; &lt;if test="code != null"&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test="id != null"&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test="idList !=null "&gt; and ID in ( &lt;foreach collection="idList" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; map中有list或array时，foreach中的collection必须是具体list或array的变量名。比如这里map含有一个名为idList的list，所以map中用idList(map的key)来取值，这点和单独传list或array时不太一样。 4、传入java对象调用方法 12345public boolean findUserListByDTO(UserDTO userDTO)&#123; Object count = getSqlSession().findUserListByDTO(userDTO); int totalCount = Integer.parseInt(count.toString()); return totalCount &gt; 0 ? true : false; &#125; 对应的mapper： 1234567891011121314151617select id="findUserListByDTO" parameterType="UserDTO" resultType="java.lang.Integer"&gt; SELECT COUNT(*) FROM USER user &lt;where&gt; &lt;if test="code != null"&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test="id != null"&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test="idList !=null "&gt; and ID in ( &lt;foreach collection="idList" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; JAVA对象中有list或array时，foreach中的collection必须是具体list或array的变量名。比如这里UserDTO含有一个名为idList的list，所以UserDTO中用idList取值，这点和单独传list或array时不太一样。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mybatis foreach 使用错误记录]]></title>
    <url>%2F2019%2F07%2F13%2FMybatis-foreach-%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[使用Mybatis foreach遍历时，Error evaluating expression ‘sessionIdList’. Return value () was not iterable.问题的起因时这样的，同事用map方式传值，map里需要存入一个集合，但是在判断集合为空时放了空字符串，导致mybatis在遍历时无法把空字符串解析成集合。 传入map 1234567891011HashMap&lt;String, Object&gt; mapper = new HashMap&lt;String, Object&gt;(16);// 获取要遍历的sessionIdList集合List&lt;Long&gt; list = getSessionIdList();if (CollectionUtils.isEmpty(list)) &#123; mapper.put(&quot;sessionIdList&quot;, &quot;&quot;); //罪魁祸首在这里&#125; else &#123; mapper.put(&quot;sessionIdList&quot;, list);&#125;List&lt;RequestLog&gt; requestLogList = requestLogService.getRequestLogList(mapper); 对应mapper 1234567891011121314151617&lt;select id=&quot;getRequestLogList&quot; parameterType=&quot;java.util.HashMap&quot; resultType=&quot;RequestLog&quot;&gt; SELECT * FROM USER request_log &lt;where&gt; &lt;if test=&quot;code != null and code != &apos;&apos; &quot;&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test=&quot;id != null and id != &apos;&apos;&quot;&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test=&quot;sessionIdList !=null and sessionIdList.size &gt; 0&quot;&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 当调用getSessionIdList()方法获取到的集合为空时，传入的值是空字符串而非集合类型，从而在mapper.xml中解析时无法把字符串类型解析成集合类型，报出了Return value () was not iterable的问题。 另外还发现了另一个问题，在判断mapper.xm中集合是否为空时，不能与空字符串比较进行判断，而应该用size方法，否则会引起 invalid comparison: java.util.ArrayList and java.lang.String 把集合类型与字符串类型作比较，引起了“无效的比较”错误 反例 123456&lt;if test=&quot;sessionIdList !=null and sessionIdList != &apos;&apos;&quot;&gt; &lt;!--这里用!=&apos;&apos; 进行判断会报错，因为引起了集合与String类型的比较--&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; 正例123456&lt;if test=&quot;sessionIdList !=null and sessionIdList.size &gt; 0 &apos;&apos;&quot;&gt; &lt;!--这里应该用sessionIdList.size &gt; 0 来判断集合是否为空--&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; 总结当用mabatis foreach遍历时，遍历对象的类型一定要确认是否正确。一般有三种：list(集合)、array(数组)、map(map中传入集合)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
</search>
