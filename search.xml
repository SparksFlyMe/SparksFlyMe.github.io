<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[JAVA 中的 CAS]]></title>
    <url>%2F2020%2F08%2F06%2FJAVA-%E4%B8%AD%E7%9A%84-CAS%2F</url>
    <content type="text"><![CDATA[简介在计算机科学中，比较和交换（Conmpare And Swap）是用于实现多线程同步的原子指令。 它将内存位置的内容与给定值进行比较，只有在相同的情况下，将该内存位置的内容修改为新的给定值。 这是作为单个原子操作完成的。 原子性保证新值基于最新信息计算; 如果该值在同一时间被另一个线程更新，则写入将失败。 操作结果必须说明是否进行替换; 这可以通过一个简单的布尔响应（这个变体通常称为比较和设置），或通过返回从内存位置读取的值来完成。 一个 CAS 涉及到以下操作：我们假设内存中的原数据V，旧的预期值A，需要修改的新值B。 比较 A 与 V 是否相等。（比较）如果比较相等，将 B 写入 V。（交换）返回操作是否成功。当多个线程同时对某个资源进行CAS操作，只能有一个线程操作成功，但是并不会阻塞其他线程,其他线程只会收到操作失败的信号。可见 CAS 其实是一个乐观锁。 如上图中，主存中保存V值，线程中要使用V值要先从主存中读取V值到线程的工作内存A中，然后计算后变成B值，最后再把B值写回到内存V值中。多个线程共用V值都是如此操作。CAS的核心是在将B值写入到V之前要比较A值和V值是否相同，如果不相同证明此时V值已经被其他线程改变，重新将V值赋给A，并重新计算得到B，如果相同，则将B值赋给V。 如果不使用CAS机制，看看存在什么问题，假如V=1，现在Thread1要对V进行加1，Thread2也要对V进行加1，首先Thread1读取V=1到自己工作内存A中此时A=1，假设Thread2此时也读取V=1到自己的工作内存A中，分别进行加1操作后，两个线程中B的值都为2，此时写回到V中时发现V的值为2，但是两个线程分别对V进行加处理结果却只加了1有问题。 ABA 问题CAS 由三个步骤组成，分别是“读取-&gt;比较-&gt;写回”。考虑这样一种情况，线程1和线程2同时执行 CAS 逻辑，两个线程的执行顺序如下： 时刻1：线程1执行读取操作，获取原值 A，然后线程被切换走时刻2：线程2执行完成 CAS 操作将原值由 A 修改为 B时刻3：线程2再次执行 CAS 操作，并将原值由 B 修改为 A时刻4：线程1恢复运行，将比较值（compareValue）与原值（oldValue）进行比较，发现两个值相等。然后用新值（newValue）写入内存中，完成 CAS 操作 如上流程，线程1并不知道原值已经被修改过了，在它看来并没什么变化，所以它会继续往下执行流程。对于 ABA 问题，通常的处理措施是对每一次 CAS 操作设置版本号。java.util.concurrent.atomic 包下提供了一个可处理 ABA 问题的原子类 AtomicStampedReference。 ABA问题的解决办法 1.在变量前面追加版本号：每次变量更新就把版本号加1，则A-B-A就变成1A-2B-3A。2.atomic包下的AtomicStampedReference类：其compareAndSet方法首先检查当前引用是否等于预期引用，并且当前标志是否等于预期标志，如果全部相等，则以原子方式将该引用的该标志的值设置为给定的更新值。 其他问题 CAS除了ABA问题，仍然存在循环时间长开销大和只能保证一个共享变量的原子操作 1. 循环时间长开销大 自旋CAS如果长时间不成功，会给CPU带来非常大的执行开销。如果JVM能支持处理器提供的pause指令那么效率会有一定的提升，pause指令有两个作用，第一它可以延迟流水线执行指令（de-pipeline）,使CPU不会消耗过多的执行资源，延迟的时间取决于具体实现的版本，在一些处理器上延迟时间是零。第二它可以避免在退出循环的时候因内存顺序冲突（memory order violation）而引起CPU流水线被清空（CPU pipeline flush），从而提高CPU的执行效率。 2. 只能保证一个共享变量的原子操作 当对一个共享变量执行操作时，我们可以使用循环CAS的方式来保证原子操作，但是对多个共享变量操作时，循环CAS就无法保证操作的原子性，这个时候就可以用锁，或者有一个取巧的办法，就是把多个共享变量合并成一个共享变量来操作。比如有两个共享变量i＝2,j=a，合并一下ij=2a，然后用CAS来操作ij。从Java1.5开始JDK提供了AtomicReference类来保证引用对象之间的原子性，你可以把多个变量放在一个对象里来进行CAS操作。 CAS 的应用 1.Java的concurrent包下就有很多类似的实现类，如Atomic开头那些。 2.自旋锁 3.令牌桶限流器 令牌桶限流器就是系统以恒定的速度向桶内增加令牌。每次请求前从令牌桶里面获取令牌。如果获取到令牌就才可以进行访问。当令牌桶内没有令牌的时候，拒绝提供服务。我们来看看 eureka 的限流器是如何使用 CAS 来维护多线程环境下对 token 的增加和分发的。]]></content>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kafka通过控制台模拟消息发送和消息接收正常,但是通过javaAPI操作生产者发送消息不成功]]></title>
    <url>%2F2020%2F04%2F19%2Fkafka%E9%80%9A%E8%BF%87%E6%8E%A7%E5%88%B6%E5%8F%B0%E6%A8%A1%E6%8B%9F%E6%B6%88%E6%81%AF%E5%8F%91%E9%80%81%E5%92%8C%E6%B6%88%E6%81%AF%E6%8E%A5%E6%94%B6%E6%AD%A3%E5%B8%B8-%E4%BD%86%E6%98%AF%E9%80%9A%E8%BF%87javaAPI%E6%93%8D%E4%BD%9C%E7%94%9F%E4%BA%A7%E8%80%85%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E4%B8%8D%E6%88%90%E5%8A%9F%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;通过命令行工具kafka-console-producer.sh和kafka-console-consumer.sh是能够相互通信的，producer发布的信息consumer能够接收到。 &emsp;&emsp;但是java通过kafka-client的API写的代码始终不能跟kafka通信：java producer的消息发不出去， java comsumer也收不到任何消息。仔细检查了下代码中IP、端口都没有写错。 解决办法：将kafka/config/server.properties文件中advertised.listeners改为如下属性。 1advertised.listeners=PLAINTEXT://192.168.17.101:9092 192.168.17.101是kafka所在服务器的IP。改完后重启，OK了。Java端的代码就能通信了]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo博客中公式无法显示的问题]]></title>
    <url>%2F2020%2F01%2F04%2FHexo%E5%8D%9A%E5%AE%A2%E4%B8%AD%E5%85%AC%E5%BC%8F%E6%97%A0%E6%B3%95%E6%98%BE%E7%A4%BA%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[&emsp;&emsp;在用markdown写技术文档时，免不了会碰到数学公式。常用的Markdown编辑器都会集成Mathjax，用来渲染文档中的类Latex格式书写的数学公式。基于Hexo搭建的个人博客，默认情况下渲染数学公式却会出现各种各样的问题。 在文章的Front-matter里打开mathjax开关，如下：12345678910title: Kafka进阶author: kaizhangtags: - Kafka categories: - 中间件 date: 2020-01-02 20:20:00mathjax: true]]></content>
      <categories>
        <category>Blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka进阶]]></title>
    <url>%2F2020%2F01%2F02%2FKafka%E8%BF%9B%E9%98%B6%2F</url>
    <content type="text"><![CDATA[第1章 Kafka 架构深入1.1 Kafka 工作流程及文件存储机制 Kafka 工作流程 &emsp;&emsp;图中的0、1、2、3等就是偏移量”offset“，每个分区都有自己的从头开始的偏移量，是每个分区独立的，所以kafka中不能保证消息的全局有序性，只能保证区内有序。 &emsp;&emsp;Kafka 中消息是以 topic 进行分类的，生产者生产消息，消费者消费消息，都是面向 topic的。 &emsp;&emsp;topic 是逻辑上的概念，而 partition 是物理上的概念，每个 partition 对应于一个 log 文件，该 log 文件中存储的就是 producer 生产的数据。Producer 生产的数据会被不断追加到该log 文件末端，且每条数据都有自己的 offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个 offset，以便出错恢复时，从上次的位置继续消费。 Kafka文件存储机制 &emsp;&emsp;由于生产者生产的消息会不断追加到 log 文件末尾，为防止 log 文件过大导致数据定位效率低下，Kafka 采取了分片和索引机制，将每个 partition 分为多个 segment（当log文件达到指定大小时，自动生成新文件，默认是1G）。每个 segment对应两个文件——“.index”文和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic 名称+分区序号。例如，first 这个 topic 有三个分区，则其对应的文件夹为 first-0, first-1, first-2。 12345600000000000000000000.index00000000000000000000.log00000000000000170410.index00000000000000170410.log00000000000000239430.index00000000000000239430.log index 和 log 文件以当前 segment 的第一条消息的 offset 命名。下图为 index 文件和 log 文件的结构示图。 &emsp;&emsp;“.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元 数据指向对应数据文件中message 的物理偏移地址。 1.2 Kafka生产者1.2.1 分区策略（1） 分区的原因 方便在集群中扩展，每个 Partition 可以通过调整以适应它所在的机器，而一个 topic又可以有多个 Partition组成，因此整个集群就可以适应任意大小的数据了； 可以提高并发，因为可以以 Partition 为单位读写了。 （2）分区的原则&emsp;&emsp;我们需要将 producer 发送的数据封装成一个 ProducerRecord 对象。 指明 partition 的情况下，直接将指明的值直接作为 partiton 值； 没有指明 partition 值但有 key 的情况下，将 key 的 hash 值与 topic 的 partition 数进行取余得到 partition 值； 既没有 partition 值又没有 key 值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与 topic 可用的 partition 总数取余得到 partition 值，也就是常说的 round-robin 算法。 1.2.2 数据可靠性保证&emsp;&emsp;为保证 producer 发送的数据，能可靠的发送到指定的 topic，topic 的每个 partition 收到producer 发送的数据后，都需要向 producer 发送 ack（acknowledgement 确认收到），如果producer 收到 ack，就会进行下一轮的发送，否则重新发送数据。 （1）副本数据同步策略 方案 优点 缺点 半数以上完成同步，就发送 ack 延迟低 选举新的 leader 时，容忍 n 台节点的故障，需要 2n+1 个副本 全部完成同步，才发送 ack 选举新的 leader 时，容忍 n 台节点的故障，需要 n+1 个副本 延迟高 Kafka 选择了第二种方案，原因如下： 同样为了容忍 n 台节点的故障，第一种方案需要 2n+1 个副本，而第二种方案只需要 n+1个副本，而Kafka 的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。【半数以上完成同步才可以发ACK，如果挂了n台，那么就需要有另外n台正常发送（这样正常发送的刚好是总数（挂的和没挂的）的一半（n（挂的）+n（正常的）=2n））,因为是半数以上所以2n+1.(所以总数2n+1的时候最多只能容忍n台有故障)】 虽然第二种方案的网络延迟会比较高，但网络延迟对 Kafka 的影响较小。 （2）ISR&emsp;&emsp;采用第二种方案之后，设想以下情景：leader 收到数据，所有 follower 都开始同步数据，但有一个 follower，因为某种故障，迟迟不能与 leader 进行同步，那 leader 就要一直等下去，直到它完成同步，才能发送 ack。这个问题怎么解决呢？ &emsp;&emsp;Leader 维护了一个动态的 in-sync replica set (ISR，同步副本)，意为和 leader 保持同步的 follower 集合。当 ISR 中的 follower 完成数据的同步之后，leader 就会给 follower 发送 ack。如果 follower长时间未向 leader 同 步 数 据 ，则 该 follower 将 被 踢 出 ISR ， 该 时 间 阈 值 由 replica.lag.time.max.ms 参数设定。Leader 发生故障之后，就会从 ISR 中选举新的 leader。 （3）ack 应答机制&emsp;&emsp;对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等 ISR 中的 follower 全部接收成功。&emsp;&emsp;所以 Kafka 为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。acks 参数配置：&emsp;&emsp;acks： 0： &emsp;&emsp;producer 不等待 broker 的 ack，这一操作提供了一个最低的延迟，broker 一接收到还没有写入磁盘就已经返回，当 broker 故障时有可能丢失数据； 1： &emsp;&emsp;producer 等待 broker 的 ack，partition 的 leader 落盘成功后返回 ack，如果在 follower（ISR中的follower）同步成功之前 leader 故障，那么将会丢失数据；该值为ack的默认值 -1(或者all)： &emsp;&emsp;producer 等待 broker 的 ack，partition 的 leader 和 follower（ISR中的follower）全部落盘成功后才返回 ack。但是如果在 follower 同步完成后，broker 发送 ack 之前，leader 发生故障，那么会造成数据重复。 &emsp;&emsp;但是这样就代表数据一定不会丢失了吗？当然不是，如果你的Partition只有一个副本，也就是一个Leader，任何Follower都没有，你认为acks=all有用吗？当然没用了，因为ISR里就一个Leader，他接收完消息后宕机，也会导致数据丢失。所以说，这个acks=all，必须跟ISR列表里至少有2个以上的副本配合使用，起码是有一个Leader和一个Follower才可以。这样才能保证说写一条数据过去，一定是2个以上的副本都收到了才算是成功，此时任何一个副本宕机，不会导致数据丢失。 （4）故障处理细节 LEO：称为“日志末端偏移量”，指的是每个副本最大的 offset；HW：称为“高水位”，指的是消费者能见到的最大的 offset，消费者只能拉取到这个offset之前的消息，ISR 队列中最小的 LEO。 1）follower 故障 &emsp;&emsp;follower 发生故障后会被临时踢出 ISR，待 该 follower 恢复后，follower 会读取本地磁盘记录的上次的 HW，并将 log 文件高于 HW 的部分截取掉，从 HW 开始向 leader 进行同步。等该 follower 的 LEO 大于等于该 Partition 的 HW，即 follower 追上 leader 之后，就可以重新加入 ISR 了。 2）leader 故障 &emsp;&emsp;leader 发生故障之后，会从 ISR 中选出一个新的 leader，之后，为保证多个副本之间的数据一致性，其余的 follower 会先将各自的 log 文件高于 HW 的部分截掉，然后从新的 leader同步数据。 &emsp;&emsp;注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 1.2.3 Exactly Once 语义&emsp;&emsp;将服务器的 ACK 级别设置为-1，可以保证 Producer 到 Server 之间不会丢失数据，即 At Least Once 语义。相对的，将服务器 ACK 级别设置为 0，可以保证生产者每条消息只会被发送一次，即 At Most Once 语义。 &emsp;&emsp;At Least Once 可以保证数据不丢失，但是不能保证数据不重复；相对的，At Most Once可以保证数据不重复，但是不能保证数据不丢失。但是，对于一些非常重要的信息，比如说交易数据，下游数据消费者要求数据既不重复也不丢失，即 Exactly Once 语义。在 0.11 版本以前的 Kafka，对此是无能为力的，只能保证数据不丢失，再在下游消费者对数据做全局去重。对于多个下游应用的情况，每个都需要单独做全局去重，这就对性能造成了很大影响。 &emsp;&emsp;0.11 版本的 Kafka，引入了一项重大特性：幂等性。所谓的幂等性就是指 Producer 不论向 Server 发送多少次重复数据，Server 端都只会持久化一条。幂等性结合 At Least Once 语义，就构成了 Kafka 的 Exactly Once 语义。即： At Least Once + 幂等性 = Exactly Once&emsp;&emsp;要启用幂等性，只需要将 Producer 的参数中 enable.idompotence 设置为 true 即可。Kafka的幂等性实现其实就是将原来下游需要做的去重放在了数据上游。开启幂等性的 Producer 在初始化的时候会被分配一个 PID，发往同一 Partition 的消息会附带 Sequence Number。而Broker 端会对&lt;PID, Partition, SeqNumber&gt;做缓存，当具有相同主键的消息提交时，Broker 只会持久化一条。 &emsp;&emsp;但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。 1.3 Kafka消费者1.3.1 消费方式&emsp;&emsp;consumer 采用 pull（拉）模式从 broker 中读取数据。&emsp;&emsp;push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由 broker 决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成 consumer 来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而 pull 模式则可以根据 consumer 的消费能力以适当的速率消费消息。&emsp;&emsp;pull 模式不足之处是，如果 kafka 没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka 的消费者在消费数据时会传入一个时长参数 timeout，如果当前没有数据可供消费，consumer 会等待一段时间之后再返回，这段时长即为 timeout。 1.3.2 分区分配策略&emsp;&emsp;一个 consumer group 中有多个 consumer，一个 topic 有多个 partition，所以必然会涉及到 partition 的分配问题，即确定那个 partition 由哪个 consumer 来消费。 &emsp;&emsp;Kafka分区分配策略，就是它的内部的默认的分区分配策略：Range（范围） 和 RoundRobin（轮询）。当下面的事情发生的时候，Kafka将会进行一次分区分配。 ​ 1、当同一个Consumer Group 内新增消费者。（消费者层面发生改动） ​ 2、消费者离开当前所属的Consumer Group，包括Shuts down（关闭、停工）或者是crashes（崩溃）。（消费者层面发生改动） ​ 3、消费者订阅的主题新增分区的时候 &emsp;&emsp;将分区的所有权从一个消费者移到另一个消费者成为重新平衡（rebalance），如何rebalance就涉及到分区分配策略。 &emsp;&emsp;我们假设有一个名为Top1的主题，它有十个分区，然后我们有两个消费者（C1、C2）来消费这十个分区里面的数据，而且C1的num.streams=1，C2的num.streams=2。 第一个默认的分区策略：Range Startegy（根据范围消费） &emsp;&emsp;Range startegy是对每个主题而言的 ， 首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母进行排序。在对十个分区排序的话是0-9；消费者线程排完序是C1-0，C2-0，C2-1。然后用partitions的总数除以消费者的总数来决定每个消费者线程消费几个分区。如果有余数，那么前面的几个消费者线程将会多消费一个分区。在我们的例子里面，我们有十个分区，三个消费者线程，10/3=3—-1，那么消费者线程C1-0 将会多消费一个分区，所以最后分区分配的结构看起来是这样的： 12345C1-0将消费0,1,2,3分区C2-0将消费4,5,6分区C2-1将消费7,8,9分区 如果有第十一个分区的话，那么分区是这样的： 12345C1-0将消费0,1,2,3分区C2-0将消费4,5,6,7分区C2-1将消费8,9,10分区 如果我们有2个主题（T1和T2），分别都有十个分区，那么最后的分配结果是： 12345C1-0将消费T1主题中的0,1,2,3分区以及T2主题中0,1,2,3分区C2-0将消费T1主题中的4,5,6分区以及T2主题中的4,5,6,分区C2-1将消费T1主题中的7,8,9分区以及T2主题中的7,8,9分区 &emsp;&emsp;这就是消费的策略！ 就是用总的分区数/消费者线程总数=每个消费者线程应该消费的分区数。当还有余数的时候就将余数分别分发到另外的消费组线程中。 &emsp;&emsp;在这里我们不难看出来。C1-0消费者线程比其他消费者线程多消费了两个分区，这就是Range Strategy的一个明显的弊端。 当分区很多的时候，会有个别的线程压力巨大！ 第二个默认的分区策略：RoundRobin strategy（轮询的消费策略） 在使用RoundRobin Starategy的时候我们必须满足两个条件： 1、同一个consumer Group里面的所有消费者的num.streams必须相等； 2、每个消费者订阅的 主题必须相同; &emsp;&emsp;在这里我们假设2个消费者的num.streams=2. RoundRobin starategy的工作原理： 将所有主题的分区组成TopicAndPartition列表，然后对TopAndPartition列表按照hashcode进行排序。最后按照round-robin风格将分区分别分配给不同的消费者线程。 &emsp;&emsp;在我们的例子中，假如按照hashcode排序完的topic-partition组依次为T1-5,T1-3,T1-0.T1-8.T1-2,T1-1,T1-4,T1-6,T1-9，我们的消费者线程排序为C1-0, C1-1 ,C2-0,C2-1，最后分区分配结果为： 1234567C1-0将消费T1-5 , T1-2 , T1-6 分区；C1-1将消费T1-3 , T1-1 , T1-9 分区；C2-0将消费T1-0 , T1-4分区；C2-1将消费T1-8 , T1-7分区； &emsp;&emsp;我们可以通过partition.assignment.strategy参数选择range或roundrobin。 partition.assignment.strategy参数默认的值是range。 1.3.3 offset 的维护&emsp;&emsp;由于 consumer 在消费过程中可能会出现断电宕机等故障，consumer 恢复后，需要从故障前的位置的继续消费，所以 consumer 需要实时记录自己消费到了哪个 offset，以便故障恢复后继续消费。 &emsp;&emsp;Kafka 0.9 版本之前，consumer 默认将 offset 保存在 Zookeeper 中，从 0.9 版本开始，consumer 默认将 offset 保存在 Kafka 一个内置的 topic 中，该 topic 为__consumer_offsets。 1）修改配置文件 consumer.properties 12exclude.internal.topics=false（没有就新增） 2）读取 offset ​ 0.11.0.0 之前版本: 1bin/kafka-console-consumer.sh --topic __consumer_offsets -zookeeper server1:2181/kafka --formatter &quot;kafka.coordinator.GroupMetadataManager\$OffsetsMessageFormatter&quot; --consumer.config config/consumer.properties --from-beginning ​ 0.11.0.0 之后版本(含): 1bin/kafka-console-consumer.sh --topic __consumer_offsets -zookeeper server1:2181/kafka --formatter &quot;kafka.coordinator.group.GroupMetadataManager\$OffsetsMessageForm atter&quot; --consumer.config config/consumer.properties --frombeginning 1.3.4 消费者组案例1）需求：测试同一个消费者组中的消费者，同一时刻只能有一个消费者消费。 2）案例实操 （1）在 server1、server2 上修改/opt/software/kafka/config/consumer.properties 配置文件中的 group.id 属性为任意组名。 12[kaizhang@server1 config]$ vi consumer.properties group.id=kaizhang （2）在 server1、server2上分别启动消费者 12[kaizhang@server1 kafka]$ bin/kafka-console-consumer.sh --zookeeper server1:2181/kafka --topic first --consumer.config config/consumer.properties[kaizhang@server2 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first --consumer.config config/consumer.properties （3）在 server3上启动生产者 12[kaizhang@server3 kafka]$ bin/kafka-console-producer.sh --broker-list hadoop102:9092 --topic first &gt;hello world （4）查看 server1和 server2的接收者。 &emsp;&emsp;同一时刻只有一个消费者接收到消息。 1.4 Kafka 高效读写数据1）顺序写磁盘 &emsp;&emsp;Kafka 的 producer 生产数据，要写入到 log 文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到 600M/s，而随机写只有 100K/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。 2）零拷贝技术 &emsp;&emsp;零拷贝：零拷贝并不是不需要拷贝，而是减少不必要的拷贝次数。通常是说在IO读写过程中。 传统的读取文件数据并发送到网络的步骤如下： （1）操作系统将数据从磁盘文件中读取到内核空间的页面缓存； （2）应用程序将数据从内核空间读入用户空间缓冲区； （3）应用程序将读到数据写回内核空间并放入socket缓冲区； （4）操作系统将数据从socket缓冲区复制到网卡接口，此时数据才能通过网络发送。 &emsp;&emsp;“零拷贝技术”只用将磁盘文件的数据复制到页面缓存中一次，然后将数据从页面缓存直接发送到网络中（发送给不同的订阅者时，都可以使用同一个页面缓存），避免了重复复制操作。 &emsp;&emsp;如果有10个消费者，传统方式下，数据复制次数为4*10=40次，而使用“零拷贝技术”只需要1+10=11次，一次为从磁盘复制到页面缓存，10次表示10个消费者各自读取一次页面缓存。 1.5 Zookeeper 在 Kafka 中的作用&emsp;&emsp;Kafka 集群中有一个 broker 会被选举为 Controller，负责管理集群 broker 的上下线，所有 topic 的分区副本分配和 leader 选举等工作。&emsp;&emsp;Controller 的管理工作都是依赖于 Zookeeper 的。 以下为 partition 的 leader 选举过程：]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kafka入门+实战]]></title>
    <url>%2F2019%2F11%2F30%2FKafka%E5%85%A5%E9%97%A8%2B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[Kafka入门+实战第 1 章 Kafka概述1.1 定义Kafka 是一个分布式的基于发布/订阅模式的消息队列（Message Queue），主要应用于大数据实时处理领域。 1.2 消息队列1.2.1 传统消息队列的应用场景MQ传统应用场景之异步处理： 使用消息队列的好处 解耦允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 可恢复性系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 缓冲有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况（生产消息的速度 &gt; 消费消息的速度）。 灵活性 &amp; 峰值处理能力 （削峰）在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 异步通信很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 1.2.2 消息队列的两种模式 点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除）消息生产者生产消息发送到Queue中，然后消息消费者从Queue中取出并且消费消息。 消息被消费以后，Queue 中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue 支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 发布/订阅模式（一对多，消费者消费数据之后不会清除消息） 消息生产者（发布）将消息发布到 topic 中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到 topic 的消息会被所有订阅者消费。其中，发布/订阅模式又分为两种： 一种为消费者主动拉取队列中数据(Kafka就属于这种模式)。这种模式有一个缺点就是要维护一个长轮询，不断去拉取数据，造成资源浪费。 一种为由队列主动推送给消费者。 图为发布订阅模式： 1.3 Kafka 基础架构Kafka架构图： Producer ：消息生产者，就是向 kafka broker 发消息的客户端； Consumer ：消息消费者，向 kafka broker 取消息的客户端； Consumer Group （CG）：消费者组，由多个 consumer 组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个组内消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。 如果分区数大于或者等于组中的消费者实例数，一个消费者会负责多个分区。当然最理想的是分区数等于组中的消费者实例数。 Broker ：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker可以容纳多个 topic。 Topic ：可以理解为一个队列，生产者和消费者面向的都是一个 topic Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker（即服务器）上，一个 topic 可以分为多个 partition，每个 partition 是一个有序的队列； Replica：副本，为保证集群中的某个节点发生故障时，该节点上的 partition 数据不丢失，且 kafka 仍然能够继续工作，kafka 提供了副本机制，一个 topic 的每个分区都有若干个副本，一个 leader 和若干个 follower。 leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是 leader。 follower：每个分区多个副本中的“从”，实时从 leader 中同步数据，保持和 leader 数据的同步。leader 发生故障时，某个 follower 会成为新的 leader。 第 2 章 Kafka 快速入门2.1 安装部署2.1.1 集群规划linux1 linux2 linux3 zk zk zk kafka kafka kafka 2.1.2 安装包下载下载地址 这里使用0.11.0.0版本 2.1.3 集群部署 下面流程中，只有配置文件中的broker.id不同，其他在各个机器上都是相同的操作 1、解压安装包：把下载的安装包放到/opt/software下并解压 1[kaizhang@localhost software]$ tar -zxvf kafka_2.11-0.11.0.0.gz 2、修改解压后的文件名 1[kaizhang@localhost software]$ mv kafka_2.11-0.11.0.0 kafka 3、在/opt/software/kafka 目录下创建 logs 文件夹 1[kaizhang@localhost kafka_2.11-0.11.0.0]$ mkdir logs 4、修改配置文件 12[kaizhang@localhost kafka_2.11-0.11.0.0]$ cd config[kaizhang@localhost config]$ vi server.properties ​ 修改部分配置如下：(其中三台centos中broker.id分别是1、2、3，不得重复) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163# see kafka.server.KafkaConfig for additional details and defaults ############################# Server Basics ############################# # 基本配置 # The id of the broker. This must be set to a unique integer for each broker. broker id, id必须是唯一的整数 broker.id=0 # Switch to enable topic deletion or not, default value is false # 是否可以删除topic，如果为true，我们可以在命令行删除topic，否则，不能。 #delete.topic.enable=true ############################# Socket Server Settings ############################# # socket配置 # The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured. # FORMAT: # listeners = listener_name://host_name:port # EXAMPLE: # listeners = PLAINTEXT://your.host.name:9092 # broker监听地址。如果没有配置，默认为java.net.InetAddress.getCanonicalHostName()方法返回的地址 #listeners=PLAINTEXT://:9092 # Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for "listeners" if configured. Otherwise, it will use the value # returned from java.net.InetAddress.getCanonicalHostName(). # broker的主机名和端口号将会广播给消费者与生产者。如果没有设置，默认为监听配置，否者使用 # java.net.InetAddress.getCanonicalHostName()方法返回的地址 #advertised.listeners=PLAINTEXT://your.host.name:9092，每台服务器填写各自ip advertised.listeners=PLAINTEXT://192.168.17.101:9092 # Maps listener names to security protocols, the default is for them to be the same. See the config documentation for more details # 监听协议，默认为PLAINTEXT #listener.security.protocol.map=PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL # The number of threads that the server uses for receiving requests from the network and sending responses to the network # 服务器接受请求和相应请求的线程数 num.network.threads=3 # The number of threads that the server uses for processing requests, which may include disk I/O # 处理请求的线程数，包括磁盘的IO操作 num.io.threads=8 # The send buffer (SO_SNDBUF) used by the socket server # 服务器socket发送缓存 socket.send.buffer.bytes=102400 # The receive buffer (SO_RCVBUF) used by the socket server # 服务器socket接受缓存 socket.receive.buffer.bytes=102400 # The maximum size of a request that the socket server will accept (protection against OOM) # 服务器接收请求的最大值 socket.request.max.bytes=104857600 ############################# Log Basics ############################# # log基本配置 # A comma seperated list of directories under which to store log files # kafka运行日志存放的路径log.dirs=/opt/software/kafka/logs # The default number of log partitions per topic. More partitions allow greater # parallelism for consumption, but this will also result in more files across # the brokers. # 每个topic的默认日志分区数。允许分区数大于并行消费数，这样可能导致，更多的文件将会跨broker num.partitions=1 # The number of threads per data directory to be used for log recovery at startup and flushing at shutdown. # This value is recommended to be increased for installations with data dirs located in RAID array. # 在启动和关闭刷新时，没有数据目录用于日志恢复的线程数。 # 这个值，强烈建议在随着在RAID阵列中的安装数据目录的增长而增长。 num.recovery.threads.per.data.dir=1 ############################# Internal Topic Settings ############################# # 内部topic配置 # The replication factor for the group metadata internal topics "__consumer_offsets" and "__transaction_state" # For anything other than development testing,a value greater than 1 is recommended for to ensure availability such as 3. # 内部__consumer_offsets和__transaction_state两个topic，分组元数据的复制因子。 # 除开发测试外的使用，强烈建议值大于1，以保证可用性，比如3。 offsets.topic.replication.factor=1 transaction.state.log.replication.factor=1 transaction.state.log.min.isr=1 ############################# Log Flush Policy ############################# # 日志刷新策略 # Messages are immediately written to the filesystem but by default we only fsync() to sync # the OS cache lazily. The following configurations control the flush of data to disk. # There are a few important trade-offs here: # 消息立刻被写到文件系统，默认调用fsync方法，懒同步操作系统缓存。下面的配置用于控制刷新数据到磁盘。 # 这里是一些折中方案： # 1. Durability: Unflushed data may be lost if you are not using replication. # 持久性：如果没有使用replication，没刷新的数据可能丢失。 # 2. Latency: Very large flush intervals may lead to latency spikes when the flush does occur as there will be a lot of data to flush. # 延迟性：当有大量的数据需要刷新，刷新操作发生时，比较大的刷新间隔可能会导致延时。 # 3. Throughput: The flush is generally the most expensive operation, and a small flush interval may lead to exceessive seeks. # 吞吐量：刷新操作代价比较高，较小的刷新间隔，将会引起过渡的seek文件操作。 # The settings below allow one to configure the flush policy to flush data after a period of time or # every N messages (or both). This can be done globally and overridden on a per-topic basis. # 下面的配置刷新策略，允许在一个的刷新间隔或消息数量下，刷新数据，这个配置是全局的，可以在每个主题下重写。 # The number of messages to accept before forcing a flush of data to disk # 在强制刷新数据到磁盘前，允许接受消息数量 #log.flush.interval.messages=10000 # The maximum amount of time a message can sit in a log before we force a flush # 在强制刷新前，一个消息可以日志中停留在最大时间 #log.flush.interval.ms=1000 ############################# Log Retention Policy ############################# # 日志保留策略 # The following configurations control the disposal of log segments. The policy can # be set to delete segments after a period of time, or after a given size has accumulated. # A segment will be deleted whenever *either* of these criteria are met. Deletion always happens # from the end of the log. # 下面的配置用于控制日志segments的处理。这些策略可以在一定的时间间隔和数据累积到一定的size，可以删除segments。两种策略只要有 一种触发，segments将会被删除。删除总是从log的末端。 # The minimum age of a log file to be eligible for deletion due to age # log文件的保留的时间，超时将被删除log.retention.hours=168 # A size-based retention policy for logs. Segments are pruned from the log as long as the remaining # segments don't drop below log.retention.bytes. Functions independently of log.retention.hours. # log文件保留的size #log.retention.bytes=1073741824 # The maximum size of a log segment file. When this size is reached a new log segment will be created. # 日志segments文件最大size，当日志文件的大于最大值，则创建一个新的log segment log.segment.bytes=1073741824 # The interval at which log segments are checked to see if they can be deleted according # to the retention policies # 日志保留检查间隔 log.retention.check.interval.ms=300000 ############################# Zookeeper ############################# # Zookeeper配置 # Zookeeper connection string (see zookeeper docs for details). # This is a comma separated host:port pairs, each corresponding to a zk # server. e.g. "127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002". # You can also append an optional chroot string to the urls to specify the # root directory for all kafka znodes. # 配置连接Zookeeper集群地址，zk允许每个客户端为自己设置已给命名空间。如果一个zookeeper客户端设置了Chroot，那么该客户端对服务器的任何操作，都将会被限制在自己的命名空间下。# 客户端可以通过在connectString中添加后缀的方式来设置Chroot，如下所示：192.168.0.1:2181,192.168.0.2:2181,192.168.0.3:2181/apps/X # 这个client的chrootPath就是/apps/X，将这样一个connectString传入客户端的ConnectStringParser后就能够解析出Chroot并保存在chrootPath属性中zookeeper.connect=server1:2181,server2:2181,server3:2181/kafka # Timeout in ms for connecting to zookeeper # 连接zookeeper超时时间 zookeeper.connection.timeout.ms=6000 ############################# Group Coordinator Settings ############################# # 分组协调配置 # The following configuration specifies the time, in milliseconds, that the GroupCoordinator will delay the initial consumer rebalance. # The rebalance will be further delayed by the value of group.initial.rebalance.delay.ms as new members join the group, up to a maximum of max.poll.interval.ms. # The default value for this is 3 seconds. # We override this to 0 here as it makes for a better out-of-the-box experience for development and testing. # However, in production environments the default value of 3 seconds is more suitable as this will help to avoid unnecessary, and potentially expensive, rebalances during application startup. # 下面的配置为毫秒时间，用于延时消费者重平衡的时间。重平衡将会进一步在新成员添加分组是，延时group.initial.rebalance.delay.ms时间，直到到达maximum of max.poll.interval.ms时间。 # 默认值为3秒，我们重写0，主要是用户开发测试体验。在生产环境下，默认值3s，在应用启动期间， 帮助避免不必要及潜在的代价高的rebalances，是比较合适的。 group.initial.rebalance.delay.ms=0 5、配置环境变量 1234567kaizhang@localhost config]$ sudo vi /etc/profile #KAFKA_HOMEexport KAFKA_HOME=/opt/software/kafkaexport PATH=$PATH:$KAFKA_HOME/bin [kaizhang@localhost config]$ source /etc/profile 6、启动集群 ​ 依次在三台机器上启动kafka 123[kaizhang@server1 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties [kaizhang@server2 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties [kaizhang@server3 kafka]$ bin/kafka-server-start.sh -daemon config/server.properties 7、关闭集群 123[kaizhang@server1 kafka]$ bin/kafka-server-stop.sh[kaizhang@server2 kafka]$ bin/kafka-server-stop.sh[kaizhang@server3 kafka]$ bin/kafka-server-stop.sh 8、kafka操作脚本 123456789101112131415#!/bin/bashcase $1 in"start")&#123; echo "========== server1 start ==========" sh /opt/software/kafka/bin/kafka-server-start.sh -daemon /opt/software/kafka/config/server.properties&#125;;case $1 in"stop")&#123; echo "========== server1 stop ===========" sh /opt/software/kafka/bin/kafka-server-stop.sh -daemon /opt/software/kafka/config/server.properties&#125;; esac 2.2 Kafka命令行操作(在/opt/software/kafka目录下执行命令)1、查看当前服务器中的所有 topic 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --list__consumer_offsetsfivefour 2、创建 topic 12[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 2 --partitions 3 --topic firstCreated topic "first". 选项说明：—topic 定义 topic 名—replication-factor 定义副本数—partitions 定义分区数 在创建完topic后，可以到kafka数据文件中查看，这里到设置的目录里查看即 log.dirs=/opt/software/kafka/logs 12drwxrwxr-x. 2 kaizhang kaizhang 141 12月 13 00:04 first-0drwxrwxr-x. 2 kaizhang kaizhang 141 12月 13 00:04 first-1 在first-0中，first是主题名称，0为分区 这里主题first是3个分区，2个副本，所以在三台服务器的日志文件中共可以看到6个first相关的数据： first-0、first-1、first-2、first-0、first-1、first-2 如果创建的是2个分区，3个副本，则数据为： first-0、first-0、first-0、first-1、first-1、first-1 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 0 --partitions 3 --topic firstError while executing topic command : replication factor must be larger than 0[2019-12-13 00:39:23,664] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor must be larger than 0 (kafka.admin.TopicCommand$) 1234[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --create --replication-factor 4 --partitions 2 --topic firstError while executing topic command : replication factor: 4 larger than available brokers: 3[2019-12-13 00:22:07,440] ERROR org.apache.kafka.common.errors.InvalidReplicationFactorException: replication factor: 4 larger than available brokers: 3 (kafka.admin.TopicCommand$) 这里可以看到，如果副本数等于0或者大于brokers的数量，都是会报错的 3、删除topic 123[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --delete --topic firstTopic first is marked for deletion.Note: This will have no impact if delete.topic.enable is not set to true. 需要 server.properties 中设置 delete.topic.enable=true 否则只是标记删除。 4、发送消息 123[kaizhang@server1 kafka]$ bin/kafka-console-producer.sh --topic first --broker-list server1:9092&gt;hello world&gt;kafka 其中9092是kafka服务器需要监听的端口号，在server.properties文件中配置。 5、消费消息 123456#通过zookeeper消费，这是kafka 0.9版本之前的消费方式，0.9版本之前消息是存放在zookeeper中的[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --zookeeper server1:2181/kafka --topic first #通过kafka 0.9版本之后的消费方式[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first#从最初的的消息开始读取[kaizhang@server3 kafka]$ bin/kafka-console-consumer.sh --bootstrap-server server1:9092 --topic first --from-beginning --from-beginning：会把主题中以往所有的数据都读取出来。 6、查看某个 Topic 的详情 12345[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --describe --topic firstTopic:first PartitionCount:3 ReplicationFactor:2 Configs: Topic: first Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: first Partition: 1 Leader: 2 Replicas: 2,3 Isr: 2,3 Topic: first Partition: 2 Leader: 3 Replicas: 3,1 Isr: 3,1 7、修改分区数 123[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --alter --topic first --partitions 6 WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedAdding partitions succeeded! 12345678910111213[kaizhang@server1 kafka]$ bin/kafka-topics.sh --zookeeper server1:2181/kafka --alter --topic first --partitions 2WARNING: If partitions are increased for a topic that has a key, the partition logic or ordering of the messages will be affectedError while executing topic command : The number of partitions for a topic can only be increased[2019-12-13 00:31:37,496] ERROR kafka.admin.AdminOperationException: The number of partitions for a topic can only be increased at kafka.admin.AdminUtils$.addPartitions(AdminUtils.scala:292) at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:147) at kafka.admin.TopicCommand$$anonfun$alterTopic$1.apply(TopicCommand.scala:124) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at kafka.admin.TopicCommand$.alterTopic(TopicCommand.scala:124) at kafka.admin.TopicCommand$.main(TopicCommand.scala:64) at kafka.admin.TopicCommand.main(TopicCommand.scala) (kafka.admin.TopicCommand$) 这里可以看到，分区数只能增加，不能减少 按照Kafka现有的代码逻辑而言，此功能完全可以实现，不过也会使得代码的复杂度急剧增大。实现此功能需要考虑的因素很多，比如删除掉的分区中的消息该作何处理？如果随着分区一起消失则消息的可靠性得不到保障；如果需要保留则又需要考虑如何保留。直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于Spark、Flink这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入到现有的分区中，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题、以及分区和副本的状态机切换问题都是不得不面对的。反观这个功能的收益点却是很低，如果真的需要实现此类的功能，完全可以重新创建一个分区数较小的主题，然后将现有主题中的消息按照既定的逻辑复制过去即可。虽然分区数不可以减少，但是分区对应的副本数是可以减少的，这个其实很好理解，你关闭一个副本时就相当于副本数减少了。 不过正规的做法是使用kafka-reassign-partition.sh脚本来实现，具体用法可以自行搜索。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper集群搭建及内部原理]]></title>
    <url>%2F2019%2F09%2F29%2FZookeeper%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%86%85%E9%83%A8%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[第1章 Zookeeper 实战1.1 分布式安装部署0）集群规划在CentOS_1、CentOS_2 和CentOS_3 三个服务器上部署 Zookeeper。 1）解压安装1、解压 zookeeper 安装包到/opt/software/目录下1[kaizhang@localhost software]$ tar -zxvf zookeeper-3.4.14.tar.gz 2、在/opt/software/zookeeper-3.4.14/这个目录下创建 zkData1[kaizhang@localhost zookeeper-3.4.14]$ mkdir -p zkData 3、复制/opt/software/zookeeper-3.4.14/conf 这个目录下的 zoo_sample.cfg 为 zoo.cfg1[kaizhang@localhost conf]$ cp zoo_sample.cfg zoo.cfg 2）配置 zoo.cfg 文件1、具体配置123456dataDir=/opt/software/zookeeper-3.4.14/zkData 增加如下配置 #######################cluster##########################server.1=192.168.17.101:2888:3888server.2=192.168.17.102:2888:3888server.3=192.168.17.103:2888:3888 2、配置参数解读1234567Server.A=B:C:D。 A 是一个数字，表示这个是第几号服务器； B 是这个服务器的 ip 地址； C 是这个服务器与集群中的 Leader 服务器交换信息的端口； D 是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。集群模式下配置一个文件myid，这个文件在zkData目录下，这个文件里面有一个数据就是 A 的值，Zookeeper 启动时读取此文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是哪个 server。 3） 修改日志输出位置 当执行zkServer.sh 时，会在执行命令的文件夹下会产生zookeeper.out日志文件记录zookeeper的运行日志，该种方式会让日志文件不便于查找，容易遗忘。这里修改日志输出到指定文件夹。 1、修改bin/log4j.properties文件 zookeeper.out文件属于运行时的日志文件，通过conf/log4j.properties文件配置。 1234567# 以下是原配置zookeeper.root.logger=INFO, CONSOLElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender# 以下是修改后配置zookeeper.root.logger=INFO, ROLLINGFILElog4j.appender.ROLLINGFILE=org.apache.log4j.RollingFileAppender 2、在/opt/software/zookeeper-3.4.14下创建日志目录1[kaizhang@localhost zookeeper-3.4.14]$ mkdir logs 3、修改bin/zkEnv.sh12345678910111213141516171819202122# 以下是原配置if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;.&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,CONSOLE&quot;fi# 以下是修改后配置if [ &quot;x$&#123;ZOO_LOG_DIR&#125;&quot; = &quot;x&quot; ]then ZOO_LOG_DIR=&quot;/opt/software/zookeeper-3.4.14/logs&quot;fiif [ &quot;x$&#123;ZOO_LOG4J_PROP&#125;&quot; = &quot;x&quot; ]then ZOO_LOG4J_PROP=&quot;INFO,ROLLINGFILE&quot;fi 3）集群操作1、在/opt/software/zookeeper-3.4.14/zkData 目录下创建一个 myid 的文件1[kaizhang@localhost zkData]$ touch myid 添加 myid 文件，注意一定要在 linux 里面创建，在 notepad++里面很可能乱码 2、编辑 myid 文件1[kaizhang@localhost zkData]$ vi myid 在文件中添加与 server 对应的编号：如 1 3、在其他机器上进行相同的操作 ，并分别修改 myid 文件中内容为 2、34、在三台机器上分别启动 zookeeper1[kaizhang@localhost bin]$ ./zkServer.sh start 5、查看状态1234567891011121314[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: leader[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: follower[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: follower 6、操作脚本12345678910111213141516171819#!/bin/bashcase $1 in"start")&#123; echo "========== server2 start ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh start&#125;;;"stop")&#123; echo "========== server2 stop ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh stop&#125;;;"status")&#123; echo "========== server2 status ==========" sh /opt/software/zookeeper-3.4.14/bin/zkServer.sh status&#125;;;esac 7、Chroot：客户端隔离命名空间​ 在3.2.0及其之后版本的ZooKeeper中，添加了“Chroot”特性，该特性允许每个客户端为自己设置一个命名空间（Namespace）。如果一个ZooKeeper客户端设置了Chroot，那么该客户端对服务器的任何操作，都将会被限制在其自己的命名空间下。 ​ 如果使用 “127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002/app/a” ，客户端将把”/app/a”作为跟路径，并且所有的路径都与这个根路径相关，比如getting、setting等。”/foo/bar” 将导致操作在”/app/a/foo/bar”（从服务端的观点来看）。这个特性在多租户下面是非常也有用的，ZooKeeper服务的每个用户可以有不同的根路径。这让再使用变得非常简单，因为每个用户都可以编写代码让他的应用好像在”/”根路径下，但实际的位置能在部署时决定，这对于实现不同应用之间的相互隔离非常有帮助。 ​ 客户端可以通过在connecString中添加后缀的方式来设置Chroot，如下所示： 192.168.0.1:2181,192.168.0.1:2181,192.168.0.1:2181/apps/a ​ 将这样一个connectString传入客户端的ConnectStringParser后就能够解析出Chroot并保存在chrootPath属性中 1.2 客户端命令行操作 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 使用 ls 命令来查看当前znode中所包含的内容 ls2 path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建 -s 含有序列 -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 ​ 1）启动客户端 1[kaizhang@localhost bin]$ ./zkCli.sh ​ 2）显示所有操作命令 1[zk: localhost:2181(CONNECTED) 0] help ​ 3）查看当前 znode 中所包含的内容 12[zk: localhost:2181(CONNECTED) 1] ls /[zookeeper] ​ 4）查看当前节点数据并能看到更新次数等数据 12345678910111213[zk: localhost:2181(CONNECTED) 2] ls2 /[zookeeper]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x0cversion = -1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 1 ​ 5）创建普通节点 12345[zk: localhost:2181(CONNECTED) 3] create /app1 &quot;hello app1&quot; Created /app1[zk: localhost:2181(CONNECTED) 4] create /app1/server101 &quot;192.168.1.101&quot;Created /app1/server101 ​ 6）获得节点的值 123456789101112131415161718192021222324252627[zk: localhost:2181(CONNECTED) 5] get /app1hello app1cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x100000004mtime = Mon Sep 30 21:27:00 CST 2019pZxid = 0x100000007cversion = 1dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 10numChildren = 1[zk: localhost:2181(CONNECTED) 6] get /app1/server101192.168.1.101cZxid = 0x100000007ctime = Mon Sep 30 21:28:25 CST 2019mZxid = 0x100000007mtime = Mon Sep 30 21:28:25 CST 2019pZxid = 0x100000007cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 13numChildren = 0 ​ 7）创建短暂节点 12[zk: localhost:2181(CONNECTED) 8] create -e /app-emphemeral 8888 Created /app-emphemeral ​ （1）在当前客户端是能查看到的 12[zk: localhost:2181(CONNECTED) 6] ls / [zookeeper, app1, app-emphemeral] ​ （2）退出当前客户端然后再重启客户端 12[zk: localhost:2181(CONNECTED) 7] quit[kaizhang@localhost bin]$ ./zkCli.sh ​ （3）再次查看根目录下短暂节点已经删除 12[zk: localhost:2181(CONNECTED) 0] ls /[zookeeper, app1] ​ 8）创建带序号的节点 ​ （1）先创建一个普通的根节点 app2 12[zk: localhost:2181(CONNECTED) 2] create /app2 &quot;app2&quot; Created /app2 ​ （2）创建带序号的节点 123456[zk: localhost:2181(CONNECTED) 4] create -s /app2/aa 888Created /app2/aa0000000000[zk: localhost:2181(CONNECTED) 5] create -s /app2/bb 888Created /app2/bb0000000001[zk: localhost:2181(CONNECTED) 6] create -s /app2/cc 888 Created /app2/cc0000000002 如果原节点下有 1 个节点，则再排序时从 1 开始，以此类推。 12[zk: localhost:2181(CONNECTED) 7] create -s /app1/aa 888 Created /app1/aa0000000001 9）修改节点数据值 123456789101112[zk: localhost:2181(CONNECTED) 8] set /app1 999 cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x10000001fmtime = Mon Sep 30 21:46:15 CST 2019pZxid = 0x100000007cversion = 1dataVersion = 3aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 1 10）节点的值变化监听 ​ （1）在 101 主机上注册监听/app1 节点数据变化 1[zk: localhost:2181(CONNECTED) 26] get /app1 watch ​ （2）在 102 主机上修改/app1 节点的数据 1[zk: localhost:2181(CONNECTED) 5] set /app1 777 ​ （3）观察 101 主机收到数据变化的监听 123WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/app1 11）节点的子节点变化监听（路径变化） （1）在 101 主机上注册监听/app1 节点的子节点变化 12[zk: localhost:2181(CONNECTED) 17] ls /app1 watch[server101] ​ （2）在 102 主机/app1 节点上创建子节点 12[zk: localhost:2181(CONNECTED) 15] create /app1/bb 666Created /app1/bb ​ （3）观察 101 主机收到子节点变化的监听 123WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/app1 12）删除节点 1[zk: localhost:2181(CONNECTED) 4] delete /app1/bb 13）递归删除节点 1[zk: localhost:2181(CONNECTED) 7] rmr /app2 14）查看节点状态 123456789101112[zk: localhost:2181(CONNECTED) 8] stat /app1 cZxid = 0x100000004ctime = Mon Sep 30 21:27:00 CST 2019mZxid = 0x100000021mtime = Mon Sep 30 21:51:53 CST 2019pZxid = 0x100000022cversion = 2dataVersion = 5aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 2 1.3 API 应用参考： https://github.com/SparksFlyMe/zookeeperlearning 第2章 Zookeeper 内部原理2.1 选举机制1）半数机制（Paxos 协议）：集群中半数以上机器存活，集群可用。所以 zookeeper适合装在奇数台机器上。 2）Zookeeper 虽然在配置文件中并没有指定 master 和 slave。但是，zookeeper 工作时，是有一个节点为 leader，其他则为 follower，Leader 是通过内部的选举机制临时产生的 3）以一个简单的例子来说明整个选举的过程。 假设有五台服务器组成的 zookeeper 集群，它们的 id 从 1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器 1 启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是 LOOKING 状态。 （2）服务器 2 启动，它与最开始启动的服务器 1 进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以 id 值较大的服务器 2 胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是 3)，所以服务器 1、2 还是继续保持LOOKING 状态。 （3）服务器 3 启动，根据前面的理论分析，服务器 3 成为服务器 1、2、3 中的老大，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的 leader。 （4）服务器 4 启动，根据前面的分析，理论上服务器 4 应该是服务器 1、2、3、4 中最大的，但是由于前面已经有半数以上的服务器选举了服务器 3，所以它只能接收当小弟的命了。 （5）服务器 5 启动，同 4 一样当小弟。 2.2 节点类型1）Znode 有两种类型：短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除 2）Znode 有四种形式的目录节点（默认是 persistent ）（1）持久化目录节点（PERSISTENT） 客户端与 zookeeper 断开连接后，该节点依旧存在（2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与 zookeeper 断开连接后，该节点依旧存在，只是 Zookeeper 给该节点名称进行顺序编号 ​ （3）临时目录节点（EPHEMERAL）客户端与 zookeeper 断开连接后，该节点被删除（4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL） 客户端与 zookeeper 断开连接后，该节点被删除，只是 Zookeeper 给该节点名称进行顺序编号 3）创建 znode 时设置顺序标识，znode 名称后会附加一个值，顺序号是一个单调递增的计数器，由父节点维护 4）在分布式系统中，顺序号可以被用于为所有的事件进行全局排序，这样客户端可以通过顺序号推断事件的顺序 2.3 stat 结构体1）czxid- 引起这个 znode 创建的 zxid，创建节点的事务的 zxid 每次修改 ZooKeeper 状态都会收到一个 zxid 形式的时间戳，也就是 ZooKeeper 事务 ID。事务 ID 是 ZooKeeper 中所有修改总的次序。每个修改都有唯一的 zxid，如果 zxid1 小于 zxid2，那么 zxid1 在 zxid2 之前发生。 2）ctime - znode 被创建的毫秒数(从 1970 年开始)3）mzxid - znode 最后更新的 zxid4）mtime - znode 最后修改的毫秒数(从 1970 年开始)5）pZxid-znode 最后更新的子节点 zxid6）cversion - znode 子节点变化号，znode 子节点修改次数7）dataversion - znode 数据变化号8）aclVersion - znode 访问控制列表的变化号9）ephemeralOwner- 如果是临时节点，这个是 znode 拥有者的 session id。如果不是临时节点则是 0。10）dataLength- znode 的数据长度11）numChildren - znode 子节点数量 2.4 监听器原理 1）监听原理详解：（1）首先要有一个 main()线程（2）在 main 线程中创建 Zookeeper 客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 （3）通过 connect 线程将注册的监听事件发送给 Zookeeper。（4）在 Zookeeper 的注册监听器列表中将注册的监听事件添加到列表中。（5）Zookeeper 监听到有数据或路径变化，就会将这个消息发送给 listener 线程。（6）listener 线程内部调用了 process（）方法。 2）常见的监听（1）监听节点数据的变化： 1get path [watch] （2）监听子节点增减的变化 1ls path [watch] 2.5 写数据流程 ZooKeeper 的写数据流程主要分为以下几步：1）比如 Client 向 ZooKeeper 的 Server1 上写数据，发送一个写请求。 2）如果 Server1 不是 Leader，那么 Server1 会把接受到的请求进一步转发给 Leader，因为每个 ZooKeeper 的 Server 里面有一个是 Leader。这个 Leader 会将写请求广播给各个Server，比如 Server1 和 Server2， 各个 Server 写成功后就会通知 Leader。 3）当 Leader 收到大多数 Server 数据写成功了，那么就说明数据写成功了。如果这里三个节点的话，只要有两个节点数据写成功了，那么就认为数据写成功了。写成功之后，Leader 会告诉 Server1 数据写成功了。 4）Server1 会进一步通知 Client 数据写成功了，这时就认为整个写操作成功。ZooKeeper 整个写数据流程就是这样的。]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper概述及单机搭建]]></title>
    <url>%2F2019%2F09%2F28%2FZookeeper%E6%A6%82%E8%BF%B0%E5%8F%8A%E5%8D%95%E6%9C%BA%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[第 1 章 Zookeeper 概述1.1 概述 Zookeeper 是一个开源的分布式的，为分布式应用提供协调服务的 Apache 项目。 Zookeeper 从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper 就将负责通知已经在 Zookeeper 上注册的那些观察者做出相应的反应，从而实现集群中类似 Master/Slave 管理模式 。 Zookeeper=文件系统+通知机制。 1.2 特点1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。2）Leader 负责进行投票的发起和决议，更新系统状态3）Follower 用于接收客户请求并向客户端返回结果，在选举 Leader 过程中参与投票4）集群中只要有半数以上节点存活，Zookeeper 集群就能正常服务。5）全局数据一致：每个 server 保存一份相同的数据副本，client 无论连接到哪个 server，数据都是一致的。6）更新请求顺序进行，来自同一个 client 的更新请求按其发送顺序依次执行。7）数据更新原子性，一次数据更新要么成功，要么失败。8）实时性，在一定时间范围内，client 能读到最新数据。 1.3 数据结构 ZooKeeper 数据模型的结构与 Unix 文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。 很显然 zookeeper 集群自身维护了一套数据结构。这个存储结构是一个树形结构，其上的每一个节点，我们称之为”znode”，每一个 znode 默认能够存储 1MB 的数据，每个 ZNode都可以通过其路径唯一标识 1.4 应用场景提供的服务包括：分布式消息同步和协调机制、服务器节点动态上下线、统一配置管理、负载均衡、集群管理等。 1.5 下载地址https://zookeeper.apache.org 第二章 Zookeeper 安装2.1 本地模式安装部署 （单机版）1）安装前准备： ​ （1）安装 jdk​ （2）通过 xshell等工具拷贝 zookeeper 到 linux 系统下​ （3）解压到指定目录 1[kaizhang@localhost software]$ tar -zvxf zookeeper-3.4.14.tar.gz 2）配置修改 ​ （1）将/opt/software/zookeeper-3.4.14/conf 这个路径下的 zoo_sample.cfg 复制为 zoo.cfg： 1[kaizhang@localhost conf]$ cp zoo_sample.cfg zoo.cfg ​ （2）进入 zoo.cfg 文件：vim zoo.cfg ​ 修改 dataDir 路径为​ dataDir=/opt/software/zookeeper-3.4.14/zkData ​ （3）在/opt/software/zookeeper-3.4.14/这个目录上创建 zkData 文件夹 1[kaizhang@localhost zookeeper-3.4.14]$ mkdir zkData 3）操作 zookeeper ​ （1）启动 zookeeper ： 1[kaizhang@localhost bin]$ ./zkServer.sh start ​ （2）查看进程是否启动 ： 123[kaizhang@localhost bin]$ jps8465 Jps7751 QuorumPeerMain ​ （3）查看状态： 1234[kaizhang@localhost bin]$ ./zkServer.sh statusZooKeeper JMX enabled by defaultUsing config: /opt/software/zookeeper-3.4.14/bin/../conf/zoo.cfgMode: standalone ​ （4）启动客户端： 1[kaizhang@localhost bin]$ ./zkCli.sh ​ （5）退出客户端： 1[zk: localhost:2181(CONNECTED) 0] quit ​ （6）停止 zookeeper ： 1[kaizhang@localhost bin]$ ./zkServer.sh stop 2.2 配置参数解读解读zoo.cfg 文件中参数含义1）tickTime：通信心跳数。Zookeeper服务器心跳时间，单位毫秒 ​ Zookeeper使用的基本时间，服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每隔tickTime时间就会发送一个心跳，时间单位为毫秒。 ​ 它用于心跳机制，并且设置最小的session超时时间为两倍心跳时间。(session的最小超时时间是2*tickTime) 2）initLimit：LF初始通信时限 集群中的follower跟随者服务器(F)与leader领导者服务器(L)之间初始连接时能容忍的最多心跳数（tickTime的数量），用它来限定集群中的Zookeeper服务器连接到Leader的时限。 ​ 投票选举新leader的初始化时间​ Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。 ​ Leader允许F在initLimit时间内完成这个工作。 3）syncLimit：LF 同步通信时限 集群中Leader与Follower之间的最大响应时间单位，假如响应超过syncLimit * tickTime，Leader认为Follwer死掉，从服务器列表中删除Follwer。 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。 如果L发出心跳包在syncLimit之后，还没有从F那收到响应，那么就认为这个F已经不在线了。 4）dataDir：数据文件目录+数据持久化路径 保存内存数据库快照信息的位置，如果没有其他说明，更新的事务日志也保存到数据库。 5）clientPort：客户端连接端口 监听客户端连接的端口]]></content>
      <categories>
        <category>中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring自动装配]]></title>
    <url>%2F2019%2F07%2F21%2FSpring%E8%87%AA%E5%8A%A8%E8%A3%85%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Spring的自动装配功能：自动装配：Spring利用依赖注入（DI），完成对IOC容器中各种组件的依赖关系赋值@Autowired的使用场景在构造器（CONSTRUCTOR），参数（PARAMETER），方法（METHOD），属性（FIELD），注释类型（ANNOTATION_TYPE）上，都能使用这个注解{@link Autowired}。且都是从容器中获取参数组件的值 标注在构造器上：默认加载在ioc容器中的组件，容器启动会调用无参构造器创建对象，再进行初始化赋值等操作。 构造器要用的组件，都是从容器中获取。如果组件只有一个有参构造器，这个有参构造器的@Autowired可以省略，参数位置的组件还是可以自动从容器中获取。 12345678910@Componentpublic class PersonService &#123; private Student student; @Autowired //可以省略 public PersonService(Student student)&#123; this.student = student; System.out.println("PersonService 有参构造器。。。"); &#125;&#125; 标注在参数上： 12345678@Componentpublic class PersonService &#123; private Student student; public PersonService(@Autowired Student student)&#123; this.student = student; &#125;&#125; 标注在方法上：Spring容器创建当前对象，就会调用方法，完成赋值；方法使用的参数，自定义类型的值从ioc容器中获取。常用的有：@Bean + 方法参数，参数从容器中获取 1234567891011121314@Componentpublic class PersonService &#123; private Student student; /** * * @param student 这里方法的参数（student）的值，从ioc容器中获取 * @return */ @Autowired public Student getStudent(Student student) &#123; return this.student = student; &#125;&#125; @Autowired的详细使用 默认优先按照类型去容器中找对应的组件。 1annotationConfigApplicationContext.getBean(BookDao.class); 如果找到多个相同类型的组件，再将属性的名称作为组件的id去容器中查找。 12345 /** * 这里bookDao为注入的属性名 */annotationConfigApplicationContext.getBean("bookDao") @Qualifier(“bookDao)：使用@Qualifier指定需要装配的组件的id，而不是使用属性名。 自动装配默认一定要将属性赋值好，如果没有就会报错。 @Autowired(required = true)，required默认为true，改为false后就不会报错 @Primary：让Spring进行自动装配的时候，默认使用首先的bean，也可以继续使用@Qualifier指定需要装配的bean的名字 @Autowired是Spring规范的注解，同时Spring还支持java规范的注解：@Resource（JSR250规范）和@Inject(JSR330规范) @Resource 可以和@Autowired一样实现自动装配功能；默认是按照组件名称进行装配的，不支持@Primary,也不支持@Autowired（require=false） @Inject 需要导入javax的包，和Autowired的功能一样。没有require=false的功能 注意事项这里说到的对各种组件的自动装配，前提是这些一定是Spring的组件（即需要添加@Configuration等注解）]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring 注册组件的几种方式]]></title>
    <url>%2F2019%2F07%2F14%2FSpring-%E6%B3%A8%E5%86%8C%E7%BB%84%E4%BB%B6%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[在Spring中，给容器注册组件有多种方式，从之前的xml配置到后来的全注解配置，省去了复杂繁琐的xml配置，给开发人员带来的极大便利。给容器中注册组件的几种方式：1、包扫描 + 组件标注注解（@Controller、@Service、@Repository、@Component）2、@Bean（导入第三方包里面的组件，比如RestTemplate）3、@Import（快速给容器中导入一个组件） @Import（要导入到容器的组件）；容器中就会自动注册这个组件，id默认是全类名 实现ImportSelector类：返回需要导入的组件的全类名数组 实现ImportBeanDefinitionRegistrar：手动注册bean到容器中 4、使用Spring提供的FactoryBean(工厂Bean)]]></content>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mybatis foreach循环 collection的三种方式]]></title>
    <url>%2F2019%2F07%2F13%2FMybatis-foreach%E5%BE%AA%E7%8E%AF-collection%E7%9A%84%E4%B8%89%E7%A7%8D%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[自从有了Mybatis加强版Mybatis,再也不用写那么复杂麻烦的xml配置文件了！但是偶尔也有需求还得用xml，对于Mybatis的foreach也是经常用到的，但是每次都是从项目里粘贴复制、修修改改就用，对于其具体使用理解也是模模糊糊，导致有时用到的时候遇到各种问题，此文就针对Mybatis的foreach做个记录，以便更透彻的理解与使用。foreach标签主要用于构建in条件，它可以在sql中对集合进行迭代，通常可以将之用到批量删除、添加等操作中，示例如下： 1234567891011&lt;select id="getRequestLogList" parameterType="java.util.HashMap" resultType="RequestLog"&gt; SELECT * FROM request_log &lt;where&gt; &lt;if test="sessionIdList !=null and sessionIdList.size &gt; 0"&gt; and sessionId in ( &lt;foreach collection="sessionIdList" item="item" index="index" separator=","&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 加入我们输入的参数为 Long sessionIdList[] = {1L, 2L, 4L}; 那么对应执行的sql就是 ： 1SELECT * FROM request_log where sessionId in (1, 2, 4); foreach元素的属性主要有 item，index，collection，open，separator，close。 ​ item：表示集合中每一个元素进行迭代时的别名， ​ index：指 定一个名字，用于表示在迭代过程中，每次迭代到的位置， ​ open：表示该语句以什么开始， ​ separator：表示在每次进行迭代之间以什么符号作为分隔 符， ​ close：表示以什么结束。 在使用foreach的时候最关键的也是最容易出错的就是collection属性，该属性是必须指定的，但是在不同情况 下，该属性的值是不一样的，主要有一下3种情况： ​ 如果传入的是单参数且参数类型是一个List的时候，collection属性值为list ​ 如果传入的是单参数且参数类型是一个array数组的时候，collection的属性值为array ​ 如果传入的参数是多个的时候，我们就需要把它们封装成一个Map了，当然单参数也可 1、collection属性为List方法调用 123public List&lt;Area&gt; findUserListByIdList(List&lt;Long&gt; idList) &#123; return getSqlSession().findUserListByIdList(idList); &#125; 对应的mapper： 123456789&lt;select id="findUserListByIdList" parameterType="java.util.ArrayList" resultType="User"&gt; select * from user &lt;where&gt; ID in ( &lt;foreach collection="list" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/where&gt; &lt;/select&gt; 即单独传入list时，foreach中的collection必须是 list，不管变量的具体名称是什么。比如这里变量名为idList， collection却是List。 2、collection属性为array方法调用 123public List&lt;Area&gt; findUserListByIdList(int[] ids) &#123; return getSqlSession().findUserListByIdList(ids); &#125; 对应的mapper： 123456789&lt;select id="findUserListByIdList" parameterType="java.util.HashList" resultType="User"&gt; select * from user &lt;where&gt; ID in ( &lt;foreach collection="array" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/where&gt; &lt;/select&gt; 单独传入数组时，foreach中的collection必须是 array，不管变量的具体名称是什么。比如这里变量名为ids，collection却是array。 3、collection属性为map方法调用： 12345public boolean exists(Map&lt;String, Object&gt; map)&#123; Object count = getSqlSession().exists(map); int totalCount = Integer.parseInt(count.toString()); return totalCount &gt; 0 ? true : false; &#125; 对应的mapper： 1234567891011121314151617&lt;select id="exists" parameterType="java.util.HashMap" resultType="java.lang.Integer"&gt; SELECT COUNT(*) FROM USER user &lt;where&gt; &lt;if test="code != null"&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test="id != null"&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test="idList !=null "&gt; and ID in ( &lt;foreach collection="idList" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; map中有list或array时，foreach中的collection必须是具体list或array的变量名。比如这里map含有一个名为idList的list，所以map中用idList(map的key)来取值，这点和单独传list或array时不太一样。 4、传入java对象调用方法 12345public boolean findUserListByDTO(UserDTO userDTO)&#123; Object count = getSqlSession().findUserListByDTO(userDTO); int totalCount = Integer.parseInt(count.toString()); return totalCount &gt; 0 ? true : false; &#125; 对应的mapper： 1234567891011121314151617select id="findUserListByDTO" parameterType="UserDTO" resultType="java.lang.Integer"&gt; SELECT COUNT(*) FROM USER user &lt;where&gt; &lt;if test="code != null"&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test="id != null"&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test="idList !=null "&gt; and ID in ( &lt;foreach collection="idList" item="guard" index="index" separator=","&gt; #&#123;guard&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; JAVA对象中有list或array时，foreach中的collection必须是具体list或array的变量名。比如这里UserDTO含有一个名为idList的list，所以UserDTO中用idList取值，这点和单独传list或array时不太一样。]]></content>
  </entry>
  <entry>
    <title><![CDATA[Mybatis foreach 使用错误记录]]></title>
    <url>%2F2019%2F07%2F13%2FMybatis-foreach-%E9%94%99%E8%AF%AF%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[使用Mybatis foreach遍历时，Error evaluating expression ‘sessionIdList’. Return value () was not iterable.问题的起因时这样的，同事用map方式传值，map里需要存入一个集合，但是在判断集合为空时放了空字符串，导致mybatis在遍历时无法把空字符串解析成集合。 传入map 1234567891011HashMap&lt;String, Object&gt; mapper = new HashMap&lt;String, Object&gt;(16);// 获取要遍历的sessionIdList集合List&lt;Long&gt; list = getSessionIdList();if (CollectionUtils.isEmpty(list)) &#123; mapper.put(&quot;sessionIdList&quot;, &quot;&quot;); //罪魁祸首在这里&#125; else &#123; mapper.put(&quot;sessionIdList&quot;, list);&#125;List&lt;RequestLog&gt; requestLogList = requestLogService.getRequestLogList(mapper); 对应mapper 1234567891011121314151617&lt;select id=&quot;getRequestLogList&quot; parameterType=&quot;java.util.HashMap&quot; resultType=&quot;RequestLog&quot;&gt; SELECT * FROM USER request_log &lt;where&gt; &lt;if test=&quot;code != null and code != &apos;&apos; &quot;&gt; and CODE = #&#123;code&#125; &lt;/if&gt; &lt;if test=&quot;id != null and id != &apos;&apos;&quot;&gt; and ID = #&#123;id&#125; &lt;/if&gt; &lt;if test=&quot;sessionIdList !=null and sessionIdList.size &gt; 0&quot;&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; &lt;/where&gt; &lt;/select&gt; 当调用getSessionIdList()方法获取到的集合为空时，传入的值是空字符串而非集合类型，从而在mapper.xml中解析时无法把字符串类型解析成集合类型，报出了Return value () was not iterable的问题。 另外还发现了另一个问题，在判断mapper.xm中集合是否为空时，不能与空字符串比较进行判断，而应该用size方法，否则会引起 invalid comparison: java.util.ArrayList and java.lang.String 把集合类型与字符串类型作比较，引起了“无效的比较”错误 反例 123456&lt;if test=&quot;sessionIdList !=null and sessionIdList != &apos;&apos;&quot;&gt; &lt;!--这里用!=&apos;&apos; 进行判断会报错，因为引起了集合与String类型的比较--&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; 正例123456&lt;if test=&quot;sessionIdList !=null and sessionIdList.size &gt; 0 &apos;&apos;&quot;&gt; &lt;!--这里应该用sessionIdList.size &gt; 0 来判断集合是否为空--&gt; and sessionId in ( &lt;foreach collection=&quot;sessionIdList&quot; item=&quot;item&quot; index=&quot;index&quot; separator=&quot;,&quot;&gt; #&#123;item&#125; &lt;/foreach&gt; ) &lt;/if&gt; 总结当用mabatis foreach遍历时，遍历对象的类型一定要确认是否正确。一般有三种：list(集合)、array(数组)、map(map中传入集合)]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>Mybatis</tag>
      </tags>
  </entry>
</search>
